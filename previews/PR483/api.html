<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>API | SymbolicRegression.jl</title>
    <meta name="description" content="Documentation for SymbolicRegression.jl">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/SymbolicRegression.jl/previews/PR483/assets/style.CFahYqTk.css" as="style">
    <link rel="preload stylesheet" href="/SymbolicRegression.jl/previews/PR483/vp-icons.css" as="style">
    
    <script type="module" src="/SymbolicRegression.jl/previews/PR483/assets/app.CMJX_E_Y.js"></script>
    <link rel="preload" href="/SymbolicRegression.jl/previews/PR483/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/SymbolicRegression.jl/previews/PR483/assets/chunks/theme.CUTpitD4.js">
    <link rel="modulepreload" href="/SymbolicRegression.jl/previews/PR483/assets/chunks/framework.DRv5kib6.js">
    <link rel="modulepreload" href="/SymbolicRegression.jl/previews/PR483/assets/api.md.DfbDqNuT.lean.js">
    <link rel="icon" href="/SymbolicRegression.jl/previews/PR483/favicon.ico">
    <script src="/SymbolicRegression.jl/versions.js"></script>
    <script src="/SymbolicRegression.jl/previews/PR483/siteinfo.js"></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a9a9e638><!--[--><!--]--><!--[--><span tabindex="-1" data-v-492508fc></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-492508fc>Skip to content</a><!--]--><!----><header class="VPNav" data-v-a9a9e638 data-v-f1e365da><div class="VPNavBar" data-v-f1e365da data-v-822684d1><div class="wrapper" data-v-822684d1><div class="container" data-v-822684d1><div class="title" data-v-822684d1><div class="VPNavBarTitle has-sidebar" data-v-822684d1 data-v-0f4f798b><a class="title" href="/SymbolicRegression.jl/previews/PR483/" data-v-0f4f798b><!--[--><!--]--><!--[--><img class="VPImage logo" src="/SymbolicRegression.jl/previews/PR483/logo.svg" width="24" height="24" alt data-v-35a7d0b8><!--]--><span data-v-0f4f798b>SymbolicRegression.jl</span><!--[--><!--]--></a></div></div><div class="content" data-v-822684d1><div class="content-body" data-v-822684d1><!--[--><!--]--><div class="VPNavBarSearch search" data-v-822684d1><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-822684d1 data-v-e6d46098><span id="main-nav-aria-label" class="visually-hidden" data-v-e6d46098> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/SymbolicRegression.jl/previews/PR483/index" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Examples</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/SymbolicRegression.jl/previews/PR483/examples" data-v-acbfed09><!--[--><span data-v-acbfed09>Short Examples</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/SymbolicRegression.jl/previews/PR483/examples/template_expression" data-v-acbfed09><!--[--><span data-v-acbfed09>Template Expressions</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/SymbolicRegression.jl/previews/PR483/examples/parameterized_function" data-v-acbfed09><!--[--><span data-v-acbfed09>Parameterized Expressions</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/SymbolicRegression.jl/previews/PR483/examples/template_parametric_expression" data-v-acbfed09><!--[--><span data-v-acbfed09>Parameterized Template Expressions</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/SymbolicRegression.jl/previews/PR483/examples/custom_types" data-v-acbfed09><!--[--><span data-v-acbfed09>Custom Types</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/SymbolicRegression.jl/previews/PR483/slurm" data-v-acbfed09><!--[--><span data-v-acbfed09>Using SymbolicRegression.jl on a Cluster</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/SymbolicRegression.jl/previews/PR483/api" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>API</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/SymbolicRegression.jl/previews/PR483/losses" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Losses</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/SymbolicRegression.jl/previews/PR483/types" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Types</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/SymbolicRegression.jl/previews/PR483/customization" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Customization</span><!--]--></a><!--]--><!--[--><!----><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-822684d1 data-v-af096f4a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-af096f4a data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-822684d1 data-v-164c457f data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/MilesCranmer/SymbolicRegression.jl" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-822684d1 data-v-925effce data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-04f5c5e9><span class="vpi-more-horizontal icon" data-v-04f5c5e9></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--[--><!----><div class="group" data-v-925effce><div class="item appearance" data-v-925effce><p class="label" data-v-925effce>Appearance</p><div class="appearance-action" data-v-925effce><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-925effce data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div></div></div><div class="group" data-v-925effce><div class="item social-links" data-v-925effce><div class="VPSocialLinks social-links-list" data-v-925effce data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/MilesCranmer/SymbolicRegression.jl" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><div class="VPFlyout VPNolebaseEnhancedReadabilitiesMenu VPNolebaseEnhancedReadabilitiesMenuFlyout" aria-label="Enhanced Readability" role="menuitem" data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><span class="i-icon-park-outline:book-open option-icon" data-v-04f5c5e9></span><!----><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--]--></div></div></div><!--]--><!--]--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-822684d1 data-v-5dea55bf><span class="container" data-v-5dea55bf><span class="top" data-v-5dea55bf></span><span class="middle" data-v-5dea55bf></span><span class="bottom" data-v-5dea55bf></span></span></button></div></div></div></div><div class="divider" data-v-822684d1><div class="divider-line" data-v-822684d1></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-a9a9e638 data-v-070ab83d><div class="container" data-v-070ab83d><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-070ab83d><span class="vpi-align-left menu-icon" data-v-070ab83d></span><span class="menu-text" data-v-070ab83d>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-070ab83d data-v-168ddf5d><button data-v-168ddf5d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-a9a9e638 data-v-18756405><div class="curtain" data-v-18756405></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18756405><span class="visually-hidden" id="sidebar-aria-label" data-v-18756405> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/index" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Home</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>Examples</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/examples" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Short Examples</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/examples/template_expression" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Template Expressions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/examples/parameterized_function" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Parameterized Expressions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/examples/template_parametric_expression" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Parameterized Template Expressions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/examples/custom_types" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Custom Types</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/slurm" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Using SymbolicRegression.jl on a Cluster</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 has-active" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/api" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/losses" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Losses</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/types" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Types</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/SymbolicRegression.jl/previews/PR483/customization" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Customization</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a9a9e638 data-v-91765379><div class="VPDoc has-sidebar has-aside" data-v-91765379 data-v-83890dd9><!--[--><!--]--><div class="container" data-v-83890dd9><div class="aside" data-v-83890dd9><div class="aside-curtain" data-v-83890dd9></div><div class="aside-container" data-v-83890dd9><div class="aside-content" data-v-83890dd9><div class="VPDocAside" data-v-83890dd9 data-v-6d7b3c46><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-6d7b3c46 data-v-b38bf2ff><div class="content" data-v-b38bf2ff><div class="outline-marker" data-v-b38bf2ff></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b38bf2ff>On this page</div><ul class="VPDocOutlineItem root" data-v-b38bf2ff data-v-3f927ebe><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-6d7b3c46></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-83890dd9><div class="content-container" data-v-83890dd9><!--[--><!--]--><main class="main" data-v-83890dd9><div style="position:relative;" class="vp-doc _SymbolicRegression_jl_previews_PR483_api" data-v-83890dd9><div><h1 id="API" tabindex="-1">API <a class="header-anchor" href="#API" aria-label="Permalink to &quot;API {#API}&quot;">​</a></h1><h2 id="MLJ-interface" tabindex="-1">MLJ interface <a class="header-anchor" href="#MLJ-interface" aria-label="Permalink to &quot;MLJ interface {#MLJ-interface}&quot;">​</a></h2><details class="jldocstring custom-block" open><summary><a id="SymbolicRegression.MLJInterfaceModule.SRRegressor" href="#SymbolicRegression.MLJInterfaceModule.SRRegressor"><span class="jlbinding">SymbolicRegression.MLJInterfaceModule.SRRegressor</span></a> <span class="VPBadge info jlObjectType jlType"><!--[-->Type<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">SRRegressor</span></span></code></pre></div><p>A model type for constructing a Symbolic Regression via Evolutionary Search, based on <a href="https://github.com/MilesCranmer/SymbolicRegression.jl" target="_blank" rel="noreferrer">SymbolicRegression.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SRRegressor = @load SRRegressor pkg=SymbolicRegression</span></span></code></pre></div><p>Do <code>model = SRRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>SRRegressor(defaults=...)</code>.</p><p>Single-target Symbolic Regression regressor (<code>SRRegressor</code>) searches for symbolic expressions that predict a single target variable from a set of input variables. All data is assumed to be <code>Continuous</code>. The search is performed using an evolutionary algorithm. This algorithm is described in the paper <a href="https://arxiv.org/abs/2305.01582" target="_blank" rel="noreferrer">https://arxiv.org/abs/2305.01582</a>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>mach = machine(model, X, y)</span></span></code></pre></div><p>OR</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>mach = machine(model, X, y, w)</span></span></code></pre></div><p>Here:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>. Variable names in discovered expressions will be taken from the column names of <code>X</code>, if available. Units in columns of <code>X</code> (use <code>DynamicQuantities</code> for units) will trigger dimensional analysis to be used.</p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code>. Units in <code>y</code> (use <code>DynamicQuantities</code> for units) will trigger dimensional analysis to be used.</p></li><li><p><code>w</code> is the observation weights which can either be <code>nothing</code> (default) or an <code>AbstractVector</code> whose element scitype is <code>Count</code> or <code>Continuous</code>.</p></li></ul><p>Train the machine using <code>fit!(mach)</code>, inspect the discovered expressions with <code>report(mach)</code>, and predict on new data with <code>predict(mach, Xnew)</code>. Note that unlike other regressors, symbolic regression stores a list of trained models. The model chosen from this list is defined by the function <code>selection_method</code> keyword argument, which by default balances accuracy and complexity. You can override this at prediction time by passing a named tuple with keys <code>data</code> and <code>idx</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>defaults</code>: What set of defaults to use for <code>Options</code>. The default, <code>nothing</code>, will simply take the default options from the current version of SymbolicRegression. However, you may also select the defaults from an earlier version, such as <code>v&quot;0.24.5&quot;</code>.</p></li><li><p><code>binary_operators</code>: Vector of binary operators (functions) to use. Each operator should be defined for two input scalars, and one output scalar. All operators need to be defined over the entire real line (excluding infinity - these are stopped before they are input), or return <code>NaN</code> where not defined. For speed, define it so it takes two reals of the same type as input, and outputs the same type. For the SymbolicUtils simplification backend, you will need to define a generic method of the operator so it takes arbitrary types.</p></li><li><p><code>operator_enum_constructor</code>: Constructor function to use for creating the operators enum. By default, OperatorEnum is used, but you can provide a different constructor like GenericOperatorEnum. The constructor must accept the keyword arguments &#39;binary_operators&#39; and &#39;unary_operators&#39;.</p></li><li><p><code>unary_operators</code>: Same, but for unary operators (one input scalar, gives an output scalar).</p></li><li><p><code>constraints</code>: Array of pairs specifying size constraints for each operator. The constraints for a binary operator should be a 2-tuple (e.g., <code>(-1, -1)</code>) and the constraints for a unary operator should be an <code>Int</code>. A size constraint is a limit to the size of the subtree in each argument of an operator. e.g., <code>[(^)=&gt;(-1, 3)]</code> means that the <code>^</code> operator can have arbitrary size (<code>-1</code>) in its left argument, but a maximum size of <code>3</code> in its right argument. Default is no constraints.</p></li><li><p><code>batching</code>: Whether to evolve based on small mini-batches of data, rather than the entire dataset.</p></li><li><p><code>batch_size</code>: What batch size to use if using batching.</p></li><li><p><code>elementwise_loss</code>: What elementwise loss function to use. Can be one of the following losses, or any other loss of type <code>SupervisedLoss</code>. You can also pass a function that takes a scalar target (left argument), and scalar predicted (right argument), and returns a scalar. This will be averaged over the predicted data. If weights are supplied, your function should take a third argument for the weight scalar. Included losses: Regression: - <code>LPDistLoss{P}()</code>, - <code>L1DistLoss()</code>, - <code>L2DistLoss()</code> (mean square), - <code>LogitDistLoss()</code>, - <code>HuberLoss(d)</code>, - <code>L1EpsilonInsLoss(ϵ)</code>, - <code>L2EpsilonInsLoss(ϵ)</code>, - <code>PeriodicLoss(c)</code>, - <code>QuantileLoss(τ)</code>, Classification: - <code>ZeroOneLoss()</code>, - <code>PerceptronLoss()</code>, - <code>L1HingeLoss()</code>, - <code>SmoothedL1HingeLoss(γ)</code>, - <code>ModifiedHuberLoss()</code>, - <code>L2MarginLoss()</code>, - <code>ExpLoss()</code>, - <code>SigmoidLoss()</code>, - <code>DWDMarginLoss(q)</code>.</p></li><li><p><code>loss_function</code>: Alternatively, you may redefine the loss used as any function of <code>tree::AbstractExpressionNode{T}</code>, <code>dataset::Dataset{T}</code>, and <code>options::AbstractOptions</code>, so long as you output a non-negative scalar of type <code>T</code>. This is useful if you want to use a loss that takes into account derivatives, or correlations across the dataset. This also means you could use a custom evaluation for a particular expression. If you are using <code>batching=true</code>, then your function should accept a fourth argument <code>idx</code>, which is either <code>nothing</code> (indicating that the full dataset should be used), or a vector of indices to use for the batch. For example,</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  function my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}</span></span>
<span class="line"><span>      prediction, flag = eval_tree_array(tree, dataset.X, options)</span></span>
<span class="line"><span>      if !flag</span></span>
<span class="line"><span>          return L(Inf)</span></span>
<span class="line"><span>      end</span></span>
<span class="line"><span>      return sum((prediction .- dataset.y) .^ 2) / dataset.n</span></span>
<span class="line"><span>  end</span></span></code></pre></div></li><li><p><code>loss_function_expression</code>: Similar to <code>loss_function</code>, but takes <code>AbstractExpression</code> instead of <code>AbstractExpressionNode</code> as its first argument. Useful for <code>TemplateExpressionSpec</code>.</p></li><li><p><code>loss_scale</code>: Determines how loss values are scaled when computing scores. Options are:</p><ul><li><p><code>:log</code> (default): Uses logarithmic scaling of loss ratios. This mode requires non-negative loss values and is ideal for traditional loss functions that are always positive.</p></li><li><p><code>:linear</code>: Uses direct differences between losses. This mode handles any loss values (including negative) and is useful for custom loss functions, especially those based on likelihoods.</p></li></ul></li><li><p><code>expression_spec::AbstractExpressionSpec</code>: A specification of what types of expressions to use in the search. For example, <code>ExpressionSpec()</code> (default). You can also see <code>TemplateExpressionSpec</code> and <code>ParametricExpressionSpec</code> for specialized cases.</p></li><li><p><code>populations</code>: How many populations of equations to use.</p></li><li><p><code>population_size</code>: How many equations in each population.</p></li><li><p><code>ncycles_per_iteration</code>: How many generations to consider per iteration.</p></li><li><p><code>tournament_selection_n</code>: Number of expressions considered in each tournament.</p></li><li><p><code>tournament_selection_p</code>: The fittest expression in a tournament is to be selected with probability <code>p</code>, the next fittest with probability <code>p*(1-p)</code>, and so forth.</p></li><li><p><code>topn</code>: Number of equations to return to the host process, and to consider for the hall of fame.</p></li><li><p><code>complexity_of_operators</code>: What complexity should be assigned to each operator, and the occurrence of a constant or variable. By default, this is 1 for all operators. Can be a real number as well, in which case the complexity of an expression will be rounded to the nearest integer. Input this in the form of, e.g., [(^) =&gt; 3, sin =&gt; 2].</p></li><li><p><code>complexity_of_constants</code>: What complexity should be assigned to use of a constant. By default, this is 1.</p></li><li><p><code>complexity_of_variables</code>: What complexity should be assigned to use of a variable, which can also be a vector indicating different per-variable complexity. By default, this is 1.</p></li><li><p><code>complexity_mapping</code>: Alternatively, you can pass a function that takes the expression as input and returns the complexity. Make sure that this operates on <code>AbstractExpression</code> (and unpacks to <code>AbstractExpressionNode</code>), and returns an integer.</p></li><li><p><code>alpha</code>: The probability of accepting an equation mutation during regularized evolution is given by exp(-delta_loss/(alpha * T)), where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.</p></li><li><p><code>maxsize</code>: Maximum size of equations during the search.</p></li><li><p><code>maxdepth</code>: Maximum depth of equations during the search, by default this is set equal to the maxsize.</p></li><li><p><code>parsimony</code>: A multiplicative factor for how much complexity is punished.</p></li><li><p><code>dimensional_constraint_penalty</code>: An additive factor if the dimensional constraint is violated.</p></li><li><p><code>dimensionless_constants_only</code>: Whether to only allow dimensionless constants.</p></li><li><p><code>use_frequency</code>: Whether to use a parsimony that adapts to the relative proportion of equations at each complexity; this will ensure that there are a balanced number of equations considered for every complexity.</p></li><li><p><code>use_frequency_in_tournament</code>: Whether to use the adaptive parsimony described above inside the score, rather than just at the mutation accept/reject stage.</p></li><li><p><code>adaptive_parsimony_scaling</code>: How much to scale the adaptive parsimony term in the loss. Increase this if the search is spending too much time optimizing the most complex equations.</p></li><li><p><code>turbo</code>: Whether to use <code>LoopVectorization.@turbo</code> to evaluate expressions. This can be significantly faster, but is only compatible with certain operators. <em>Experimental!</em></p></li><li><p><code>bumper</code>: Whether to use Bumper.jl for faster evaluation. <em>Experimental!</em></p></li><li><p><code>migration</code>: Whether to migrate equations between processes.</p></li><li><p><code>hof_migration</code>: Whether to migrate equations from the hall of fame to processes.</p></li><li><p><code>fraction_replaced</code>: What fraction of each population to replace with migrated equations at the end of each cycle.</p></li><li><p><code>fraction_replaced_hof</code>: What fraction to replace with hall of fame equations at the end of each cycle.</p></li><li><p><code>fraction_replaced_guesses</code>: What fraction to replace with user-provided guess expressions at the end of each cycle.</p></li><li><p><code>should_simplify</code>: Whether to simplify equations. If you pass a custom objective, this will be set to <code>false</code>.</p></li><li><p><code>should_optimize_constants</code>: Whether to use an optimization algorithm to periodically optimize constants in equations.</p></li><li><p><code>optimizer_algorithm</code>: Select algorithm to use for optimizing constants. Default is <code>Optim.BFGS(linesearch=LineSearches.BackTracking())</code>.</p></li><li><p><code>optimizer_nrestarts</code>: How many different random starting positions to consider for optimization of constants.</p></li><li><p><code>optimizer_probability</code>: Probability of performing optimization of constants at the end of a given iteration.</p></li><li><p><code>optimizer_iterations</code>: How many optimization iterations to perform. This gets passed to <code>Optim.Options</code> as <code>iterations</code>. The default is 8.</p></li><li><p><code>optimizer_f_calls_limit</code>: How many function calls to allow during optimization. This gets passed to <code>Optim.Options</code> as <code>f_calls_limit</code>. The default is <code>10_000</code>.</p></li><li><p><code>optimizer_options</code>: General options for the constant optimization. For details we refer to the documentation on <code>Optim.Options</code> from the <code>Optim.jl</code> package. Options can be provided here as <code>NamedTuple</code>, e.g. <code>(iterations=16,)</code>, as a <code>Dict</code>, e.g. Dict(:x_tol =&gt; 1.0e-32,), or as an <code>Optim.Options</code> instance.</p></li><li><p><code>autodiff_backend</code>: The backend to use for differentiation, which should be an instance of <code>AbstractADType</code> (see <code>ADTypes.jl</code>). Default is <code>nothing</code>, which means <code>Optim.jl</code> will estimate gradients (likely with finite differences). You can also pass a symbolic version of the backend type, such as <code>:Zygote</code> for Zygote.jl or <code>:Mooncake</code> for Mooncake.jl. Most backends will not work, and many will never work due to incompatibilities, though support for some is gradually being added.</p></li><li><p><code>perturbation_factor</code>: When mutating a constant, either multiply or divide by (1+perturbation_factor)^(rand()+1).</p></li><li><p><code>probability_negate_constant</code>: Probability of negating a constant in the equation when mutating it.</p></li><li><p><code>mutation_weights</code>: Relative probabilities of the mutations. The struct <code>MutationWeights</code> (or any <code>AbstractMutationWeights</code>) should be passed to these options. See its documentation on <code>MutationWeights</code> for the different weights.</p></li><li><p><code>crossover_probability</code>: Probability of performing crossover.</p></li><li><p><code>annealing</code>: Whether to use simulated annealing.</p></li><li><p><code>warmup_maxsize_by</code>: Whether to slowly increase the max size from 5 up to <code>maxsize</code>. If nonzero, specifies the fraction through the search at which the maxsize should be reached.</p></li><li><p><code>verbosity</code>: Whether to print debugging statements or not.</p></li><li><p><code>print_precision</code>: How many digits to print when printing equations. By default, this is 5.</p></li><li><p><code>output_directory</code>: The base directory to save output files to. Files will be saved in a subdirectory according to the run ID. By default, this is <code>./outputs</code>.</p></li><li><p><code>save_to_file</code>: Whether to save equations to a file during the search.</p></li><li><p><code>bin_constraints</code>: See <code>constraints</code>. This is the same, but specified for binary operators only (for example, if you have an operator that is both a binary and unary operator).</p></li><li><p><code>una_constraints</code>: Likewise, for unary operators.</p></li><li><p><code>seed</code>: What random seed to use. <code>nothing</code> uses no seed.</p></li><li><p><code>progress</code>: Whether to use a progress bar output (<code>verbosity</code> will have no effect).</p></li><li><p><code>early_stop_condition</code>: Float - whether to stop early if the mean loss gets below this value. Function - a function taking (loss, complexity) as arguments and returning true or false.</p></li><li><p><code>timeout_in_seconds</code>: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).</p></li><li><p><code>max_evals</code>: Int (or Nothing) - the maximum number of evaluations of expressions to perform.</p></li><li><p><code>input_stream</code>: the stream to read user input from. By default, this is <code>stdin</code>. If you encounter issues with reading from <code>stdin</code>, like a hang, you can simply pass <code>devnull</code> to this argument.</p></li><li><p><code>skip_mutation_failures</code>: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated expression with the original expression and proceed normally.</p></li><li><p><code>nested_constraints</code>: Specifies how many times a combination of operators can be nested. For example, <code>[sin =&gt; [cos =&gt; 0], cos =&gt; [cos =&gt; 2]]</code> specifies that <code>cos</code> may never appear within a <code>sin</code>, but <code>sin</code> can be nested with itself an unlimited number of times. The second term specifies that <code>cos</code> can be nested up to 2 times within a <code>cos</code>, so that <code>cos(cos(cos(x)))</code> is allowed (as well as any combination of <code>+</code> or <code>-</code> within it), but <code>cos(cos(cos(cos(x))))</code> is not allowed. When an operator is not specified, it is assumed that it can be nested an unlimited number of times. This requires that there is no operator which is used both in the unary operators and the binary operators (e.g., <code>-</code> could be both subtract, and negation). For binary operators, both arguments are treated the same way, and the max of each argument is constrained.</p></li><li><p><code>deterministic</code>: Use a global counter for the birth time, rather than calls to <code>time()</code>. This gives perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used in serial mode.</p></li><li><p><code>define_helper_functions</code>: Whether to define helper functions for constructing and evaluating trees.</p></li><li><p><code>niterations::Int=10</code>: The number of iterations to perform the search. More iterations will improve the results.</p></li><li><p><code>parallelism=:multithreading</code>: What parallelism mode to use. The options are <code>:multithreading</code>, <code>:multiprocessing</code>, and <code>:serial</code>. By default, multithreading will be used. Multithreading uses less memory, but multiprocessing can handle multi-node compute. If using <code>:multithreading</code> mode, the number of threads available to julia are used. If using <code>:multiprocessing</code>, <code>numprocs</code> processes will be created dynamically if <code>procs</code> is unset. If you have already allocated processes, pass them to the <code>procs</code> argument and they will be used. You may also pass a string instead of a symbol, like <code>&quot;multithreading&quot;</code>.</p></li><li><p><code>numprocs::Union{Int, Nothing}=nothing</code>: The number of processes to use, if you want <code>equation_search</code> to set this up automatically. By default this will be <code>4</code>, but can be any number (you should pick a number &lt;= the number of cores available).</p></li><li><p><code>procs::Union{Vector{Int}, Nothing}=nothing</code>: If you have set up a distributed run manually with <code>procs = addprocs()</code> and <code>@everywhere</code>, pass the <code>procs</code> to this keyword argument.</p></li><li><p><code>addprocs_function::Union{Function, Nothing}=nothing</code>: If using multiprocessing (<code>parallelism=:multithreading</code>), and are not passing <code>procs</code> manually, then they will be allocated dynamically using <code>addprocs</code>. However, you may also pass a custom function to use instead of <code>addprocs</code>. This function should take a single positional argument, which is the number of processes to use, as well as the <code>lazy</code> keyword argument. For example, if set up on a slurm cluster, you could pass <code>addprocs_function = addprocs_slurm</code>, which will set up slurm processes.</p></li><li><p><code>heap_size_hint_in_bytes::Union{Int,Nothing}=nothing</code>: On Julia 1.9+, you may set the <code>--heap-size-hint</code> flag on Julia processes, recommending garbage collection once a process is close to the recommended size. This is important for long-running distributed jobs where each process has an independent memory, and can help avoid out-of-memory errors. By default, this is set to <code>Sys.free_memory() / numprocs</code>.</p></li><li><p><code>worker_imports::Union{Vector{Symbol},Nothing}=nothing</code>: If you want to import additional modules on each worker, pass them here as a vector of symbols. By default some of the extensions will automatically be loaded when needed.</p></li><li><p><code>runtests::Bool=true</code>: Whether to run (quick) tests before starting the search, to see if there will be any problems during the equation search related to the host environment.</p></li><li><p><code>run_id::Union{String,Nothing}=nothing</code>: A unique identifier for the run. This will be used to store outputs from the run in the <code>outputs</code> directory. If not specified, a unique ID will be generated.</p></li><li><p><code>loss_type::Type=Nothing</code>: If you would like to use a different type for the loss than for the data you passed, specify the type here. Note that if you pass complex data <code>::Complex{L}</code>, then the loss type will automatically be set to <code>L</code>.</p></li><li><p><code>selection_method::Function</code>: Function to selection expression from the Pareto frontier for use in <code>predict</code>. See <code>SymbolicRegression.MLJInterfaceModule.choose_best</code> for an example. This function should return a single integer specifying the index of the expression to use. By default, this maximizes the score (a pound-for-pound rating) of expressions reaching the threshold of 1.5x the minimum loss. To override this at prediction time, you can pass a named tuple with keys <code>data</code> and <code>idx</code> to <code>predict</code>. See the Operations section for details.</p></li><li><p><code>dimensions_type::AbstractDimensions</code>: The type of dimensions to use when storing the units of the data. By default this is <code>DynamicQuantities.SymbolicDimensions</code>.</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: Return predictions of the target given features <code>Xnew</code>, which should have same scitype as <code>X</code> above. The expression used for prediction is defined by the <code>selection_method</code> function, which can be seen by viewing <code>report(mach).best_idx</code>.</p></li><li><p><code>predict(mach, (data=Xnew, idx=i))</code>: Return predictions of the target given features <code>Xnew</code>, which should have same scitype as <code>X</code> above. By passing a named tuple with keys <code>data</code> and <code>idx</code>, you are able to specify the equation you wish to evaluate in <code>idx</code>.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>best_idx::Int</code>: The index of the best expression in the Pareto frontier, as determined by the <code>selection_method</code> function. Override in <code>predict</code> by passing a named tuple with keys <code>data</code> and <code>idx</code>.</p></li><li><p><code>equations::Vector{Node{T}}</code>: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). <code>T</code> is equal to the element type of the passed data.</p></li><li><p><code>equation_strings::Vector{String}</code>: The expressions discovered by the search, represented as strings for easy inspection.</p></li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>best_idx::Int</code>: The index of the best expression in the Pareto frontier, as determined by the <code>selection_method</code> function. Override in <code>predict</code> by passing a named tuple with keys <code>data</code> and <code>idx</code>.</p></li><li><p><code>equations::Vector{Node{T}}</code>: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity).</p></li><li><p><code>equation_strings::Vector{String}</code>: The expressions discovered by the search, represented as strings for easy inspection.</p></li><li><p><code>complexities::Vector{Int}</code>: The complexity of each expression in the Pareto frontier.</p></li><li><p><code>losses::Vector{L}</code>: The loss of each expression in the Pareto frontier, according to the loss function specified in the model. The type <code>L</code> is the loss type, which is usually the same as the element type of data passed (i.e., <code>T</code>), but can differ if complex data types are passed.</p></li><li><p><code>scores::Vector{L}</code>: A metric which considers both the complexity and loss of an expression, equal to the change in the log-loss divided by the change in complexity, relative to the previous expression along the Pareto frontier. A larger score aims to indicate an expression is more likely to be the true expression generating the data, but this is very problem-dependent and generally several other factors should be considered.</p></li></ul><p><strong>Examples</strong></p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MLJ</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">SRRegressor </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> @load</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SRRegressor pkg</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">SymbolicRegression</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X, y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> @load_boston</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> SRRegressor</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(binary_operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], unary_operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[exp], niterations</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mach </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> machine</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(model, X, y)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">fit!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mach)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y_hat </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> predict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mach, X)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># View the equation used:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">r </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> report</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mach)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">println</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Equation used:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, r</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">equation_strings[r</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">best_idx])</span></span></code></pre></div><p>With units and variable names:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MLJ</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DynamicQuantities</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">SRegressor </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> @load</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SRRegressor pkg</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">SymbolicRegression</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (; x1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> us</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;km/h&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, x2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> us</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;km&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> @.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">us</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;h&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> SRRegressor</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(binary_operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mach </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> machine</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(model, X, y)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">fit!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mach)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y_hat </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> predict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mach, X)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># View the equation used:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">r </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> report</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mach)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">println</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Equation used:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, r</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">equation_strings[r</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">best_idx])</span></span></code></pre></div><p>See also <a href="/SymbolicRegression.jl/previews/PR483/api#SymbolicRegression.MLJInterfaceModule.MultitargetSRRegressor"><code>MultitargetSRRegressor</code></a>.</p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/MLJInterface.jl#L778-L1191" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block" open><summary><a id="SymbolicRegression.MLJInterfaceModule.MultitargetSRRegressor" href="#SymbolicRegression.MLJInterfaceModule.MultitargetSRRegressor"><span class="jlbinding">SymbolicRegression.MLJInterfaceModule.MultitargetSRRegressor</span></a> <span class="VPBadge info jlObjectType jlType"><!--[-->Type<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">MultitargetSRRegressor</span></span></code></pre></div><p>A model type for constructing a Multi-Target Symbolic Regression via Evolutionary Search, based on <a href="https://github.com/MilesCranmer/SymbolicRegression.jl" target="_blank" rel="noreferrer">SymbolicRegression.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>MultitargetSRRegressor = @load MultitargetSRRegressor pkg=SymbolicRegression</span></span></code></pre></div><p>Do <code>model = MultitargetSRRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>MultitargetSRRegressor(defaults=...)</code>.</p><p>Multi-target Symbolic Regression regressor (<code>MultitargetSRRegressor</code>) conducts several searches for expressions that predict each target variable from a set of input variables. All data is assumed to be <code>Continuous</code>. The search is performed using an evolutionary algorithm. This algorithm is described in the paper <a href="https://arxiv.org/abs/2305.01582" target="_blank" rel="noreferrer">https://arxiv.org/abs/2305.01582</a>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>mach = machine(model, X, y)</span></span></code></pre></div><p>OR</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>mach = machine(model, X, y, w)</span></span></code></pre></div><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype</li></ul><p><code>Continuous</code>; check column scitypes with <code>schema(X)</code>. Variable names in discovered expressions will be taken from the column names of <code>X</code>, if available. Units in columns of <code>X</code> (use <code>DynamicQuantities</code> for units) will trigger dimensional analysis to be used.</p><ul><li><p><code>y</code> is the target, which can be any table of target variables whose element scitype is <code>Continuous</code>; check the scitype with <code>schema(y)</code>. Units in columns of <code>y</code> (use <code>DynamicQuantities</code> for units) will trigger dimensional analysis to be used.</p></li><li><p><code>w</code> is the observation weights which can either be <code>nothing</code> (default) or an <code>AbstractVector</code> whose element scitype is <code>Count</code> or <code>Continuous</code>. The same weights are used for all targets.</p></li></ul><p>Train the machine using <code>fit!(mach)</code>, inspect the discovered expressions with <code>report(mach)</code>, and predict on new data with <code>predict(mach, Xnew)</code>. Note that unlike other regressors, symbolic regression stores a list of lists of trained models. The models chosen from each of these lists is defined by the function <code>selection_method</code> keyword argument, which by default balances accuracy and complexity. You can override this at prediction time by passing a named tuple with keys <code>data</code> and <code>idx</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>defaults</code>: What set of defaults to use for <code>Options</code>. The default, <code>nothing</code>, will simply take the default options from the current version of SymbolicRegression. However, you may also select the defaults from an earlier version, such as <code>v&quot;0.24.5&quot;</code>.</p></li><li><p><code>binary_operators</code>: Vector of binary operators (functions) to use. Each operator should be defined for two input scalars, and one output scalar. All operators need to be defined over the entire real line (excluding infinity - these are stopped before they are input), or return <code>NaN</code> where not defined. For speed, define it so it takes two reals of the same type as input, and outputs the same type. For the SymbolicUtils simplification backend, you will need to define a generic method of the operator so it takes arbitrary types.</p></li><li><p><code>operator_enum_constructor</code>: Constructor function to use for creating the operators enum. By default, OperatorEnum is used, but you can provide a different constructor like GenericOperatorEnum. The constructor must accept the keyword arguments &#39;binary_operators&#39; and &#39;unary_operators&#39;.</p></li><li><p><code>unary_operators</code>: Same, but for unary operators (one input scalar, gives an output scalar).</p></li><li><p><code>constraints</code>: Array of pairs specifying size constraints for each operator. The constraints for a binary operator should be a 2-tuple (e.g., <code>(-1, -1)</code>) and the constraints for a unary operator should be an <code>Int</code>. A size constraint is a limit to the size of the subtree in each argument of an operator. e.g., <code>[(^)=&gt;(-1, 3)]</code> means that the <code>^</code> operator can have arbitrary size (<code>-1</code>) in its left argument, but a maximum size of <code>3</code> in its right argument. Default is no constraints.</p></li><li><p><code>batching</code>: Whether to evolve based on small mini-batches of data, rather than the entire dataset.</p></li><li><p><code>batch_size</code>: What batch size to use if using batching.</p></li><li><p><code>elementwise_loss</code>: What elementwise loss function to use. Can be one of the following losses, or any other loss of type <code>SupervisedLoss</code>. You can also pass a function that takes a scalar target (left argument), and scalar predicted (right argument), and returns a scalar. This will be averaged over the predicted data. If weights are supplied, your function should take a third argument for the weight scalar. Included losses: Regression: - <code>LPDistLoss{P}()</code>, - <code>L1DistLoss()</code>, - <code>L2DistLoss()</code> (mean square), - <code>LogitDistLoss()</code>, - <code>HuberLoss(d)</code>, - <code>L1EpsilonInsLoss(ϵ)</code>, - <code>L2EpsilonInsLoss(ϵ)</code>, - <code>PeriodicLoss(c)</code>, - <code>QuantileLoss(τ)</code>, Classification: - <code>ZeroOneLoss()</code>, - <code>PerceptronLoss()</code>, - <code>L1HingeLoss()</code>, - <code>SmoothedL1HingeLoss(γ)</code>, - <code>ModifiedHuberLoss()</code>, - <code>L2MarginLoss()</code>, - <code>ExpLoss()</code>, - <code>SigmoidLoss()</code>, - <code>DWDMarginLoss(q)</code>.</p></li><li><p><code>loss_function</code>: Alternatively, you may redefine the loss used as any function of <code>tree::AbstractExpressionNode{T}</code>, <code>dataset::Dataset{T}</code>, and <code>options::AbstractOptions</code>, so long as you output a non-negative scalar of type <code>T</code>. This is useful if you want to use a loss that takes into account derivatives, or correlations across the dataset. This also means you could use a custom evaluation for a particular expression. If you are using <code>batching=true</code>, then your function should accept a fourth argument <code>idx</code>, which is either <code>nothing</code> (indicating that the full dataset should be used), or a vector of indices to use for the batch. For example,</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  function my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}</span></span>
<span class="line"><span>      prediction, flag = eval_tree_array(tree, dataset.X, options)</span></span>
<span class="line"><span>      if !flag</span></span>
<span class="line"><span>          return L(Inf)</span></span>
<span class="line"><span>      end</span></span>
<span class="line"><span>      return sum((prediction .- dataset.y) .^ 2) / dataset.n</span></span>
<span class="line"><span>  end</span></span></code></pre></div></li><li><p><code>loss_function_expression</code>: Similar to <code>loss_function</code>, but takes <code>AbstractExpression</code> instead of <code>AbstractExpressionNode</code> as its first argument. Useful for <code>TemplateExpressionSpec</code>.</p></li><li><p><code>loss_scale</code>: Determines how loss values are scaled when computing scores. Options are:</p><ul><li><p><code>:log</code> (default): Uses logarithmic scaling of loss ratios. This mode requires non-negative loss values and is ideal for traditional loss functions that are always positive.</p></li><li><p><code>:linear</code>: Uses direct differences between losses. This mode handles any loss values (including negative) and is useful for custom loss functions, especially those based on likelihoods.</p></li></ul></li><li><p><code>expression_spec::AbstractExpressionSpec</code>: A specification of what types of expressions to use in the search. For example, <code>ExpressionSpec()</code> (default). You can also see <code>TemplateExpressionSpec</code> and <code>ParametricExpressionSpec</code> for specialized cases.</p></li><li><p><code>populations</code>: How many populations of equations to use.</p></li><li><p><code>population_size</code>: How many equations in each population.</p></li><li><p><code>ncycles_per_iteration</code>: How many generations to consider per iteration.</p></li><li><p><code>tournament_selection_n</code>: Number of expressions considered in each tournament.</p></li><li><p><code>tournament_selection_p</code>: The fittest expression in a tournament is to be selected with probability <code>p</code>, the next fittest with probability <code>p*(1-p)</code>, and so forth.</p></li><li><p><code>topn</code>: Number of equations to return to the host process, and to consider for the hall of fame.</p></li><li><p><code>complexity_of_operators</code>: What complexity should be assigned to each operator, and the occurrence of a constant or variable. By default, this is 1 for all operators. Can be a real number as well, in which case the complexity of an expression will be rounded to the nearest integer. Input this in the form of, e.g., [(^) =&gt; 3, sin =&gt; 2].</p></li><li><p><code>complexity_of_constants</code>: What complexity should be assigned to use of a constant. By default, this is 1.</p></li><li><p><code>complexity_of_variables</code>: What complexity should be assigned to use of a variable, which can also be a vector indicating different per-variable complexity. By default, this is 1.</p></li><li><p><code>complexity_mapping</code>: Alternatively, you can pass a function that takes the expression as input and returns the complexity. Make sure that this operates on <code>AbstractExpression</code> (and unpacks to <code>AbstractExpressionNode</code>), and returns an integer.</p></li><li><p><code>alpha</code>: The probability of accepting an equation mutation during regularized evolution is given by exp(-delta_loss/(alpha * T)), where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.</p></li><li><p><code>maxsize</code>: Maximum size of equations during the search.</p></li><li><p><code>maxdepth</code>: Maximum depth of equations during the search, by default this is set equal to the maxsize.</p></li><li><p><code>parsimony</code>: A multiplicative factor for how much complexity is punished.</p></li><li><p><code>dimensional_constraint_penalty</code>: An additive factor if the dimensional constraint is violated.</p></li><li><p><code>dimensionless_constants_only</code>: Whether to only allow dimensionless constants.</p></li><li><p><code>use_frequency</code>: Whether to use a parsimony that adapts to the relative proportion of equations at each complexity; this will ensure that there are a balanced number of equations considered for every complexity.</p></li><li><p><code>use_frequency_in_tournament</code>: Whether to use the adaptive parsimony described above inside the score, rather than just at the mutation accept/reject stage.</p></li><li><p><code>adaptive_parsimony_scaling</code>: How much to scale the adaptive parsimony term in the loss. Increase this if the search is spending too much time optimizing the most complex equations.</p></li><li><p><code>turbo</code>: Whether to use <code>LoopVectorization.@turbo</code> to evaluate expressions. This can be significantly faster, but is only compatible with certain operators. <em>Experimental!</em></p></li><li><p><code>bumper</code>: Whether to use Bumper.jl for faster evaluation. <em>Experimental!</em></p></li><li><p><code>migration</code>: Whether to migrate equations between processes.</p></li><li><p><code>hof_migration</code>: Whether to migrate equations from the hall of fame to processes.</p></li><li><p><code>fraction_replaced</code>: What fraction of each population to replace with migrated equations at the end of each cycle.</p></li><li><p><code>fraction_replaced_hof</code>: What fraction to replace with hall of fame equations at the end of each cycle.</p></li><li><p><code>fraction_replaced_guesses</code>: What fraction to replace with user-provided guess expressions at the end of each cycle.</p></li><li><p><code>should_simplify</code>: Whether to simplify equations. If you pass a custom objective, this will be set to <code>false</code>.</p></li><li><p><code>should_optimize_constants</code>: Whether to use an optimization algorithm to periodically optimize constants in equations.</p></li><li><p><code>optimizer_algorithm</code>: Select algorithm to use for optimizing constants. Default is <code>Optim.BFGS(linesearch=LineSearches.BackTracking())</code>.</p></li><li><p><code>optimizer_nrestarts</code>: How many different random starting positions to consider for optimization of constants.</p></li><li><p><code>optimizer_probability</code>: Probability of performing optimization of constants at the end of a given iteration.</p></li><li><p><code>optimizer_iterations</code>: How many optimization iterations to perform. This gets passed to <code>Optim.Options</code> as <code>iterations</code>. The default is 8.</p></li><li><p><code>optimizer_f_calls_limit</code>: How many function calls to allow during optimization. This gets passed to <code>Optim.Options</code> as <code>f_calls_limit</code>. The default is <code>10_000</code>.</p></li><li><p><code>optimizer_options</code>: General options for the constant optimization. For details we refer to the documentation on <code>Optim.Options</code> from the <code>Optim.jl</code> package. Options can be provided here as <code>NamedTuple</code>, e.g. <code>(iterations=16,)</code>, as a <code>Dict</code>, e.g. Dict(:x_tol =&gt; 1.0e-32,), or as an <code>Optim.Options</code> instance.</p></li><li><p><code>autodiff_backend</code>: The backend to use for differentiation, which should be an instance of <code>AbstractADType</code> (see <code>ADTypes.jl</code>). Default is <code>nothing</code>, which means <code>Optim.jl</code> will estimate gradients (likely with finite differences). You can also pass a symbolic version of the backend type, such as <code>:Zygote</code> for Zygote.jl or <code>:Mooncake</code> for Mooncake.jl. Most backends will not work, and many will never work due to incompatibilities, though support for some is gradually being added.</p></li><li><p><code>perturbation_factor</code>: When mutating a constant, either multiply or divide by (1+perturbation_factor)^(rand()+1).</p></li><li><p><code>probability_negate_constant</code>: Probability of negating a constant in the equation when mutating it.</p></li><li><p><code>mutation_weights</code>: Relative probabilities of the mutations. The struct <code>MutationWeights</code> (or any <code>AbstractMutationWeights</code>) should be passed to these options. See its documentation on <code>MutationWeights</code> for the different weights.</p></li><li><p><code>crossover_probability</code>: Probability of performing crossover.</p></li><li><p><code>annealing</code>: Whether to use simulated annealing.</p></li><li><p><code>warmup_maxsize_by</code>: Whether to slowly increase the max size from 5 up to <code>maxsize</code>. If nonzero, specifies the fraction through the search at which the maxsize should be reached.</p></li><li><p><code>verbosity</code>: Whether to print debugging statements or not.</p></li><li><p><code>print_precision</code>: How many digits to print when printing equations. By default, this is 5.</p></li><li><p><code>output_directory</code>: The base directory to save output files to. Files will be saved in a subdirectory according to the run ID. By default, this is <code>./outputs</code>.</p></li><li><p><code>save_to_file</code>: Whether to save equations to a file during the search.</p></li><li><p><code>bin_constraints</code>: See <code>constraints</code>. This is the same, but specified for binary operators only (for example, if you have an operator that is both a binary and unary operator).</p></li><li><p><code>una_constraints</code>: Likewise, for unary operators.</p></li><li><p><code>seed</code>: What random seed to use. <code>nothing</code> uses no seed.</p></li><li><p><code>progress</code>: Whether to use a progress bar output (<code>verbosity</code> will have no effect).</p></li><li><p><code>early_stop_condition</code>: Float - whether to stop early if the mean loss gets below this value. Function - a function taking (loss, complexity) as arguments and returning true or false.</p></li><li><p><code>timeout_in_seconds</code>: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).</p></li><li><p><code>max_evals</code>: Int (or Nothing) - the maximum number of evaluations of expressions to perform.</p></li><li><p><code>input_stream</code>: the stream to read user input from. By default, this is <code>stdin</code>. If you encounter issues with reading from <code>stdin</code>, like a hang, you can simply pass <code>devnull</code> to this argument.</p></li><li><p><code>skip_mutation_failures</code>: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated expression with the original expression and proceed normally.</p></li><li><p><code>nested_constraints</code>: Specifies how many times a combination of operators can be nested. For example, <code>[sin =&gt; [cos =&gt; 0], cos =&gt; [cos =&gt; 2]]</code> specifies that <code>cos</code> may never appear within a <code>sin</code>, but <code>sin</code> can be nested with itself an unlimited number of times. The second term specifies that <code>cos</code> can be nested up to 2 times within a <code>cos</code>, so that <code>cos(cos(cos(x)))</code> is allowed (as well as any combination of <code>+</code> or <code>-</code> within it), but <code>cos(cos(cos(cos(x))))</code> is not allowed. When an operator is not specified, it is assumed that it can be nested an unlimited number of times. This requires that there is no operator which is used both in the unary operators and the binary operators (e.g., <code>-</code> could be both subtract, and negation). For binary operators, both arguments are treated the same way, and the max of each argument is constrained.</p></li><li><p><code>deterministic</code>: Use a global counter for the birth time, rather than calls to <code>time()</code>. This gives perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used in serial mode.</p></li><li><p><code>define_helper_functions</code>: Whether to define helper functions for constructing and evaluating trees.</p></li><li><p><code>niterations::Int=10</code>: The number of iterations to perform the search. More iterations will improve the results.</p></li><li><p><code>parallelism=:multithreading</code>: What parallelism mode to use. The options are <code>:multithreading</code>, <code>:multiprocessing</code>, and <code>:serial</code>. By default, multithreading will be used. Multithreading uses less memory, but multiprocessing can handle multi-node compute. If using <code>:multithreading</code> mode, the number of threads available to julia are used. If using <code>:multiprocessing</code>, <code>numprocs</code> processes will be created dynamically if <code>procs</code> is unset. If you have already allocated processes, pass them to the <code>procs</code> argument and they will be used. You may also pass a string instead of a symbol, like <code>&quot;multithreading&quot;</code>.</p></li><li><p><code>numprocs::Union{Int, Nothing}=nothing</code>: The number of processes to use, if you want <code>equation_search</code> to set this up automatically. By default this will be <code>4</code>, but can be any number (you should pick a number &lt;= the number of cores available).</p></li><li><p><code>procs::Union{Vector{Int}, Nothing}=nothing</code>: If you have set up a distributed run manually with <code>procs = addprocs()</code> and <code>@everywhere</code>, pass the <code>procs</code> to this keyword argument.</p></li><li><p><code>addprocs_function::Union{Function, Nothing}=nothing</code>: If using multiprocessing (<code>parallelism=:multithreading</code>), and are not passing <code>procs</code> manually, then they will be allocated dynamically using <code>addprocs</code>. However, you may also pass a custom function to use instead of <code>addprocs</code>. This function should take a single positional argument, which is the number of processes to use, as well as the <code>lazy</code> keyword argument. For example, if set up on a slurm cluster, you could pass <code>addprocs_function = addprocs_slurm</code>, which will set up slurm processes.</p></li><li><p><code>heap_size_hint_in_bytes::Union{Int,Nothing}=nothing</code>: On Julia 1.9+, you may set the <code>--heap-size-hint</code> flag on Julia processes, recommending garbage collection once a process is close to the recommended size. This is important for long-running distributed jobs where each process has an independent memory, and can help avoid out-of-memory errors. By default, this is set to <code>Sys.free_memory() / numprocs</code>.</p></li><li><p><code>worker_imports::Union{Vector{Symbol},Nothing}=nothing</code>: If you want to import additional modules on each worker, pass them here as a vector of symbols. By default some of the extensions will automatically be loaded when needed.</p></li><li><p><code>runtests::Bool=true</code>: Whether to run (quick) tests before starting the search, to see if there will be any problems during the equation search related to the host environment.</p></li><li><p><code>run_id::Union{String,Nothing}=nothing</code>: A unique identifier for the run. This will be used to store outputs from the run in the <code>outputs</code> directory. If not specified, a unique ID will be generated.</p></li><li><p><code>loss_type::Type=Nothing</code>: If you would like to use a different type for the loss than for the data you passed, specify the type here. Note that if you pass complex data <code>::Complex{L}</code>, then the loss type will automatically be set to <code>L</code>.</p></li><li><p><code>selection_method::Function</code>: Function to selection expression from the Pareto frontier for use in <code>predict</code>. See <code>SymbolicRegression.MLJInterfaceModule.choose_best</code> for an example. This function should return a single integer specifying the index of the expression to use. By default, this maximizes the score (a pound-for-pound rating) of expressions reaching the threshold of 1.5x the minimum loss. To override this at prediction time, you can pass a named tuple with keys <code>data</code> and <code>idx</code> to <code>predict</code>. See the Operations section for details.</p></li><li><p><code>dimensions_type::AbstractDimensions</code>: The type of dimensions to use when storing the units of the data. By default this is <code>DynamicQuantities.SymbolicDimensions</code>.</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: Return predictions of the target given features <code>Xnew</code>, which should have same scitype as <code>X</code> above. The expression used for prediction is defined by the <code>selection_method</code> function, which can be seen by viewing <code>report(mach).best_idx</code>.</p></li><li><p><code>predict(mach, (data=Xnew, idx=i))</code>: Return predictions of the target given features <code>Xnew</code>, which should have same scitype as <code>X</code> above. By passing a named tuple with keys <code>data</code> and <code>idx</code>, you are able to specify the equation you wish to evaluate in <code>idx</code>.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>best_idx::Vector{Int}</code>: The index of the best expression in each Pareto frontier, as determined by the <code>selection_method</code> function. Override in <code>predict</code> by passing a named tuple with keys <code>data</code> and <code>idx</code>.</p></li><li><p><code>equations::Vector{Vector{Node{T}}}</code>: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). The outer vector is indexed by target variable, and the inner vector is ordered by increasing complexity. <code>T</code> is equal to the element type of the passed data.</p></li><li><p><code>equation_strings::Vector{Vector{String}}</code>: The expressions discovered by the search, represented as strings for easy inspection.</p></li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>best_idx::Vector{Int}</code>: The index of the best expression in each Pareto frontier, as determined by the <code>selection_method</code> function. Override in <code>predict</code> by passing a named tuple with keys <code>data</code> and <code>idx</code>.</p></li><li><p><code>equations::Vector{Vector{Node{T}}}</code>: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). The outer vector is indexed by target variable, and the inner vector is ordered by increasing complexity.</p></li><li><p><code>equation_strings::Vector{Vector{String}}</code>: The expressions discovered by the search, represented as strings for easy inspection.</p></li><li><p><code>complexities::Vector{Vector{Int}}</code>: The complexity of each expression in each Pareto frontier.</p></li><li><p><code>losses::Vector{Vector{L}}</code>: The loss of each expression in each Pareto frontier, according to the loss function specified in the model. The type <code>L</code> is the loss type, which is usually the same as the element type of data passed (i.e., <code>T</code>), but can differ if complex data types are passed.</p></li><li><p><code>scores::Vector{Vector{L}}</code>: A metric which considers both the complexity and loss of an expression, equal to the change in the log-loss divided by the change in complexity, relative to the previous expression along the Pareto frontier. A larger score aims to indicate an expression is more likely to be the true expression generating the data, but this is very problem-dependent and generally several other factors should be considered.</p></li></ul><p><strong>Examples</strong></p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MLJ</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">MultitargetSRRegressor </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> @load</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MultitargetSRRegressor pkg</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">SymbolicRegression</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (a</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), b</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), c</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (y1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> cos</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">c) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2.1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> -</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.9</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), y2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">c))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> MultitargetSRRegressor</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(binary_operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], unary_operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[exp], niterations</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mach </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> machine</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(model, X, Y)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">fit!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mach)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y_hat </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> predict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mach, X)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># View the equations used:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">r </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> report</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mach)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (output_index, (eq, i)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">zip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(r</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">equation_strings, r</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">best_idx))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    println</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Equation used for &quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, output_index, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;: &quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, eq[i])</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><p>See also <a href="/SymbolicRegression.jl/previews/PR483/api#SymbolicRegression.MLJInterfaceModule.SRRegressor"><code>SRRegressor</code></a>.</p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/MLJInterface.jl#L778-L1178" target="_blank" rel="noreferrer">source</a><!--]--></span></details><h2 id="Low-Level-API" tabindex="-1">Low-Level API <a class="header-anchor" href="#Low-Level-API" aria-label="Permalink to &quot;Low-Level API {#Low-Level-API}&quot;">​</a></h2><details class="jldocstring custom-block" open><summary><a id="SymbolicRegression.equation_search" href="#SymbolicRegression.equation_search"><span class="jlbinding">SymbolicRegression.equation_search</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">equation_search</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(X, y[; kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span></code></pre></div><p>Perform a distributed equation search for functions <code>f_i</code> which describe the mapping <code>f_i(X[:, j]) ≈ y[i, j]</code>. Options are configured using SymbolicRegression.Options(...), which should be passed as a keyword argument to options. One can turn off parallelism with <code>numprocs=0</code>, which is useful for debugging and profiling.</p><p><strong>Arguments</strong></p><ul><li><p><code>X::AbstractMatrix{T}</code>: The input dataset to predict <code>y</code> from. The first dimension is features, the second dimension is rows.</p></li><li><p><code>y::Union{AbstractMatrix{T}, AbstractVector{T}}</code>: The values to predict. The first dimension is the output feature to predict with each equation, and the second dimension is rows.</p></li><li><p><code>niterations::Int=100</code>: The number of iterations to perform the search. More iterations will improve the results.</p></li><li><p><code>weights::Union{AbstractMatrix{T}, AbstractVector{T}, Nothing}=nothing</code>: Optionally weight the loss for each <code>y</code> by this value (same shape as <code>y</code>).</p></li><li><p><code>options::AbstractOptions=Options()</code>: The options for the search, such as which operators to use, evolution hyperparameters, etc.</p></li><li><p><code>variable_names::Union{Vector{String}, Nothing}=nothing</code>: The names of each feature in <code>X</code>, which will be used during printing of equations.</p></li><li><p><code>display_variable_names::Union{Vector{String}, Nothing}=variable_names</code>: Names to use when printing expressions during the search, but not when saving to an equation file.</p></li><li><p><code>y_variable_names::Union{String,AbstractVector{String},Nothing}=nothing</code>: The names of each output feature in <code>y</code>, which will be used during printing of equations.</p></li><li><p><code>parallelism=:multithreading</code>: What parallelism mode to use. The options are <code>:multithreading</code>, <code>:multiprocessing</code>, and <code>:serial</code>. By default, multithreading will be used. Multithreading uses less memory, but multiprocessing can handle multi-node compute. If using <code>:multithreading</code> mode, the number of threads available to julia are used. If using <code>:multiprocessing</code>, <code>numprocs</code> processes will be created dynamically if <code>procs</code> is unset. If you have already allocated processes, pass them to the <code>procs</code> argument and they will be used. You may also pass a string instead of a symbol, like <code>&quot;multithreading&quot;</code>.</p></li><li><p><code>numprocs::Union{Int, Nothing}=nothing</code>: The number of processes to use, if you want <code>equation_search</code> to set this up automatically. By default this will be <code>4</code>, but can be any number (you should pick a number &lt;= the number of cores available).</p></li><li><p><code>procs::Union{Vector{Int}, Nothing}=nothing</code>: If you have set up a distributed run manually with <code>procs = addprocs()</code> and <code>@everywhere</code>, pass the <code>procs</code> to this keyword argument.</p></li><li><p><code>addprocs_function::Union{Function, Nothing}=nothing</code>: If using multiprocessing (<code>parallelism=:multiprocessing</code>), and are not passing <code>procs</code> manually, then they will be allocated dynamically using <code>addprocs</code>. However, you may also pass a custom function to use instead of <code>addprocs</code>. This function should take a single positional argument, which is the number of processes to use, as well as the <code>lazy</code> keyword argument. For example, if set up on a slurm cluster, you could pass <code>addprocs_function = addprocs_slurm</code>, which will set up slurm processes.</p></li><li><p><code>heap_size_hint_in_bytes::Union{Int,Nothing}=nothing</code>: On Julia 1.9+, you may set the <code>--heap-size-hint</code> flag on Julia processes, recommending garbage collection once a process is close to the recommended size. This is important for long-running distributed jobs where each process has an independent memory, and can help avoid out-of-memory errors. By default, this is set to <code>Sys.free_memory() / numprocs</code>.</p></li><li><p><code>worker_imports::Union{Vector{Symbol},Nothing}=nothing</code>: If you want to import additional modules on each worker, pass them here as a vector of symbols. By default some of the extensions will automatically be loaded when needed.</p></li><li><p><code>runtests::Bool=true</code>: Whether to run (quick) tests before starting the search, to see if there will be any problems during the equation search related to the host environment.</p></li><li><p><code>saved_state=nothing</code>: If you have already run <code>equation_search</code> and want to resume it, pass the state here. To get this to work, you need to have set return_state=true, which will cause <code>equation_search</code> to return the state. The second element of the state is the regular return value with the hall of fame. Note that you cannot change the operators or dataset, but most other options should be changeable.</p></li><li><p><code>return_state::Union{Bool, Nothing}=nothing</code>: Whether to return the state of the search for warm starts. By default this is false.</p></li><li><p><code>loss_type::Type=Nothing</code>: If you would like to use a different type for the loss than for the data you passed, specify the type here. Note that if you pass complex data <code>::Complex{L}</code>, then the loss type will automatically be set to <code>L</code>.</p></li><li><p><code>verbosity</code>: Whether to print debugging statements or not.</p></li><li><p><code>logger::Union{AbstractSRLogger,Nothing}=nothing</code>: An optional logger to record the progress of the search. You can use an <code>SRLogger</code> to wrap a custom logger, or pass <code>nothing</code> to disable logging.</p></li><li><p><code>progress</code>: Whether to use a progress bar output. Only available for single target output.</p></li><li><p><code>X_units::Union{AbstractVector,Nothing}=nothing</code>: The units of the dataset, to be used for dimensional constraints. For example, if <code>X_units=[&quot;kg&quot;, &quot;m&quot;]</code>, then the first feature will have units of kilograms, and the second will have units of meters.</p></li><li><p><code>y_units=nothing</code>: The units of the output, to be used for dimensional constraints. If <code>y</code> is a matrix, then this can be a vector of units, in which case each element corresponds to each output feature.</p></li><li><p><code>guesses::Union{AbstractVector,AbstractVector{&lt;:AbstractVector},Nothing}=nothing</code>: Initial guess equations to seed the search. Examples:</p><ul><li><p>Single output: <code>[&quot;x1^2 + x2&quot;, &quot;sin(x1) * x2&quot;]</code></p></li><li><p>Multi-output: <code>[[&quot;x1 + x2&quot;], [&quot;x1 * x2&quot;, &quot;x1 - x2&quot;]]</code></p></li></ul><p>Constants will be automatically optimized.</p></li></ul><p><strong>Returns</strong></p><ul><li><code>hallOfFame::HallOfFame</code>: The best equations seen during the search. hallOfFame.members gives an array of <code>PopMember</code> objects, which have their tree (equation) stored in <code>.tree</code>. Their loss is given in <code>.loss</code>. The array of <code>PopMember</code> objects is enumerated by size from <code>1</code> to <code>options.maxsize</code>.</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/SymbolicRegression.jl#L355-L458" target="_blank" rel="noreferrer">source</a><!--]--></span></details><h2 id="Options" tabindex="-1">Options <a class="header-anchor" href="#Options" aria-label="Permalink to &quot;Options {#Options}&quot;">​</a></h2><details class="jldocstring custom-block" open><summary><a id="SymbolicRegression.CoreModule.OptionsStructModule.Options" href="#SymbolicRegression.CoreModule.OptionsStructModule.Options"><span class="jlbinding">SymbolicRegression.CoreModule.OptionsStructModule.Options</span></a> <span class="VPBadge info jlObjectType jlType"><!--[-->Type<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Options</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(;kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractOptions</span></span></code></pre></div><p>Construct options for <code>equation_search</code> and other functions. The current arguments have been tuned using the median values from <a href="https://github.com/MilesCranmer/PySR/discussions/115" target="_blank" rel="noreferrer">https://github.com/MilesCranmer/PySR/discussions/115</a>.</p><p><strong>Arguments</strong></p><ul><li><p><code>defaults</code>: What set of defaults to use for <code>Options</code>. The default, <code>nothing</code>, will simply take the default options from the current version of SymbolicRegression. However, you may also select the defaults from an earlier version, such as <code>v&quot;0.24.5&quot;</code>.</p></li><li><p><code>binary_operators</code>: Vector of binary operators (functions) to use. Each operator should be defined for two input scalars, and one output scalar. All operators need to be defined over the entire real line (excluding infinity - these are stopped before they are input), or return <code>NaN</code> where not defined. For speed, define it so it takes two reals of the same type as input, and outputs the same type. For the SymbolicUtils simplification backend, you will need to define a generic method of the operator so it takes arbitrary types.</p></li><li><p><code>operator_enum_constructor</code>: Constructor function to use for creating the operators enum. By default, OperatorEnum is used, but you can provide a different constructor like GenericOperatorEnum. The constructor must accept the keyword arguments &#39;binary_operators&#39; and &#39;unary_operators&#39;.</p></li><li><p><code>unary_operators</code>: Same, but for unary operators (one input scalar, gives an output scalar).</p></li><li><p><code>constraints</code>: Array of pairs specifying size constraints for each operator. The constraints for a binary operator should be a 2-tuple (e.g., <code>(-1, -1)</code>) and the constraints for a unary operator should be an <code>Int</code>. A size constraint is a limit to the size of the subtree in each argument of an operator. e.g., <code>[(^)=&gt;(-1, 3)]</code> means that the <code>^</code> operator can have arbitrary size (<code>-1</code>) in its left argument, but a maximum size of <code>3</code> in its right argument. Default is no constraints.</p></li><li><p><code>batching</code>: Whether to evolve based on small mini-batches of data, rather than the entire dataset.</p></li><li><p><code>batch_size</code>: What batch size to use if using batching.</p></li><li><p><code>elementwise_loss</code>: What elementwise loss function to use. Can be one of the following losses, or any other loss of type <code>SupervisedLoss</code>. You can also pass a function that takes a scalar target (left argument), and scalar predicted (right argument), and returns a scalar. This will be averaged over the predicted data. If weights are supplied, your function should take a third argument for the weight scalar. Included losses: Regression: - <code>LPDistLoss{P}()</code>, - <code>L1DistLoss()</code>, - <code>L2DistLoss()</code> (mean square), - <code>LogitDistLoss()</code>, - <code>HuberLoss(d)</code>, - <code>L1EpsilonInsLoss(ϵ)</code>, - <code>L2EpsilonInsLoss(ϵ)</code>, - <code>PeriodicLoss(c)</code>, - <code>QuantileLoss(τ)</code>, Classification: - <code>ZeroOneLoss()</code>, - <code>PerceptronLoss()</code>, - <code>L1HingeLoss()</code>, - <code>SmoothedL1HingeLoss(γ)</code>, - <code>ModifiedHuberLoss()</code>, - <code>L2MarginLoss()</code>, - <code>ExpLoss()</code>, - <code>SigmoidLoss()</code>, - <code>DWDMarginLoss(q)</code>.</p></li><li><p><code>loss_function</code>: Alternatively, you may redefine the loss used as any function of <code>tree::AbstractExpressionNode{T}</code>, <code>dataset::Dataset{T}</code>, and <code>options::AbstractOptions</code>, so long as you output a non-negative scalar of type <code>T</code>. This is useful if you want to use a loss that takes into account derivatives, or correlations across the dataset. This also means you could use a custom evaluation for a particular expression. If you are using <code>batching=true</code>, then your function should accept a fourth argument <code>idx</code>, which is either <code>nothing</code> (indicating that the full dataset should be used), or a vector of indices to use for the batch. For example,</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  function my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}</span></span>
<span class="line"><span>      prediction, flag = eval_tree_array(tree, dataset.X, options)</span></span>
<span class="line"><span>      if !flag</span></span>
<span class="line"><span>          return L(Inf)</span></span>
<span class="line"><span>      end</span></span>
<span class="line"><span>      return sum((prediction .- dataset.y) .^ 2) / dataset.n</span></span>
<span class="line"><span>  end</span></span></code></pre></div></li><li><p><code>loss_function_expression</code>: Similar to <code>loss_function</code>, but takes <code>AbstractExpression</code> instead of <code>AbstractExpressionNode</code> as its first argument. Useful for <code>TemplateExpressionSpec</code>.</p></li><li><p><code>loss_scale</code>: Determines how loss values are scaled when computing scores. Options are:</p><ul><li><p><code>:log</code> (default): Uses logarithmic scaling of loss ratios. This mode requires non-negative loss values and is ideal for traditional loss functions that are always positive.</p></li><li><p><code>:linear</code>: Uses direct differences between losses. This mode handles any loss values (including negative) and is useful for custom loss functions, especially those based on likelihoods.</p></li></ul></li><li><p><code>expression_spec::AbstractExpressionSpec</code>: A specification of what types of expressions to use in the search. For example, <code>ExpressionSpec()</code> (default). You can also see <code>TemplateExpressionSpec</code> and <code>ParametricExpressionSpec</code> for specialized cases.</p></li><li><p><code>populations</code>: How many populations of equations to use.</p></li><li><p><code>population_size</code>: How many equations in each population.</p></li><li><p><code>ncycles_per_iteration</code>: How many generations to consider per iteration.</p></li><li><p><code>tournament_selection_n</code>: Number of expressions considered in each tournament.</p></li><li><p><code>tournament_selection_p</code>: The fittest expression in a tournament is to be selected with probability <code>p</code>, the next fittest with probability <code>p*(1-p)</code>, and so forth.</p></li><li><p><code>topn</code>: Number of equations to return to the host process, and to consider for the hall of fame.</p></li><li><p><code>complexity_of_operators</code>: What complexity should be assigned to each operator, and the occurrence of a constant or variable. By default, this is 1 for all operators. Can be a real number as well, in which case the complexity of an expression will be rounded to the nearest integer. Input this in the form of, e.g., [(^) =&gt; 3, sin =&gt; 2].</p></li><li><p><code>complexity_of_constants</code>: What complexity should be assigned to use of a constant. By default, this is 1.</p></li><li><p><code>complexity_of_variables</code>: What complexity should be assigned to use of a variable, which can also be a vector indicating different per-variable complexity. By default, this is 1.</p></li><li><p><code>complexity_mapping</code>: Alternatively, you can pass a function that takes the expression as input and returns the complexity. Make sure that this operates on <code>AbstractExpression</code> (and unpacks to <code>AbstractExpressionNode</code>), and returns an integer.</p></li><li><p><code>alpha</code>: The probability of accepting an equation mutation during regularized evolution is given by exp(-delta_loss/(alpha * T)), where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.</p></li><li><p><code>maxsize</code>: Maximum size of equations during the search.</p></li><li><p><code>maxdepth</code>: Maximum depth of equations during the search, by default this is set equal to the maxsize.</p></li><li><p><code>parsimony</code>: A multiplicative factor for how much complexity is punished.</p></li><li><p><code>dimensional_constraint_penalty</code>: An additive factor if the dimensional constraint is violated.</p></li><li><p><code>dimensionless_constants_only</code>: Whether to only allow dimensionless constants.</p></li><li><p><code>use_frequency</code>: Whether to use a parsimony that adapts to the relative proportion of equations at each complexity; this will ensure that there are a balanced number of equations considered for every complexity.</p></li><li><p><code>use_frequency_in_tournament</code>: Whether to use the adaptive parsimony described above inside the score, rather than just at the mutation accept/reject stage.</p></li><li><p><code>adaptive_parsimony_scaling</code>: How much to scale the adaptive parsimony term in the loss. Increase this if the search is spending too much time optimizing the most complex equations.</p></li><li><p><code>turbo</code>: Whether to use <code>LoopVectorization.@turbo</code> to evaluate expressions. This can be significantly faster, but is only compatible with certain operators. <em>Experimental!</em></p></li><li><p><code>bumper</code>: Whether to use Bumper.jl for faster evaluation. <em>Experimental!</em></p></li><li><p><code>migration</code>: Whether to migrate equations between processes.</p></li><li><p><code>hof_migration</code>: Whether to migrate equations from the hall of fame to processes.</p></li><li><p><code>fraction_replaced</code>: What fraction of each population to replace with migrated equations at the end of each cycle.</p></li><li><p><code>fraction_replaced_hof</code>: What fraction to replace with hall of fame equations at the end of each cycle.</p></li><li><p><code>fraction_replaced_guesses</code>: What fraction to replace with user-provided guess expressions at the end of each cycle.</p></li><li><p><code>should_simplify</code>: Whether to simplify equations. If you pass a custom objective, this will be set to <code>false</code>.</p></li><li><p><code>should_optimize_constants</code>: Whether to use an optimization algorithm to periodically optimize constants in equations.</p></li><li><p><code>optimizer_algorithm</code>: Select algorithm to use for optimizing constants. Default is <code>Optim.BFGS(linesearch=LineSearches.BackTracking())</code>.</p></li><li><p><code>optimizer_nrestarts</code>: How many different random starting positions to consider for optimization of constants.</p></li><li><p><code>optimizer_probability</code>: Probability of performing optimization of constants at the end of a given iteration.</p></li><li><p><code>optimizer_iterations</code>: How many optimization iterations to perform. This gets passed to <code>Optim.Options</code> as <code>iterations</code>. The default is 8.</p></li><li><p><code>optimizer_f_calls_limit</code>: How many function calls to allow during optimization. This gets passed to <code>Optim.Options</code> as <code>f_calls_limit</code>. The default is <code>10_000</code>.</p></li><li><p><code>optimizer_options</code>: General options for the constant optimization. For details we refer to the documentation on <code>Optim.Options</code> from the <code>Optim.jl</code> package. Options can be provided here as <code>NamedTuple</code>, e.g. <code>(iterations=16,)</code>, as a <code>Dict</code>, e.g. Dict(:x_tol =&gt; 1.0e-32,), or as an <code>Optim.Options</code> instance.</p></li><li><p><code>autodiff_backend</code>: The backend to use for differentiation, which should be an instance of <code>AbstractADType</code> (see <code>ADTypes.jl</code>). Default is <code>nothing</code>, which means <code>Optim.jl</code> will estimate gradients (likely with finite differences). You can also pass a symbolic version of the backend type, such as <code>:Zygote</code> for Zygote.jl or <code>:Mooncake</code> for Mooncake.jl. Most backends will not work, and many will never work due to incompatibilities, though support for some is gradually being added.</p></li><li><p><code>perturbation_factor</code>: When mutating a constant, either multiply or divide by (1+perturbation_factor)^(rand()+1).</p></li><li><p><code>probability_negate_constant</code>: Probability of negating a constant in the equation when mutating it.</p></li><li><p><code>mutation_weights</code>: Relative probabilities of the mutations. The struct <code>MutationWeights</code> (or any <code>AbstractMutationWeights</code>) should be passed to these options. See its documentation on <code>MutationWeights</code> for the different weights.</p></li><li><p><code>crossover_probability</code>: Probability of performing crossover.</p></li><li><p><code>annealing</code>: Whether to use simulated annealing.</p></li><li><p><code>warmup_maxsize_by</code>: Whether to slowly increase the max size from 5 up to <code>maxsize</code>. If nonzero, specifies the fraction through the search at which the maxsize should be reached.</p></li><li><p><code>verbosity</code>: Whether to print debugging statements or not.</p></li><li><p><code>print_precision</code>: How many digits to print when printing equations. By default, this is 5.</p></li><li><p><code>output_directory</code>: The base directory to save output files to. Files will be saved in a subdirectory according to the run ID. By default, this is <code>./outputs</code>.</p></li><li><p><code>save_to_file</code>: Whether to save equations to a file during the search.</p></li><li><p><code>bin_constraints</code>: See <code>constraints</code>. This is the same, but specified for binary operators only (for example, if you have an operator that is both a binary and unary operator).</p></li><li><p><code>una_constraints</code>: Likewise, for unary operators.</p></li><li><p><code>seed</code>: What random seed to use. <code>nothing</code> uses no seed.</p></li><li><p><code>progress</code>: Whether to use a progress bar output (<code>verbosity</code> will have no effect).</p></li><li><p><code>early_stop_condition</code>: Float - whether to stop early if the mean loss gets below this value. Function - a function taking (loss, complexity) as arguments and returning true or false.</p></li><li><p><code>timeout_in_seconds</code>: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).</p></li><li><p><code>max_evals</code>: Int (or Nothing) - the maximum number of evaluations of expressions to perform.</p></li><li><p><code>input_stream</code>: the stream to read user input from. By default, this is <code>stdin</code>. If you encounter issues with reading from <code>stdin</code>, like a hang, you can simply pass <code>devnull</code> to this argument.</p></li><li><p><code>skip_mutation_failures</code>: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated expression with the original expression and proceed normally.</p></li><li><p><code>nested_constraints</code>: Specifies how many times a combination of operators can be nested. For example, <code>[sin =&gt; [cos =&gt; 0], cos =&gt; [cos =&gt; 2]]</code> specifies that <code>cos</code> may never appear within a <code>sin</code>, but <code>sin</code> can be nested with itself an unlimited number of times. The second term specifies that <code>cos</code> can be nested up to 2 times within a <code>cos</code>, so that <code>cos(cos(cos(x)))</code> is allowed (as well as any combination of <code>+</code> or <code>-</code> within it), but <code>cos(cos(cos(cos(x))))</code> is not allowed. When an operator is not specified, it is assumed that it can be nested an unlimited number of times. This requires that there is no operator which is used both in the unary operators and the binary operators (e.g., <code>-</code> could be both subtract, and negation). For binary operators, both arguments are treated the same way, and the max of each argument is constrained.</p></li><li><p><code>deterministic</code>: Use a global counter for the birth time, rather than calls to <code>time()</code>. This gives perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used in serial mode.</p></li><li><p><code>define_helper_functions</code>: Whether to define helper functions for constructing and evaluating trees.</p></li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/Options.jl#L491-L500" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block" open><summary><a id="SymbolicRegression.CoreModule.MutationWeightsModule.MutationWeights" href="#SymbolicRegression.CoreModule.MutationWeightsModule.MutationWeights"><span class="jlbinding">SymbolicRegression.CoreModule.MutationWeightsModule.MutationWeights</span></a> <span class="VPBadge info jlObjectType jlType"><!--[-->Type<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">MutationWeights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(;kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractMutationWeights</span></span></code></pre></div><p>This defines how often different mutations occur. These weightings will be normalized to sum to 1.0 after initialization.</p><p><strong>Arguments</strong></p><ul><li><p><code>mutate_constant::Float64</code>: How often to mutate a constant.</p></li><li><p><code>mutate_operator::Float64</code>: How often to mutate an operator.</p></li><li><p><code>mutate_feature::Float64</code>: How often to mutate which feature a variable node references.</p></li><li><p><code>swap_operands::Float64</code>: How often to swap the operands of a binary operator.</p></li><li><p><code>rotate_tree::Float64</code>: How often to perform a tree rotation at a random node.</p></li><li><p><code>add_node::Float64</code>: How often to append a node to the tree.</p></li><li><p><code>insert_node::Float64</code>: How often to insert a node into the tree.</p></li><li><p><code>delete_node::Float64</code>: How often to delete a node from the tree.</p></li><li><p><code>simplify::Float64</code>: How often to simplify the tree.</p></li><li><p><code>randomize::Float64</code>: How often to create a random tree.</p></li><li><p><code>do_nothing::Float64</code>: How often to do nothing.</p></li><li><p><code>optimize::Float64</code>: How often to optimize the constants in the tree, as a mutation. Note that this is different from <code>optimizer_probability</code>, which is performed at the end of an iteration for all individuals.</p></li><li><p><code>form_connection::Float64</code>: <strong>Only used for <code>GraphNode</code>, not regular <code>Node</code></strong>. Otherwise, this will automatically be set to 0.0. How often to form a connection between two nodes.</p></li><li><p><code>break_connection::Float64</code>: <strong>Only used for <code>GraphNode</code>, not regular <code>Node</code></strong>. Otherwise, this will automatically be set to 0.0. How often to break a connection between two nodes.</p></li></ul><p><strong>See Also</strong></p><ul><li><a href="/SymbolicRegression.jl/previews/PR483/customization#SymbolicRegression.CoreModule.MutationWeightsModule.AbstractMutationWeights"><code>AbstractMutationWeights</code></a>: Use to define custom mutation weight types.</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/MutationWeights.jl#L70-L102" target="_blank" rel="noreferrer">source</a><!--]--></span></details><h2 id="Printing" tabindex="-1">Printing <a class="header-anchor" href="#Printing" aria-label="Permalink to &quot;Printing {#Printing}&quot;">​</a></h2><details class="jldocstring custom-block" open><summary><a id="DynamicExpressions.StringsModule.string_tree" href="#DynamicExpressions.StringsModule.string_tree"><span class="jlbinding">DynamicExpressions.StringsModule.string_tree</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">string_tree</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpressionNode{T}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{AbstractOperatorEnum,Nothing}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">nothing</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    f_variable</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">F1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">string_variable,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    f_constant</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">F2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">string_constant,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    variable_names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{Array{String,1},Nothing}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">nothing</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # Deprecated</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    varMap</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">nothing</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">String</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> where</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {T,F1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Function</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,F2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Function</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p>Convert an equation to a string.</p><p><strong>Arguments</strong></p><ul><li><p><code>tree</code>: the tree to convert to a string</p></li><li><p><code>operators</code>: the operators used to define the tree</p></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><p><code>f_variable</code>: (optional) function to convert a variable to a string, with arguments <code>(feature::UInt8, variable_names)</code>.</p></li><li><p><code>f_constant</code>: (optional) function to convert a constant to a string, with arguments <code>(val,)</code></p></li><li><p><code>variable_names::Union{Array{String, 1}, Nothing}=nothing</code>: (optional) what variables to print for each feature.</p></li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/src/Strings.jl#L136-L157" target="_blank" rel="noreferrer">source</a><!--]--></span><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">string_tree</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ex</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpression</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{AbstractOperatorEnum,Nothing}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">nothing</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    variable_names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">nothing</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Convert an expression to a string representation.</p><p>This method unpacks the operators and variable names from the expression and calls <a href="./@ref StringsModule.string_tree"><code>string_tree</code></a> for <code>AbstractExpressionNode</code>.</p><p><strong>Arguments</strong></p><ul><li><p><code>ex</code>: The expression to convert to a string.</p></li><li><p><code>operators</code>: (Optional) Operators to use. If <code>nothing</code>, operators are obtained from the expression.</p></li><li><p><code>variable_names</code>: (Optional) Variable names to use in the string representation. If <code>nothing</code>, variable names are obtained from the expression.</p></li><li><p><code>kws...</code>: Additional keyword arguments.</p></li></ul><p><strong>Returns</strong></p><ul><li>A string representation of the expression.</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/src/Expression.jl#L337-L360" target="_blank" rel="noreferrer">source</a><!--]--></span><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">string_tree</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpressionNode</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, options</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractOptions</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Convert an equation to a string.</p><p><strong>Arguments</strong></p><ul><li><p><code>tree::AbstractExpressionNode</code>: The equation to convert to a string.</p></li><li><p><code>options::AbstractOptions</code>: The options holding the definition of operators.</p></li><li><p><code>variable_names::Union{Array{String, 1}, Nothing}=nothing</code>: what variables to print for each feature.</p></li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/InterfaceDynamicExpressions.jl#L187-L198" target="_blank" rel="noreferrer">source</a><!--]--></span></details><h2 id="Evaluation" tabindex="-1">Evaluation <a class="header-anchor" href="#Evaluation" aria-label="Permalink to &quot;Evaluation {#Evaluation}&quot;">​</a></h2><details class="jldocstring custom-block" open><summary><a id="DynamicExpressions.EvaluateModule.eval_tree_array" href="#DynamicExpressions.EvaluateModule.eval_tree_array"><span class="jlbinding">DynamicExpressions.EvaluateModule.eval_tree_array</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">eval_tree_array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpressionNode{T}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    cX</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractMatrix{T}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">OperatorEnum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    eval_options</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{EvalOptions,Nothing}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">nothing</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">where</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {T}</span></span></code></pre></div><p>Evaluate a binary tree (equation) over a given input data matrix. The operators contain all of the operators used. This function fuses doublets and triplets of operations for lower memory usage.</p><p><strong>Arguments</strong></p><ul><li><p><code>tree::AbstractExpressionNode</code>: The root node of the tree to evaluate.</p></li><li><p><code>cX::AbstractMatrix{T}</code>: The input data to evaluate the tree on, with shape <code>[num_features, num_rows]</code>.</p></li><li><p><code>operators::OperatorEnum</code>: The operators used in the tree.</p></li><li><p><code>eval_options::Union{EvalOptions,Nothing}</code>: See <a href="/SymbolicRegression.jl/previews/PR483/api#DynamicExpressions.EvaluateModule.EvalOptions"><code>EvalOptions</code></a> for documentation on the different evaluation modes.</p></li></ul><p><strong>Returns</strong></p><ul><li><code>(output, complete)::Tuple{AbstractVector{T}, Bool}</code>: the result, which is a 1D array, as well as if the evaluation completed successfully (true/false). A <code>false</code> complete means an infinity or nan was encountered, and a large loss should be assigned to the equation.</li></ul><p><strong>Notes</strong></p><p>This function can be represented by the following pseudocode:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>def eval(current_node)</span></span>
<span class="line"><span>    if current_node is leaf</span></span>
<span class="line"><span>        return current_node.value</span></span>
<span class="line"><span>    elif current_node is degree 1</span></span>
<span class="line"><span>        return current_node.operator(eval(current_node.left_child))</span></span>
<span class="line"><span>    else</span></span>
<span class="line"><span>        return current_node.operator(eval(current_node.left_child), eval(current_node.right_child))</span></span></code></pre></div><p>The bulk of the code is for optimizations and pre-emptive NaN/Inf checks, which speed up evaluation significantly.</p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/src/Evaluate.jl#L169-L210" target="_blank" rel="noreferrer">source</a><!--]--></span><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">eval_tree_array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpressionNode</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, cX</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractMatrix</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">GenericOperatorEnum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; throw_errors</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Evaluate a generic binary tree (equation) over a given input data, whatever that input data may be. The <code>operators</code> enum contains all of the operators used. Unlike <code>eval_tree_array</code> with the normal <code>OperatorEnum</code>, the array <code>cX</code> is sliced only along the first dimension. i.e., if <code>cX</code> is a vector, then the output of a feature node will be a scalar. If <code>cX</code> is a 3D tensor, then the output of a feature node will be a 2D tensor. Note also that <code>tree.feature</code> will index along the first axis of <code>cX</code>.</p><p>However, there is no requirement about input and output types in general. You may set up your tree such that some operator nodes work on tensors, while other operator nodes work on scalars. <code>eval_tree_array</code> will simply return <code>nothing</code> if a given operator is not defined for the given input type.</p><p>This function can be represented by the following pseudocode:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>function eval(current_node)</span></span>
<span class="line"><span>    if current_node is leaf</span></span>
<span class="line"><span>        return current_node.value</span></span>
<span class="line"><span>    elif current_node is degree 1</span></span>
<span class="line"><span>        return current_node.operator(eval(current_node.left_child))</span></span>
<span class="line"><span>    else</span></span>
<span class="line"><span>        return current_node.operator(eval(current_node.left_child), eval(current_node.right_child))</span></span></code></pre></div><p><strong>Arguments</strong></p><ul><li><p><code>tree::AbstractExpressionNode</code>: The root node of the tree to evaluate.</p></li><li><p><code>cX::AbstractArray</code>: The input data to evaluate the tree on.</p></li><li><p><code>operators::GenericOperatorEnum</code>: The operators used in the tree.</p></li><li><p><code>throw_errors::Bool=true</code>: Whether to throw errors if they occur during evaluation. Otherwise, MethodErrors will be caught before they happen and evaluation will return <code>nothing</code>, rather than throwing an error. This is useful in cases where you are unsure if a particular tree is valid or not, and would prefer to work with <code>nothing</code> as an output.</p></li></ul><p><strong>Returns</strong></p><ul><li><code>(output, complete)::Tuple{Any, Bool}</code>: the result, as well as if the evaluation completed successfully (true/false). If evaluation failed, <code>nothing</code> will be returned for the first argument. A <code>false</code> complete means an operator was called on input types that it was not defined for.</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/src/Evaluate.jl#L904-L951" target="_blank" rel="noreferrer">source</a><!--]--></span><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">eval_tree_array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ex</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpression</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    cX</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractMatrix</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{AbstractOperatorEnum,Nothing}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">nothing</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Evaluate an expression over a given input data matrix.</p><p>This method unpacks the operators from the expression and calls <a href="./@ref EvaluateModule.eval_tree_array"><code>eval_tree_array</code></a> for <code>AbstractExpressionNode</code>.</p><p><strong>Arguments</strong></p><ul><li><p><code>ex</code>: The expression to evaluate.</p></li><li><p><code>cX</code>: The input data matrix.</p></li><li><p><code>operators</code>: (Optional) Operators to use. If <code>nothing</code>, operators are obtained from the expression.</p></li><li><p><code>kws...</code>: Additional keyword arguments.</p></li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, complete)</code> indicating the result and success of the evaluation.</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/src/Expression.jl#L411-L434" target="_blank" rel="noreferrer">source</a><!--]--></span><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">eval_tree_array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{AbstractExpression,AbstractExpressionNode}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractArray</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, options</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractOptions</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Evaluate a binary tree (equation) over a given input data matrix. The operators contain all of the operators used. This function fuses doublets and triplets of operations for lower memory usage.</p><p>This function can be represented by the following pseudocode:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>function eval(current_node)</span></span>
<span class="line"><span>    if current_node is leaf</span></span>
<span class="line"><span>        return current_node.value</span></span>
<span class="line"><span>    elif current_node is degree 1</span></span>
<span class="line"><span>        return current_node.operator(eval(current_node.left_child))</span></span>
<span class="line"><span>    else</span></span>
<span class="line"><span>        return current_node.operator(eval(current_node.left_child), eval(current_node.right_child))</span></span></code></pre></div><p>The bulk of the code is for optimizations and pre-emptive NaN/Inf checks, which speed up evaluation significantly.</p><p><strong>Arguments</strong></p><ul><li><p><code>tree::Union{AbstractExpression,AbstractExpressionNode}</code>: The root node of the tree to evaluate.</p></li><li><p><code>X::AbstractArray</code>: The input data to evaluate the tree on.</p></li><li><p><code>options::AbstractOptions</code>: Options used to define the operators used in the tree.</p></li></ul><p><strong>Returns</strong></p><ul><li><code>(output, complete)::Tuple{AbstractVector, Bool}</code>: the result, which is a 1D array, as well as if the evaluation completed successfully (true/false). A <code>false</code> complete means an infinity or nan was encountered, and a large loss should be assigned to the equation.</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/InterfaceDynamicExpressions.jl#L25-L57" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block" open><summary><a id="DynamicExpressions.EvaluateModule.EvalOptions" href="#DynamicExpressions.EvaluateModule.EvalOptions"><span class="jlbinding">DynamicExpressions.EvaluateModule.EvalOptions</span></a> <span class="VPBadge info jlObjectType jlType"><!--[-->Type<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">EvalOptions</span></span></code></pre></div><p>This holds options for expression evaluation, such as evaluation backend.</p><p><strong>Fields</strong></p><ul><li><p><code>turbo::Val{T}=Val(false)</code>: If <code>Val{true}</code>, use LoopVectorization.jl for faster evaluation.</p></li><li><p><code>bumper::Val{B}=Val(false)</code>: If <code>Val{true}</code>, use Bumper.jl for faster evaluation.</p></li><li><p><code>early_exit::Val{E}=Val(true)</code>: If <code>Val{true}</code>, any element of any step becoming <code>NaN</code> or <code>Inf</code> will terminate the computation. For <code>eval_tree_array</code>, this will result in the second return value, the completion flag, being <code>false</code>. For calling an expression using <code>tree(X)</code>, this will result in <code>NaN</code>s filling the entire buffer. This early exit is performed to avoid wasting compute cycles. Setting <code>Val{false}</code> will continue the computation as usual and thus result in <code>NaN</code>s only in the elements that actually have <code>NaN</code>s.</p></li><li><p><code>buffer::Union{ArrayBuffer,Nothing}</code>: If not <code>nothing</code>, use this buffer for evaluation. This should be an instance of <code>ArrayBuffer</code> which has an <code>array</code> field and an <code>index</code> field used to iterate which buffer slot to use.</p></li><li><p><code>use_fused::Val{U}=Val(true)</code>: If <code>Val{true}</code>, use fused kernels for faster evaluation. Setting this to <code>Val{false}</code> will skip the fused kernels, meaning that you would only need to overload <code>deg0_eval</code>, <code>deg1_eval</code> and <code>deg2_eval</code> for custom evaluation.</p></li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/src/Evaluate.jl#L77-L101" target="_blank" rel="noreferrer">source</a><!--]--></span></details><h2 id="Derivatives" tabindex="-1">Derivatives <a class="header-anchor" href="#Derivatives" aria-label="Permalink to &quot;Derivatives {#Derivatives}&quot;">​</a></h2><p><code>SymbolicRegression.jl</code> can automatically and efficiently compute derivatives of expressions with respect to variables or constants. This is done using either <code>eval_diff_tree_array</code>, to compute derivative with respect to a single variable, or with <code>eval_grad_tree_array</code>, to compute the gradient with respect all variables (or, all constants). Both use forward-mode automatic, but use <code>Zygote.jl</code> to compute derivatives of each operator, so this is very efficient.</p><details class="jldocstring custom-block" open><summary><a id="DynamicExpressions.EvaluateDerivativeModule.eval_diff_tree_array" href="#DynamicExpressions.EvaluateDerivativeModule.eval_diff_tree_array"><span class="jlbinding">DynamicExpressions.EvaluateDerivativeModule.eval_diff_tree_array</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">eval_diff_tree_array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpressionNode{T}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    cX</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractMatrix{T}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">OperatorEnum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    direction</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Integer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    turbo</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{Bool,Val}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Val</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">where</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {T</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Number</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p>Compute the forward derivative of an expression, using a similar structure and optimization to eval_tree_array. <code>direction</code> is the index of a particular variable in the expression. e.g., <code>direction=1</code> would indicate derivative with respect to <code>x1</code>.</p><p><strong>Arguments</strong></p><ul><li><p><code>tree::AbstractExpressionNode</code>: The expression tree to evaluate.</p></li><li><p><code>cX::AbstractMatrix{T}</code>: The data matrix, with shape <code>[num_features, num_rows]</code>.</p></li><li><p><code>operators::OperatorEnum</code>: The operators used to create the <code>tree</code>.</p></li><li><p><code>direction::Integer</code>: The index of the variable to take the derivative with respect to.</p></li><li><p><code>turbo::Union{Bool,Val}</code>: Use LoopVectorization.jl for faster evaluation. Currently this does not have any effect.</p></li></ul><p><strong>Returns</strong></p><ul><li><code>(evaluation, derivative, complete)::Tuple{AbstractVector{T}, AbstractVector{T}, Bool}</code>: the normal evaluation, the derivative, and whether the evaluation completed as normal (or encountered a nan or inf).</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/src/EvaluateDerivative.jl#L12-L39" target="_blank" rel="noreferrer">source</a><!--]--></span><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">eval_diff_tree_array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{AbstractExpression,AbstractExpressionNode}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractArray</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, options</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractOptions</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, direction</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Compute the forward derivative of an expression, using a similar structure and optimization to eval_tree_array. <code>direction</code> is the index of a particular variable in the expression. e.g., <code>direction=1</code> would indicate derivative with respect to <code>x1</code>.</p><p><strong>Arguments</strong></p><ul><li><p><code>tree::Union{AbstractExpression,AbstractExpressionNode}</code>: The expression tree to evaluate.</p></li><li><p><code>X::AbstractArray</code>: The data matrix, with each column being a data point.</p></li><li><p><code>options::AbstractOptions</code>: The options containing the operators used to create the <code>tree</code>.</p></li><li><p><code>direction::Int</code>: The index of the variable to take the derivative with respect to.</p></li></ul><p><strong>Returns</strong></p><ul><li><code>(evaluation, derivative, complete)::Tuple{AbstractVector, AbstractVector, Bool}</code>: the normal evaluation, the derivative, and whether the evaluation completed as normal (or encountered a nan or inf).</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/InterfaceDynamicExpressions.jl#L98-L117" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block" open><summary><a id="DynamicExpressions.EvaluateDerivativeModule.eval_grad_tree_array" href="#DynamicExpressions.EvaluateDerivativeModule.eval_grad_tree_array"><span class="jlbinding">DynamicExpressions.EvaluateDerivativeModule.eval_grad_tree_array</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">eval_grad_tree_array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpressionNode{T}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, cX</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractMatrix{T}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">OperatorEnum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; variable</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{Bool,Val}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Val</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), turbo</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{Bool,Val}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Val</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre></div><p>Compute the forward-mode derivative of an expression, using a similar structure and optimization to eval_tree_array. <code>variable</code> specifies whether we should take derivatives with respect to features (i.e., cX), or with respect to every constant in the expression.</p><p><strong>Arguments</strong></p><ul><li><p><code>tree::AbstractExpressionNode{T}</code>: The expression tree to evaluate.</p></li><li><p><code>cX::AbstractMatrix{T}</code>: The data matrix, with each column being a data point.</p></li><li><p><code>operators::OperatorEnum</code>: The operators used to create the <code>tree</code>.</p></li><li><p><code>variable::Union{Bool,Val}</code>: Whether to take derivatives with respect to features (i.e., <code>cX</code> - with <code>variable=true</code>), or with respect to every constant in the expression (<code>variable=false</code>).</p></li><li><p><code>turbo::Union{Bool,Val}</code>: Use LoopVectorization.jl for faster evaluation. Currently this does not have any effect.</p></li></ul><p><strong>Returns</strong></p><ul><li><code>(evaluation, gradient, complete)::Tuple{AbstractVector{T}, AbstractMatrix{T}, Bool}</code>: the normal evaluation, the gradient, and whether the evaluation completed as normal (or encountered a nan or inf).</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/src/EvaluateDerivative.jl#L170-L192" target="_blank" rel="noreferrer">source</a><!--]--></span><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">eval_grad_tree_array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ex</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpression</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    cX</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractMatrix</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{AbstractOperatorEnum,Nothing}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">nothing</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Compute the forward-mode derivative of an expression.</p><p>This method unpacks the operators from the expression and calls <a href="./@ref EvaluateDerivativeModule.eval_grad_tree_array"><code>eval_grad_tree_array</code></a> for <code>AbstractExpressionNode</code>.</p><p><strong>Arguments</strong></p><ul><li><p><code>ex</code>: The expression to evaluate.</p></li><li><p><code>cX</code>: The data matrix.</p></li><li><p><code>operators</code>: (Optional) Operators to use. If <code>nothing</code>, operators are obtained from the expression.</p></li><li><p><code>kws...</code>: Additional keyword arguments.</p></li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, gradient, complete)</code> indicating the result, gradient, and success of the evaluation.</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/src/Expression.jl#L449-L472" target="_blank" rel="noreferrer">source</a><!--]--></span><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">eval_grad_tree_array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{AbstractExpression,AbstractExpressionNode}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, X</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractArray</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, options</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractOptions</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; variable</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Compute the forward-mode derivative of an expression, using a similar structure and optimization to eval_tree_array. <code>variable</code> specifies whether we should take derivatives with respect to features (i.e., <code>X</code>), or with respect to every constant in the expression.</p><p><strong>Arguments</strong></p><ul><li><p><code>tree::Union{AbstractExpression,AbstractExpressionNode}</code>: The expression tree to evaluate.</p></li><li><p><code>X::AbstractArray</code>: The data matrix, with each column being a data point.</p></li><li><p><code>options::AbstractOptions</code>: The options containing the operators used to create the <code>tree</code>.</p></li><li><p><code>variable::Bool</code>: Whether to take derivatives with respect to features (i.e., <code>X</code> - with <code>variable=true</code>), or with respect to every constant in the expression (<code>variable=false</code>).</p></li></ul><p><strong>Returns</strong></p><ul><li><code>(evaluation, gradient, complete)::Tuple{AbstractVector, AbstractArray, Bool}</code>: the normal evaluation, the gradient, and whether the evaluation completed as normal (or encountered a nan or inf).</li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/InterfaceDynamicExpressions.jl#L132-L152" target="_blank" rel="noreferrer">source</a><!--]--></span></details><h2 id="SymbolicUtils.jl-interface" tabindex="-1">SymbolicUtils.jl interface <a class="header-anchor" href="#SymbolicUtils.jl-interface" aria-label="Permalink to &quot;SymbolicUtils.jl interface {#SymbolicUtils.jl-interface}&quot;">​</a></h2><details class="jldocstring custom-block" open><summary><a id="DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic" href="#DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic"><span class="jlbinding">DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">node_to_symbolic</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpressionNode</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, operators</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractOperatorEnum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            variable_names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{AbstractVector{&lt;:AbstractString}, Nothing}</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">nothing</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            index_functions</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>The interface to SymbolicUtils.jl. Passing a tree to this function will generate a symbolic equation in SymbolicUtils.jl format.</p><p><strong>Arguments</strong></p><ul><li><p><code>tree::AbstractExpressionNode</code>: The equation to convert.</p></li><li><p><code>operators::AbstractOperatorEnum</code>: OperatorEnum, which contains the operators used in the equation.</p></li><li><p><code>variable_names::Union{AbstractVector{&lt;:AbstractString}, Nothing}=nothing</code>: What variable names to use for each feature. Default is [x1, x2, x3, ...].</p></li><li><p><code>index_functions::Bool=false</code>: Whether to generate special names for the operators, which then allows one to convert back to a <code>AbstractExpressionNode</code> format using <code>symbolic_to_node</code>. (CURRENTLY UNAVAILABLE - See <a href="https://github.com/MilesCranmer/SymbolicRegression.jl/pull/84" target="_blank" rel="noreferrer">https://github.com/MilesCranmer/SymbolicRegression.jl/pull/84</a>).</p></li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/SymbolicML/DynamicExpressions.jl/blob/v2.3.0/ext/DynamicExpressionsSymbolicUtilsExt.jl#L203-L221" target="_blank" rel="noreferrer">source</a><!--]--></span><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">node_to_symbolic</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tree</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractExpressionNode</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, options</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Options</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Convert an expression to SymbolicUtils.jl form.</p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/ext/SymbolicRegressionSymbolicUtilsExt.jl#L10-L14" target="_blank" rel="noreferrer">source</a><!--]--></span></details><p>Note that use of this function requires <code>SymbolicUtils.jl</code> to be installed and loaded.</p><h2 id="Pareto-frontier" tabindex="-1">Pareto frontier <a class="header-anchor" href="#Pareto-frontier" aria-label="Permalink to &quot;Pareto frontier {#Pareto-frontier}&quot;">​</a></h2><details class="jldocstring custom-block" open><summary><a id="SymbolicRegression.HallOfFameModule.calculate_pareto_frontier" href="#SymbolicRegression.HallOfFameModule.calculate_pareto_frontier"><span class="jlbinding">SymbolicRegression.HallOfFameModule.calculate_pareto_frontier</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">calculate_pareto_frontier</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(hallOfFame</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">HallOfFame{T,L,P}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">where</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {T</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DATA_TYPE</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,L</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">LOSS_TYPE</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/HallOfFame.jl#L93-L95" target="_blank" rel="noreferrer">source</a><!--]--></span></details><h2 id="Logging" tabindex="-1">Logging <a class="header-anchor" href="#Logging" aria-label="Permalink to &quot;Logging {#Logging}&quot;">​</a></h2><details class="jldocstring custom-block" open><summary><a id="SymbolicRegression.LoggingModule.SRLogger" href="#SymbolicRegression.LoggingModule.SRLogger"><span class="jlbinding">SymbolicRegression.LoggingModule.SRLogger</span></a> <span class="VPBadge info jlObjectType jlType"><!--[-->Type<!--]--></span></summary><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">SRLogger</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(logger</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractLogger</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; log_interval</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Int</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>A logger for symbolic regression that wraps another logger.</p><p><strong>Arguments</strong></p><ul><li><p><code>logger</code>: The base logger to wrap</p></li><li><p><code>log_interval</code>: Number of steps between logging events. Default is 100 (log every 100 steps).</p></li></ul><span class="VPBadge info source-link"><!--[--><a href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/e82d86e838818b17ecf05900b3c41cf4821dbc81/src/Logging.jl#L30-L38" target="_blank" rel="noreferrer">source</a><!--]--></span></details><p>The <code>SRLogger</code> allows you to track the progress of symbolic regression searches. It can wrap any <code>AbstractLogger</code> that implements the Julia logging interface, such as from TensorBoardLogger.jl or Wandb.jl.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> TensorBoardLogger</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">logger </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> SRLogger</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">TBLogger</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;logs/run&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), log_interval</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> SRRegressor</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    logger</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">logger,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    kws</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div></div></div></main><footer class="VPDocFooter" data-v-83890dd9 data-v-4f9813fa><!--[--><!--]--><div class="edit-info" data-v-4f9813fa><div class="edit-link" data-v-4f9813fa><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/MilesCranmer/SymbolicRegression.jl/edit/master/docs/src/api.md" target="_blank" rel="noreferrer" data-v-4f9813fa><!--[--><span class="vpi-square-pen edit-link-icon" data-v-4f9813fa></span> Edit this page<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-4f9813fa><span class="visually-hidden" id="doc-footer-aria-label" data-v-4f9813fa>Pager</span><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link prev" href="/SymbolicRegression.jl/previews/PR483/slurm" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Previous page</span><span class="title" data-v-4f9813fa>Using SymbolicRegression.jl on a Cluster</span><!--]--></a></div><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link next" href="/SymbolicRegression.jl/previews/PR483/losses" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Next page</span><span class="title" data-v-4f9813fa>Losses</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-a9a9e638 data-v-c970a860><div class="container" data-v-c970a860><p class="message" data-v-c970a860>Made with <a href="https://documenter.juliadocs.org/stable/" target="_blank"><strong>Documenter.jl</strong></a>, <a href="https://vitepress.dev" target="_blank"><strong>VitePress</strong></a> and <a href="https://luxdl.github.io/DocumenterVitepress.jl/stable/" target="_blank"><strong>DocumenterVitepress.jl</strong></a> <br></p><p class="copyright" data-v-c970a860>© Copyright 2025.</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api.md\":\"DfbDqNuT\",\"customization.md\":\"DlAorxZy\",\"examples.md\":\"DRqPbhn8\",\"examples_custom_types.md\":\"CWQ9JFZ8\",\"examples_parameterized_function.md\":\"7yZwqGnZ\",\"examples_template_expression.md\":\"A-Txmzpp\",\"examples_template_parametric_expression.md\":\"lc7AlYpO\",\"index.md\":\"k93SuncM\",\"index_base.md\":\"CLeTez8U\",\"losses.md\":\"EK3C_wjx\",\"slurm.md\":\"BpzdM8UD\",\"types.md\":\"DEUFgJvo\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"SymbolicRegression.jl\",\"description\":\"Documentation for SymbolicRegression.jl\",\"base\":\"/SymbolicRegression.jl/previews/PR483/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":\"deep\",\"logo\":{\"src\":\"/logo.svg\",\"width\":24,\"height\":24},\"search\":{\"provider\":\"local\",\"options\":{\"detailedView\":true}},\"nav\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Examples\",\"collapsed\":false,\"items\":[{\"text\":\"Short Examples\",\"link\":\"/examples\"},{\"text\":\"Template Expressions\",\"link\":\"/examples/template_expression\"},{\"text\":\"Parameterized Expressions\",\"link\":\"/examples/parameterized_function\"},{\"text\":\"Parameterized Template Expressions\",\"link\":\"/examples/template_parametric_expression\"},{\"text\":\"Custom Types\",\"link\":\"/examples/custom_types\"},{\"text\":\"Using SymbolicRegression.jl on a Cluster\",\"link\":\"/slurm\"}]},{\"text\":\"API\",\"link\":\"/api\"},{\"text\":\"Losses\",\"link\":\"/losses\"},{\"text\":\"Types\",\"link\":\"/types\"},{\"text\":\"Customization\",\"link\":\"/customization\"},{\"component\":\"VersionPicker\"}],\"sidebar\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Examples\",\"collapsed\":false,\"items\":[{\"text\":\"Short Examples\",\"link\":\"/examples\"},{\"text\":\"Template Expressions\",\"link\":\"/examples/template_expression\"},{\"text\":\"Parameterized Expressions\",\"link\":\"/examples/parameterized_function\"},{\"text\":\"Parameterized Template Expressions\",\"link\":\"/examples/template_parametric_expression\"},{\"text\":\"Custom Types\",\"link\":\"/examples/custom_types\"},{\"text\":\"Using SymbolicRegression.jl on a Cluster\",\"link\":\"/slurm\"}]},{\"text\":\"API\",\"link\":\"/api\"},{\"text\":\"Losses\",\"link\":\"/losses\"},{\"text\":\"Types\",\"link\":\"/types\"},{\"text\":\"Customization\",\"link\":\"/customization\"}],\"editLink\":{\"pattern\":\"https://github.com/MilesCranmer/SymbolicRegression.jl/edit/master/docs/src/:path\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/MilesCranmer/SymbolicRegression.jl\"}],\"footer\":{\"message\":\"Made with <a href=\\\"https://documenter.juliadocs.org/stable/\\\" target=\\\"_blank\\\"><strong>Documenter.jl</strong></a>, <a href=\\\"https://vitepress.dev\\\" target=\\\"_blank\\\"><strong>VitePress</strong></a> and <a href=\\\"https://luxdl.github.io/DocumenterVitepress.jl/stable/\\\" target=\\\"_blank\\\"><strong>DocumenterVitepress.jl</strong></a> <br>\",\"copyright\":\"© Copyright 2025.\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>