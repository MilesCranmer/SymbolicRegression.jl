<!DOCTYPE html><HTML lang="en"><head><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Losses · SymbolicRegression.jl</title><script data-outdated-warner="" src="../assets/warner.js"></script><link href="https://ai.damtp.cam.ac.uk/symbolicregression/stable/losses/" rel="canonical"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script data-main="../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../index_base/"><img alt="SymbolicRegression.jl logo" src="../assets/logo.svg"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../index_base/">SymbolicRegression.jl</a></span></div><form action="../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index_base/">Contents</a></li><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/">Short Examples</a></li><li><a class="tocitem" href="../examples/template_expression/">Template Expressions</a></li><li><a class="tocitem" href="../examples/parameterized_function/">Parameterized Expressions</a></li></ul></li><li><a class="tocitem" href="../api/">API</a></li><li class="is-active"><a class="tocitem" href="">Losses</a><ul class="internal"><li><a class="tocitem" href="#Regression"><span>Regression</span></a></li><li><a class="tocitem" href="#Classification"><span>Classification</span></a></li></ul></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../customization/">Customization</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="">Losses</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Losses</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/master/docs/src/losses.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="Losses"><a class="docs-heading-anchor" href="#Losses">Losses</a><a id="Losses-1"></a><a class="docs-heading-anchor-permalink" href="#Losses" title="Permalink"></a></h1><p>These losses, and their documentation, are included from the <a href="https://github.com/JuliaML/LossFunctions.jl">LossFunctions.jl</a> package.</p><p>Pass the function as, e.g., <code>elementwise_loss=L1DistLoss()</code>.</p><p>You can also declare your own loss as a function that takes two (unweighted) or three (weighted) scalar arguments. For example,</p><pre><code class="nohighlight hljs">f(x, y, w) = abs(x-y)*w
options = Options(elementwise_loss=f)</code></pre><h2 id="Regression"><a class="docs-heading-anchor" href="#Regression">Regression</a><a id="Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Regression" title="Permalink"></a></h2><p>Regression losses work on the distance between targets and predictions: <code>r = x - y</code>.</p><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.LPDistLoss" id="LossFunctions.LPDistLoss"><code>LossFunctions.LPDistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LPDistLoss{P} &lt;: DistanceLoss</code></pre><p>The P-th power absolute distance loss. It is Lipschitz continuous iff <code>P == 1</code>, convex if and only if <code>P &gt;= 1</code>, and strictly convex iff <code>P &gt; 1</code>.</p><p class="math-container">\[L(r) = |r|^P\]</p></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.L1DistLoss" id="LossFunctions.L1DistLoss"><code>LossFunctions.L1DistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L1DistLoss &lt;: DistanceLoss</code></pre><p>The absolute distance loss. Special case of the <a href="#LossFunctions.LPDistLoss"><code>LPDistLoss</code></a> with <code>P=1</code>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(r) = |r|\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    3 │\.                     ./│    1 │            ┌------------│
      │ '\.                 ./' │      │            |            │
      │   \.               ./   │      │            |            │
      │    '\.           ./'    │      │_           |           _│
    L │      \.         ./      │   L' │            |            │
      │       '\.     ./'       │      │            |            │
      │         \.   ./         │      │            |            │
    0 │          '\./'          │   -1 │------------┘            │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -3                        3
                 ŷ - y                            ŷ - y</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.L2DistLoss" id="LossFunctions.L2DistLoss"><code>LossFunctions.L2DistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L2DistLoss &lt;: DistanceLoss</code></pre><p>The least squares loss. Special case of the <a href="#LossFunctions.LPDistLoss"><code>LPDistLoss</code></a> with <code>P=2</code>. It is strictly convex.</p><p class="math-container">\[L(r) = |r|^2\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    9 │\                       /│    3 │                   .r/   │
      │".                     ."│      │                 .r'     │
      │ ".                   ." │      │              _./'       │
      │  ".                 ."  │      │_           .r/         _│
    L │   ".               ."   │   L' │         _:/'            │
      │    '\.           ./'    │      │       .r'               │
      │      \.         ./      │      │     .r'                 │
    0 │        "-.___.-"        │   -3 │  _/r'                   │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.PeriodicLoss" id="LossFunctions.PeriodicLoss"><code>LossFunctions.PeriodicLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PeriodicLoss &lt;: DistanceLoss</code></pre><p>Measures distance on a circle of specified circumference <code>c</code>.</p><p class="math-container">\[L(r) = 1 - \cos \left( \frac{2 r \pi}{c} \right)\]</p></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.HuberLoss" id="LossFunctions.HuberLoss"><code>LossFunctions.HuberLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HuberLoss &lt;: DistanceLoss</code></pre><p>Loss function commonly used for robustness to outliers. For large values of <code>d</code> it becomes close to the <a href="#LossFunctions.L1DistLoss"><code>L1DistLoss</code></a>, while for small values of <code>d</code> it resembles the <a href="#LossFunctions.L2DistLoss"><code>L2DistLoss</code></a>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(r) = \begin{cases} \frac{r^2}{2} &amp; \quad \text{if } | r | \le \alpha \\ \alpha | r | - \frac{\alpha^3}{2} &amp; \quad \text{otherwise}\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (d=1)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │                         │    1 │                .+-------│
      │                         │      │              ./'        │
      │\.                     ./│      │             ./          │
      │ '.                   .' │      │_           ./          _│
    L │   \.               ./   │   L' │           /'            │
      │     \.           ./     │      │          /'             │
      │      '.         .'      │      │        ./'              │
    0 │        '-.___.-'        │   -1 │-------+'                │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.L1EpsilonInsLoss" id="LossFunctions.L1EpsilonInsLoss"><code>LossFunctions.L1EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L1EpsilonInsLoss &lt;: DistanceLoss</code></pre><p>The <span>$ϵ$</span>-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than <span>$ϵ$</span>, but penalizes larger deviances linarily. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(r) = \max \{ 0, | r | - \epsilon \}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (ϵ=1)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │\                       /│    1 │                  ┌------│
      │ \                     / │      │                  |      │
      │  \                   /  │      │                  |      │
      │   \                 /   │      │_      ___________!     _│
    L │    \               /    │   L' │      |                  │
      │     \             /     │      │      |                  │
      │      \           /      │      │      |                  │
    0 │       \_________/       │   -1 │------┘                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.L2EpsilonInsLoss" id="LossFunctions.L2EpsilonInsLoss"><code>LossFunctions.L2EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L2EpsilonInsLoss &lt;: DistanceLoss</code></pre><p>The quadratic <span>$ϵ$</span>-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than <span>$ϵ$</span>, but penalizes larger deviances quadratically. It is convex, but not strictly convex.</p><p class="math-container">\[L(r) = \max \{ 0, | r | - \epsilon \}^2\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (ϵ=0.5)             Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    8 │                         │    1 │                  /      │
      │:                       :│      │                 /       │
      │'.                     .'│      │                /        │
      │ \.                   ./ │      │_         _____/        _│
    L │  \.                 ./  │   L' │         /               │
      │   \.               ./   │      │        /                │
      │    '\.           ./'    │      │       /                 │
    0 │      '-._______.-'      │   -1 │      /                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.LogitDistLoss" id="LossFunctions.LogitDistLoss"><code>LossFunctions.LogitDistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LogitDistLoss &lt;: DistanceLoss</code></pre><p>The distance-based logistic loss for regression. It is strictly convex and Lipschitz continuous.</p><p class="math-container">\[L(r) = - \ln \frac{4 e^r}{(1 + e^r)^2}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │                         │    1 │                   _--'''│
      │\                       /│      │                ./'      │
      │ \.                   ./ │      │              ./         │
      │  '.                 .'  │      │_           ./          _│
    L │   '.               .'   │   L' │           ./            │
      │     \.           ./     │      │         ./              │
      │      '.         .'      │      │       ./                │
    0 │        '-.___.-'        │   -1 │___.-''                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -4                        4
                 ŷ - y                            ŷ - y</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.QuantileLoss" id="LossFunctions.QuantileLoss"><code>LossFunctions.QuantileLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuantileLoss &lt;: DistanceLoss</code></pre><p>The distance-based quantile loss, also known as pinball loss, can be used to estimate conditional τ-quantiles. It is Lipschitz continuous and convex, but not strictly convex. Furthermore it is symmetric if and only if <code>τ = 1/2</code>.</p><p class="math-container">\[L(r) = \begin{cases} -\left( 1 - \tau  \right) r &amp; \quad \text{if } r &lt; 0 \\ \tau r &amp; \quad \text{if } r \ge 0 \\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (τ=0.7)             Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │'\                       │  0.3 │            ┌------------│
      │  \.                     │      │            |            │
      │   '\                    │      │_           |           _│
      │     \.                  │      │            |            │
    L │      '\              ._-│   L' │            |            │
      │        \.         ..-'  │      │            |            │
      │         '.     _r/'     │      │            |            │
    0 │           '_./'         │ -0.7 │------------┘            │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -3                        3
                 ŷ - y                            ŷ - y</code></pre></div></section></article><h2 id="Classification"><a class="docs-heading-anchor" href="#Classification">Classification</a><a id="Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Classification" title="Permalink"></a></h2><p>Classifications losses (assuming binary) work on the margin between targets and predictions: <code>r = x y</code>, assuming the target <code>y</code> is either <code>-1</code> or <code>+1</code>.</p><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.ZeroOneLoss" id="LossFunctions.ZeroOneLoss"><code>LossFunctions.ZeroOneLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZeroOneLoss &lt;: MarginLoss</code></pre><p>The classical classification loss. It penalizes every misclassified observation with a loss of <code>1</code> while every correctly classified observation has a loss of <code>0</code>. It is not convex nor continuous and thus seldom used directly. Instead one usually works with some classification-calibrated surrogate loss, such as <a href="#LossFunctions.L1HingeLoss">L1HingeLoss</a>.</p><p class="math-container">\[L(a) = \begin{cases} 1 &amp; \quad \text{if } a &lt; 0 \\ 0 &amp; \quad \text{if } a &gt;= 0\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    1 │------------┐            │    1 │                         │
      │            |            │      │                         │
      │            |            │      │                         │
      │            |            │      │_________________________│
      │            |            │      │                         │
      │            |            │      │                         │
      │            |            │      │                         │
    0 │            └------------│   -1 │                         │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                y * h(x)                         y * h(x)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.PerceptronLoss" id="LossFunctions.PerceptronLoss"><code>LossFunctions.PerceptronLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PerceptronLoss &lt;: MarginLoss</code></pre><p>The perceptron loss linearly penalizes every prediction where the resulting <code>agreement &lt;= 0</code>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \max \{ 0, -a \}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │\.                       │    0 │            ┌------------│
      │ '..                     │      │            |            │
      │   \.                    │      │            |            │
      │     '.                  │      │            |            │
    L │      '.                 │   L' │            |            │
      │        \.               │      │            |            │
      │         '.              │      │            |            │
    0 │           \.____________│   -1 │------------┘            │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.LogitMarginLoss" id="LossFunctions.LogitMarginLoss"><code>LossFunctions.LogitMarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LogitMarginLoss &lt;: MarginLoss</code></pre><p>The margin version of the logistic loss. It is infinitely many times differentiable, strictly convex, and Lipschitz continuous.</p><p class="math-container">\[L(a) = \ln (1 + e^{-a})\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │ \.                      │    0 │                  ._--/""│
      │   \.                    │      │               ../'      │
      │     \.                  │      │              ./         │
      │       \..               │      │            ./'          │
    L │         '-_             │   L' │          .,'            │
      │            '-_          │      │         ./              │
      │               '\-._     │      │      .,/'               │
    0 │                    '""*-│   -1 │__.--''                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -4                        4
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.L1HingeLoss" id="LossFunctions.L1HingeLoss"><code>LossFunctions.L1HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L1HingeLoss &lt;: MarginLoss</code></pre><p>The hinge loss linearly penalizes every predicition where the resulting <code>agreement &lt; 1</code> . It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \max \{ 0, 1 - a \}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    3 │'\.                      │    0 │                  ┌------│
      │  ''_                    │      │                  |      │
      │     \.                  │      │                  |      │
      │       '.                │      │                  |      │
    L │         ''_             │   L' │                  |      │
      │            \.           │      │                  |      │
      │              '.         │      │                  |      │
    0 │                ''_______│   -1 │------------------┘      │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.L2HingeLoss" id="LossFunctions.L2HingeLoss"><code>LossFunctions.L2HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L2HingeLoss &lt;: MarginLoss</code></pre><p>The truncated least squares loss quadratically penalizes every predicition where the resulting <code>agreement &lt; 1</code>. It is locally Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \max \{ 0, 1 - a \}^2\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │     .                   │    0 │                 ,r------│
      │     '.                  │      │               ,/        │
      │      '\                 │      │             ,/          │
      │        \                │      │           ,/            │
    L │         '.              │   L' │         ./              │
      │          '.             │      │       ./                │
      │            \.           │      │     ./                  │
    0 │              '-.________│   -5 │   ./                    │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.SmoothedL1HingeLoss" id="LossFunctions.SmoothedL1HingeLoss"><code>LossFunctions.SmoothedL1HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SmoothedL1HingeLoss &lt;: MarginLoss</code></pre><p>As the name suggests a smoothed version of the L1 hinge loss. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \begin{cases} \frac{0.5}{\gamma} \cdot \max \{ 0, 1 - a \} ^2 &amp; \quad \text{if } a \ge 1 - \gamma \\ 1 - \frac{\gamma}{2} - a &amp; \quad \text{otherwise}\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (γ=2)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │\.                       │    0 │                 ,r------│
      │ '.                      │      │               ./'       │
      │   \.                    │      │              ,/         │
      │     '.                  │      │            ./'          │
    L │      '.                 │   L' │           ,'            │
      │        \.               │      │         ,/              │
      │          ',             │      │       ./'               │
    0 │            '*-._________│   -1 │______./                 │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.ModifiedHuberLoss" id="LossFunctions.ModifiedHuberLoss"><code>LossFunctions.ModifiedHuberLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ModifiedHuberLoss &lt;: MarginLoss</code></pre><p>A special (4 times scaled) case of the <a href="#LossFunctions.SmoothedL1HingeLoss"><code>SmoothedL1HingeLoss</code></a> with <code>γ=2</code>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \begin{cases} \max \{ 0, 1 - a \} ^2 &amp; \quad \text{if } a \ge -1 \\ - 4 a &amp; \quad \text{otherwise}\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │    '.                   │    0 │                .+-------│
      │     '.                  │      │              ./'        │
      │      '\                 │      │             ,/          │
      │        \                │      │           ,/            │
    L │         '.              │   L' │         ./              │
      │          '.             │      │       ./'               │
      │            \.           │      │______/'                 │
    0 │              '-.________│   -5 │                         │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.L2MarginLoss" id="LossFunctions.L2MarginLoss"><code>LossFunctions.L2MarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L2MarginLoss &lt;: MarginLoss</code></pre><p>The margin-based least-squares loss for classification, which penalizes every prediction where <code>agreement != 1</code> quadratically. It is locally Lipschitz continuous and strongly convex.</p><p class="math-container">\[L(a) = {\left( 1 - a \right)}^2\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │     .                   │    2 │                       ,r│
      │     '.                  │      │                     ,/  │
      │      '\                 │      │                   ,/    │
      │        \                │      ├                 ,/      ┤
    L │         '.              │   L' │               ./        │
      │          '.             │      │             ./          │
      │            \.          .│      │           ./            │
    0 │              '-.____.-' │   -3 │         ./              │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.ExpLoss" id="LossFunctions.ExpLoss"><code>LossFunctions.ExpLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ExpLoss &lt;: MarginLoss</code></pre><p>The margin-based exponential loss for classification, which penalizes every prediction exponentially. It is infinitely many times differentiable, locally Lipschitz continuous and strictly convex, but not clipable.</p><p class="math-container">\[L(a) = e^{-a}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │  \.                     │    0 │               _,,---:'""│
      │   l                     │      │           _r/"'         │
      │    l.                   │      │        .r/'             │
      │     ":                  │      │      .r'                │
    L │       \.                │   L' │     ./                  │
      │        "\..             │      │    .'                   │
      │           '":,_         │      │   ,'                    │
    0 │                ""---:.__│   -5 │  ./                     │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.SigmoidLoss" id="LossFunctions.SigmoidLoss"><code>LossFunctions.SigmoidLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SigmoidLoss &lt;: MarginLoss</code></pre><p>Continuous loss which penalizes every prediction with a loss within in the range (0,2). It is infinitely many times differentiable, Lipschitz continuous but nonconvex.</p><p class="math-container">\[L(a) = 1 - \tanh(a)\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │""'--,.                  │    0 │..                     ..│
      │      '\.                │      │ "\.                 ./" │
      │         '.              │      │    ',             ,'    │
      │           \.            │      │      \           /      │
    L │            "\.          │   L' │       \         /       │
      │              \.         │      │        \.     ./        │
      │                \,       │      │         \.   ./         │
    0 │                  '"-:.__│   -1 │          ',_,'          │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#LossFunctions.DWDMarginLoss" id="LossFunctions.DWDMarginLoss"><code>LossFunctions.DWDMarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DWDMarginLoss &lt;: MarginLoss</code></pre><p>The distance weighted discrimination margin loss. It is a differentiable generalization of the <a href="#LossFunctions.L1HingeLoss">L1HingeLoss</a> that is different than the <a href="#LossFunctions.SmoothedL1HingeLoss">SmoothedL1HingeLoss</a>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \begin{cases} 1 - a &amp; \quad \text{if } a \le \frac{q}{q+1} \\ \frac{1}{a^q} \frac{q^q}{(q+1)^{q+1}} &amp; \quad \text{otherwise}\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (q=1)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │      ".                 │    0 │                     ._r-│
      │        \.               │      │                   ./    │
      │         ',              │      │                 ./      │
      │           \.            │      │                 /       │
    L │            "\.          │   L' │                .        │
      │              \.         │      │                /        │
      │               ":__      │      │               ;         │
    0 │                   '""---│   -1 │---------------┘         │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../api/">« API</a><a class="docs-footer-nextpage" href="../types/">Types »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Sunday 2 February 2025 14:41">Sunday 2 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>