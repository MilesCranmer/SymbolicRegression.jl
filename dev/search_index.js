var documenterSearchIndex = {"docs":
[{"location":"customization/#Customization","page":"Customization","title":"Customization","text":"","category":"section"},{"location":"customization/","page":"Customization","title":"Customization","text":"Many parts of SymbolicRegression.jl are designed to be customizable.","category":"page"},{"location":"customization/","page":"Customization","title":"Customization","text":"The normal way to do this in Julia is to define a new type that subtypes an abstract type from a package, and then define new methods for the type, extending internal methods on that type.","category":"page"},{"location":"customization/#Custom-Options","page":"Customization","title":"Custom Options","text":"","category":"section"},{"location":"customization/","page":"Customization","title":"Customization","text":"For example, you can define a custom options type:","category":"page"},{"location":"customization/","page":"Customization","title":"Customization","text":"AbstractOptions","category":"page"},{"location":"customization/#SymbolicRegression.CoreModule.OptionsStructModule.AbstractOptions","page":"Customization","title":"SymbolicRegression.CoreModule.OptionsStructModule.AbstractOptions","text":"AbstractOptions\n\nAn abstract type that stores all search hyperparameters for SymbolicRegression.jl. The standard implementation is Options.\n\nYou may wish to create a new subtypes of AbstractOptions to override certain functions or create new behavior. Ensure that this new type has all properties of Options.\n\nFor example, if we have new options that we want to add to Options:\n\nBase.@kwdef struct MyNewOptions\n    a::Float64 = 1.0\n    b::Int = 3\nend\n\nwe can create a combined options type that forwards properties to each corresponding type:\n\nstruct MyOptions{O<:SymbolicRegression.Options} <: SymbolicRegression.AbstractOptions\n    new_options::MyNewOptions\n    sr_options::O\nend\nconst NEW_OPTIONS_KEYS = fieldnames(MyNewOptions)\n\n# Constructor with both sets of parameters:\nfunction MyOptions(; kws...)\n    new_options_keys = filter(k -> k in NEW_OPTIONS_KEYS, keys(kws))\n    new_options = MyNewOptions(; NamedTuple(new_options_keys .=> Tuple(kws[k] for k in new_options_keys))...)\n    sr_options_keys = filter(k -> !(k in NEW_OPTIONS_KEYS), keys(kws))\n    sr_options = SymbolicRegression.Options(; NamedTuple(sr_options_keys .=> Tuple(kws[k] for k in sr_options_keys))...)\n    return MyOptions(new_options, sr_options)\nend\n\n# Make all `Options` available while also making `new_options` accessible\nfunction Base.getproperty(options::MyOptions, k::Symbol)\n    if k in NEW_OPTIONS_KEYS\n        return getproperty(getfield(options, :new_options), k)\n    else\n        return getproperty(getfield(options, :sr_options), k)\n    end\nend\n\nBase.propertynames(options::MyOptions) = (NEW_OPTIONS_KEYS..., fieldnames(SymbolicRegression.Options)...)\n\nwhich would let you access a and b from MyOptions objects, as well as making all properties of Options available for internal methods in SymbolicRegression.jl\n\n\n\n\n\n","category":"type"},{"location":"customization/","page":"Customization","title":"Customization","text":"Any function in SymbolicRegression.jl you can generally define a new method on your custom options type, to define custom behavior.","category":"page"},{"location":"customization/#Custom-Mutations","page":"Customization","title":"Custom Mutations","text":"","category":"section"},{"location":"customization/","page":"Customization","title":"Customization","text":"You can define custom mutation operators by defining a new method on mutate!, as well as subtyping AbstractMutationWeights:","category":"page"},{"location":"customization/","page":"Customization","title":"Customization","text":"mutate!\nAbstractMutationWeights\ncondition_mutation_weights!\nsample_mutation\nMutationResult","category":"page"},{"location":"customization/#SymbolicRegression.MutateModule.mutate!","page":"Customization","title":"SymbolicRegression.MutateModule.mutate!","text":"mutate!(\n    tree::N,\n    member::P,\n    ::Val{S},\n    mutation_weights::AbstractMutationWeights,\n    options::AbstractOptions;\n    kws...,\n) where {N<:AbstractExpression,P<:PopMember,S}\n\nPerform a mutation on the given tree and member using the specified mutation type S. Various kws are provided to access other data needed for some mutations.\n\nYou may overload this function to handle new mutation types for new AbstractMutationWeights types.\n\nKeywords\n\ntemperature: The temperature parameter for annealing-based mutations.\ndataset::Dataset: The dataset used for scoring.\nscore: The score of the member before mutation.\nloss: The loss of the member before mutation.\ncurmaxsize: The current maximum size constraint, which may be different from options.maxsize.\nnfeatures: The number of features in the dataset.\nparent_ref: Reference to the mutated member's parent (only used for logging purposes).\nrecorder::RecordType: A recorder to log mutation details.\n\nReturns\n\nA MutationResult{N,P} object containing the mutated tree or member (but not both), the number of evaluations performed, if any, and whether to return immediately from the mutation function, or to let the next_generation function handle accepting or rejecting the mutation. For example, a simplify operation will not change the loss, so it can always return immediately.\n\n\n\n\n\n","category":"function"},{"location":"customization/#SymbolicRegression.CoreModule.MutationWeightsModule.AbstractMutationWeights","page":"Customization","title":"SymbolicRegression.CoreModule.MutationWeightsModule.AbstractMutationWeights","text":"AbstractMutationWeights\n\nAn abstract type that defines the interface for mutation weight structures in the symbolic regression framework. Subtypes of AbstractMutationWeights specify how often different mutation operations occur during the mutation process.\n\nYou can create custom mutation weight types by subtyping AbstractMutationWeights and defining your own mutation operations. Additionally, you can overload the sample_mutation function to handle sampling from your custom mutation types.\n\nUsage\n\nTo create a custom mutation weighting scheme with new mutation types, define a new subtype of AbstractMutationWeights and implement the necessary fields. Here's an example using Base.@kwdef to define the struct with default values:\n\nusing SymbolicRegression: AbstractMutationWeights\n\n# Define custom mutation weights with default values\nBase.@kwdef struct MyMutationWeights <: AbstractMutationWeights\n    mutate_constant::Float64 = 0.1\n    mutate_operator::Float64 = 0.2\n    custom_mutation::Float64 = 0.7\nend\n\nNext, overload the sample_mutation function to include your custom mutation types:\n\n# Define the list of mutation names (symbols)\nconst MY_MUTATIONS = [\n    :mutate_constant,\n    :mutate_operator,\n    :custom_mutation\n]\n\n# Import the `sample_mutation` function to overload it\nimport SymbolicRegression: sample_mutation\nusing StatsBase: StatsBase\n\n# Overload the `sample_mutation` function\nfunction sample_mutation(w::MyMutationWeights)\n    weights = [\n        w.mutate_constant,\n        w.mutate_operator,\n        w.custom_mutation\n    ]\n    weights = weights ./ sum(weights)  # Normalize weights to sum to 1.0\n    return StatsBase.sample(MY_MUTATIONS, StatsBase.Weights(weights))\nend\n\n# Pass it when defining `Options`:\nusing SymbolicRegression: Options\noptions = Options(mutation_weights=MyMutationWeights())\n\nThis allows you to customize the mutation sampling process to include your custom mutations according to their specified weights.\n\nTo integrate your custom mutations into the mutation process, ensure that the mutation functions corresponding to your custom mutation types are defined and properly registered with the symbolic regression framework. You may need to define methods for mutate! that handle your custom mutation types.\n\nSee Also\n\nMutationWeights: A concrete implementation of AbstractMutationWeights that defines default mutation weightings.\nsample_mutation: Function to sample a mutation based on current mutation weights.\nmutate!: Function to apply a mutation to an expression tree.\nAbstractOptions: See how to extend abstract types for customizing options.\n\n\n\n\n\n","category":"type"},{"location":"customization/#SymbolicRegression.MutateModule.condition_mutation_weights!","page":"Customization","title":"SymbolicRegression.MutateModule.condition_mutation_weights!","text":"condition_mutation_weights!(weights::AbstractMutationWeights, member::PopMember, options::AbstractOptions, curmaxsize::Int)\n\nAdjusts the mutation weights based on the properties of the current member and options.\n\nThis function modifies the mutation weights to ensure that the mutations applied to the member are appropriate given its current state and the provided options. It can be overloaded to customize the behavior for different types of expressions or members.\n\nNote that the weights were already copied, so you don't need to worry about mutation.\n\nArguments\n\nweights::AbstractMutationWeights: The mutation weights to be adjusted.\nmember::PopMember: The current population member being mutated.\noptions::AbstractOptions: The options that guide the mutation process.\ncurmaxsize::Int: The current maximum size constraint for the member's expression tree.\n\n\n\n\n\n","category":"function"},{"location":"customization/#SymbolicRegression.CoreModule.MutationWeightsModule.sample_mutation","page":"Customization","title":"SymbolicRegression.CoreModule.MutationWeightsModule.sample_mutation","text":"Sample a mutation, given the weightings.\n\n\n\n\n\n","category":"function"},{"location":"customization/#SymbolicRegression.MutateModule.MutationResult","page":"Customization","title":"SymbolicRegression.MutateModule.MutationResult","text":"MutationResult{N<:AbstractExpression,P<:PopMember}\n\nRepresents the result of a mutation operation in the genetic programming algorithm. This struct is used to return values from mutate! functions.\n\nFields\n\ntree::Union{N, Nothing}: The mutated expression tree, if applicable. Either tree or member must be set, but not both.\nmember::Union{P, Nothing}: The mutated population member, if applicable. Either member or tree must be set, but not both.\nnum_evals::Float64: The number of evaluations performed during the mutation, which is automatically set to 0.0. Only used for things like optimize.\nreturn_immediately::Bool: If true, the mutation process should return immediately, bypassing further checks, used for things like simplify or optimize where you already know the loss value of the result.\n\nUsage\n\nThis struct encapsulates the result of a mutation operation. Either a new expression tree or a new population member is returned, but not both.\n\nReturn the member if you want to return immediately, and have computed the loss value as part of the mutation.\n\n\n\n\n\n","category":"type"},{"location":"customization/#Custom-Expressions","page":"Customization","title":"Custom Expressions","text":"","category":"section"},{"location":"customization/","page":"Customization","title":"Customization","text":"You can create your own expression types by defining a new type that extends AbstractExpression.","category":"page"},{"location":"customization/","page":"Customization","title":"Customization","text":"AbstractExpression","category":"page"},{"location":"customization/#DynamicExpressions.ExpressionModule.AbstractExpression","page":"Customization","title":"DynamicExpressions.ExpressionModule.AbstractExpression","text":"AbstractExpression{T,N}\n\n(Experimental) Abstract type for user-facing expression types, which contain both the raw expression tree operating on a value type of T, as well as associated metadata to evaluate and render the expression.\n\nSee ExpressionInterface for a full description of the interface implementation, as well as tests to verify correctness.\n\nIf you wish to use @parse_expression, you can also customize the parsing behavior with\n\nparse_leaf\n\n\n\n\n\n","category":"type"},{"location":"customization/","page":"Customization","title":"Customization","text":"The interface is fairly flexible, and permits you define specific functional forms, extra parameters, etc. See the documentation of DynamicExpressions.jl for more details on what methods you need to implement. You can test the implementation of a given interface by using ExpressionInterface which makes use of Interfaces.jl:","category":"page"},{"location":"customization/","page":"Customization","title":"Customization","text":"ExpressionInterface","category":"page"},{"location":"customization/#DynamicExpressions.InterfacesModule.ExpressionInterface","page":"Customization","title":"DynamicExpressions.InterfacesModule.ExpressionInterface","text":"    ExpressionInterface\n\nAn Interfaces.jl Interface with mandatory components (:get_contents, :get_metadata, :get_tree, :get_operators, :get_variable_names, :copy, :with_contents, :with_metadata) and optional components (:count_nodes, :count_constant_nodes, :count_depth, :index_constant_nodes, :has_operators, :has_constants, :get_scalar_constants, :set_scalar_constants!, :string_tree, :default_node_type, :constructorof, :tree_mapreduce).\n\nDefines the interface of AbstractExpression for user-facing expression types, which can store operators, extra parameters, functional forms, variable names, etc.\n\nExtended help\n\nMandatory keys:\n\nget_contents: extracts the runtime contents of an expression\nget_metadata: extracts the runtime metadata of an expression\nget_tree: extracts the expression tree from AbstractExpression\nget_operators: returns the operators used in the expression (or pass operators explicitly to override)\nget_variable_names: returns the variable names used in the expression (or pass variable_names explicitly to override)\ncopy: returns a copy of the expression\nwith_contents: returns the expression with different tree\nwith_metadata: returns the expression with different metadata\n\nOptional keys:\n\ncount_nodes: counts the number of nodes in the expression tree\ncount_constant_nodes: counts the number of constant nodes in the expression tree\ncount_depth: calculates the depth of the expression tree\nindex_constant_nodes: indexes constants in the expression tree\nhas_operators: checks if the expression has operators\nhas_constants: checks if the expression has constants\nget_scalar_constants: gets constants from the expression tree, returning a tuple of: (1) a flat vector of the constants, and (2) an reference object that can be used by set_scalar_constants! to efficiently set them back\nset_scalar_constants!: sets constants in the expression tree, given: (1) a flat vector of constants, (2) the expression, and (3) the reference object produced by get_scalar_constants\nstring_tree: returns a string representation of the expression tree\ndefault_node_type: returns the default node type for the expression\nconstructorof: gets the constructor function for a type\ntree_mapreduce: applies a function across the tree\n\n\n\n\n\n","category":"type"},{"location":"customization/","page":"Customization","title":"Customization","text":"Then, for SymbolicRegression.jl, you would pass expression_type to the Options constructor, as well as any expression_options you need (as a NamedTuple).","category":"page"},{"location":"customization/","page":"Customization","title":"Customization","text":"If needed, you may need to overload SymbolicRegression.ExpressionBuilder.extra_init_params in case your expression needs additional parameters. See the method for ParametricExpression as an example.","category":"page"},{"location":"customization/","page":"Customization","title":"Customization","text":"You can look at the files src/ParametricExpression.jl and src/TemplateExpression.jl for more examples of custom expression types, though note that ParametricExpression itself is defined in DynamicExpressions.jl, while that file just overloads some methods for SymbolicRegression.jl.","category":"page"},{"location":"customization/#Other-Customizations","page":"Customization","title":"Other Customizations","text":"","category":"section"},{"location":"customization/","page":"Customization","title":"Customization","text":"Other internal abstract types include the following:","category":"page"},{"location":"customization/","page":"Customization","title":"Customization","text":"AbstractRuntimeOptions\nAbstractSearchState","category":"page"},{"location":"customization/#SymbolicRegression.SearchUtilsModule.AbstractRuntimeOptions","page":"Customization","title":"SymbolicRegression.SearchUtilsModule.AbstractRuntimeOptions","text":"AbstractRuntimeOptions\n\nAn abstract type representing runtime configuration parameters for the symbolic regression algorithm.\n\nAbstractRuntimeOptions is used by equation_search to control runtime aspects such as parallelism and iteration limits. By subtyping AbstractRuntimeOptions, advanced users can customize runtime behaviors by passing it to equation_search.\n\nSee Also\n\nRuntimeOptions: Default implementation used by equation_search.\nequation_search: Main function to perform symbolic regression.\nAbstractOptions: See how to extend abstract types for customizing options.\n\n\n\n\n\n","category":"type"},{"location":"customization/#SymbolicRegression.SearchUtilsModule.AbstractSearchState","page":"Customization","title":"SymbolicRegression.SearchUtilsModule.AbstractSearchState","text":"AbstractSearchState{T,L,N}\n\nAn abstract type encapsulating the internal state of the search process during symbolic regression.\n\nAbstractSearchState instances hold information like populations and progress metrics, used internally by equation_search. Subtyping AbstractSearchState allows customization of search state management.\n\nLook through the source of equation_search to see how this is used.\n\nSee Also\n\nSearchState: Default implementation of AbstractSearchState.\nequation_search: Function where AbstractSearchState is utilized.\nAbstractOptions: See how to extend abstract types for customizing options.\n\n\n\n\n\n","category":"type"},{"location":"customization/","page":"Customization","title":"Customization","text":"These let you include custom state variables and runtime options.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"EditURL = \"../../../examples/template_expression_complex.jl\"","category":"page"},{"location":"examples/template_expression/#Searching-with-template-expressions","page":"Template Expressions","title":"Searching with template expressions","text":"","category":"section"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Template expressions are a powerful feature in SymbolicRegression.jl that allow you to impose structure on the symbolic regression search. Rather than searching for a completely free-form expression, you can specify a template that combines multiple sub-expressions in a prescribed way.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"This is particularly useful when any of the following are true:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"You have domain knowledge about the functional form of your solution\nYou want to learn expressions for a vector-valued output\nYou need to enforce constraints on which variables can appear in different parts of the expression\nYou want to share sub-expressions between multiple components","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"For example, you might know that your system follows a pattern like: sin(f(x1, x2)) + g(x3)^2 where f and g are unknown functions you want to learn. With template expressions, you can encode this structure while still letting the symbolic regression search discover the optimal form of the sub-expressions.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"In this tutorial, we'll walk through a complete example of using template expressions to learn the components of a particle's motion under magnetic and drag forces. We'll see how to:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Define the structure of our template\nSpecify constraints on which variables each sub-expression can access\nSet up the symbolic regression search\nInterpret and use the results","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Let's get started!","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"using SymbolicRegression\nusing SymbolicRegression: ValidVector\nusing Random\nusing MLJBase: machine, fit!, predict, report","category":"page"},{"location":"examples/template_expression/#The-Physical-Problem","page":"Template Expressions","title":"The Physical Problem","text":"","category":"section"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"We'll study a charged particle moving through a magnetic field with temperature-dependent drag. The total force on the particle will have two components:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"mathbfF = mathbfF_textdrag + mathbfF_textmagnetic = -eta(T)mathbfv + q mathbfv times mathbfB(t)","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"where we will take q = 1 for simplicity.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"From physics, we know:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"The magnetic force comes from a cross product with the field: mathbfF_textmagnetic = mathbfv times mathbfB\nThe drag force opposes motion, and we'll define a simple model for it: mathbfF_textdrag = -eta(T)mathbfv","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Now, the parts of this model we don't know:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"The magnetic field mathbfB(t) varies in time throughout the experiment, but this pattern repeats for each experiment. We want to learn the components of this field, symbolically!\nThe drag coefficient eta(T) depends only on temperature. We also want to figure out what this is!","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"We'll generate synthetic data from a known model and then try to rediscover these relationships, only knowing the total force on a particle for a given experiment, as well as the input variables: time, velocity, and temperature. We will do this with template expressions to encode the physical structure of the problem.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Let's say we run this experiment 1000 times:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"n = 1000\nrng = Random.MersenneTwister(0);\nnothing #hide","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Each time we run the experiment, the temperature is a bit different:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"T = 298.15 .+ 0.5 .* rand(rng, n)\nT[1:3]","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"We run the experiment, and record the velocity at a random time between 0 and 10 seconds.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"t = 10 .* rand(rng, n)\nt[1:3]","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"We introduce a particle at a random velocity between -1 and 1","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"v = [ntuple(_ -> 2 * rand(rng) - 1, 3) for _ in 1:n]\nv[1:3]","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Now, let's create the true unknown model.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Let's assume the magnetic field is sinusoidal with frequency 1 Hz along axes x and y, and decays exponentially along the z-axis:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"mathbfB(t) = beginpmatrix\nsin(omega t) \ncos(omega t) \ne^-t10\nendpmatrix\nquad textwhere quad omega = 2pi","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"This gives us a rotating magnetic field in the x-y plane that weakens along z:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"omega = 2π\nB = [(sin(omega * ti), cos(omega * ti), exp(-ti / 10)) for ti in t]\nB[1:3]","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"We assume the drag force is linear in the velocity and depends on the temperature with a power law:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"mathbfF_textdrag = -alpha T^12 mathbfv\nquad textwhere quad alpha = 10^-5","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"This creates a temperature-dependent damping effect:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"F_d = [-1e-5 * Ti^(1//2) .* vi for (Ti, vi) in zip(T, v)]\nF_d[1:3]","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Now, let's compute the true magnetic force, in 3D:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"cross((a1, a2, a3), (b1, b2, b3)) = (a2 * b3 - a3 * b2, a3 * b1 - a1 * b3, a1 * b2 - a2 * b1)\nF_mag = [cross(vi, Bi) for (vi, Bi) in zip(v, B)]\nF_mag[1:3]","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"We then sum these to get the total force:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"F = [fd .+ fm for (fd, fm) in zip(F_d, F_mag)]\nF[1:3]","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"This forms our dataset!","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"data = (; t, v, T, F, B, F_d, F_mag)\nkeys(data)","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Now, let's format the input variables for input to the regressor:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"X = (;\n    t=data.t,\n    v_x=[vi[1] for vi in data.v],\n    v_y=[vi[2] for vi in data.v],\n    v_z=[vi[3] for vi in data.v],\n    T=data.T,\n)\nkeys(X)","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Template expressions allow us to regress directly on a struct, so here we can define a Force type:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"struct Force{T}\n    x::T\n    y::T\n    z::T\nend\ny = [Force(F...) for F in data.F]\ny[1:3]","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Our input variable names are as follows:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"variable_names = [\"t\", \"v_x\", \"v_y\", \"v_z\", \"T\"]","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Template expressions require you to define a structure function, which describes how to combine the sub-expressions into a single expression, numerically evaluate them, and print them. These are evaluated using ComposableExpression for the individual subexpressions (which allow them to be composed into new expressions), and ValidVector for carrying through evaluation results.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Let's define our structure function. Note that this takes two arguments, one being a named tuple of our expressions (::ComposableExpression), and the other being a tuple of the input variables (::ValidVector).","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"function compute_force((; B_x, B_y, B_z, F_d_scale), (t, v_x, v_y, v_z, T))\n    # First, we evaluate each subexpression on the variables we wish\n    # to have each depend on:\n    _B_x = B_x(t)\n    _B_y = B_y(t)\n    _B_z = B_z(t)\n    _F_d_scale = F_d_scale(T)\n    # Note that we can also evaluate an expression multiple times,\n    # including in a hierarchy!\n\n    # Now, let's do the same computation we did above to\n    # get the total force vectors. Note that the evaluation\n    # output is wrapped in `ValidVector`, so we need\n    # to extract the `.x` to get raw vectors:\n    B = [(bx, by, bz) for (bx, by, bz) in zip(_B_x.x, _B_y.x, _B_z.x)]\n    v = [(vx, vy, vz) for (vx, vy, vz) in zip(v_x.x, v_y.x, v_z.x)]\n\n\n    # Now, let's compute the drag force using our model:\n    F_d = [_F_d_scale .* vi for (vi, _F_d_scale) in zip(v, _F_d_scale.x)]\n\n    # Now, the magnetic force:\n    F_mag = [cross(vi, Bi) for (vi, Bi) in zip(v, B)]\n\n    # Finally, we combine the drag and magnetic forces into the total force:\n    F = [Force((fd .+ fm)...) for (fd, fm) in zip(F_d, F_mag)]\n\n    # The output of this function needs to be another `ValidVector`,\n    # which carries through the validity of the evaluation. We compute\n    # this below.\n    ValidVector(F, _B_x.valid && _B_y.valid && _B_z.valid && _F_d_scale.valid)\n    # (Note that if you were doing operations that could not handle NaNs,\n    # you may need to return early - just be sure to also return the `ValidVector`!)\nend","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Note above that we have constrained what variables each subexpression depends on.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"We have constrained the magnetic field to only depend on time, and the drag force scale to only depend on temperature. The other variables we simply pass through and use in the evaluation.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Now, we can create our template expression, with the subexpression symbols we wish to learn:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"structure = TemplateStructure{(:B_x, :B_y, :B_z, :F_d_scale)}(compute_force)","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"First, let's look at an example of how this would be used in a TemplateExpression, for some guess at the form of the solution:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"options = Options(; binary_operators=(+, *, /, -), unary_operators=(sin, cos, sqrt, exp))\n# The inner operators are an `DynamicExpressions.OperatorEnum` which is used by `Expression`:\noperators = options.operators\nt = ComposableExpression(Node{Float64}(; feature=1); operators, variable_names)\nT = ComposableExpression(Node{Float64}(; feature=5); operators, variable_names)\nB_x = B_y = B_z = 2.1 * cos(t)\nF_d_scale = 1.0 * sqrt(T)\n\nex = TemplateExpression(\n    (; B_x, B_y, B_z, F_d_scale);\n    structure, operators, variable_names\n)","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"So we can see that it prints the expression as we've defined it.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Now, we can create a regressor that builds template expressions which follow this structure!","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"model = SRRegressor(;\n    binary_operators=(+, -, *, /),\n    unary_operators=(sin, cos, sqrt, exp),\n    niterations=500,\n    maxsize=35,\n    expression_type=TemplateExpression,\n    expression_options=(; structure=structure),\n    # Note that the elementwise loss needs to operate directly on each row of `y`:\n    elementwise_loss=(F1, F2) -> (F1.x - F2.x)^2 + (F1.y - F2.y)^2 + (F1.z - F2.z)^2,\n    batching=true,\n    batch_size=30,\n);\nnothing #hide","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Note how we also have to define the custom elementwise_loss function. This is because our combine_vectors function returns a Force struct, so we need to combine it against the truth!","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"Next, we can set up our machine and fit:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"mach = machine(model, X, y)","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"At this point, you would run:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"fit!(mach)","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"which should print using your combine_strings function during the search. The final result is accessible with:","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"report(mach)","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"which would return a named tuple of the fitted results, including the .equations field, which is a vector of TemplateExpression objects that dominated the Pareto front.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"<details>\n<summary> Show raw source code </summary>","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"#=\n# Searching with template expressions\n\nTemplate expressions are a powerful feature in SymbolicRegression.jl that allow you to impose structure\non the symbolic regression search. Rather than searching for a completely free-form expression, you can\nspecify a template that combines multiple sub-expressions in a prescribed way.\n\nThis is particularly useful when any of the following are true:\n- You have domain knowledge about the functional form of your solution\n- You want to learn expressions for a vector-valued output\n- You need to enforce constraints on which variables can appear in different parts of the expression\n- You want to share sub-expressions between multiple components\n\nFor example, you might know that your system follows a pattern like:\n`sin(f(x1, x2)) + g(x3)^2`\nwhere `f` and `g` are unknown functions you want to learn. With template expressions, you can encode\nthis structure while still letting the symbolic regression search discover the optimal form of the\nsub-expressions.\n\nIn this tutorial, we'll walk through a complete example of using template expressions to learn\nthe components of a particle's motion under magnetic and drag forces. We'll see how to:\n\n1. Define the structure of our template\n2. Specify constraints on which variables each sub-expression can access\n3. Set up the symbolic regression search\n4. Interpret and use the results\n\nLet's get started!\n=#\nusing SymbolicRegression\nusing SymbolicRegression: ValidVector\nusing Random\nusing MLJBase: machine, fit!, predict, report\n\n#=\n\n## The Physical Problem\n\nWe'll study a charged particle moving through a magnetic field with temperature-dependent drag.\nThe total force on the particle will have two components:\n\n\\```math\n\\mathbf{F} = \\mathbf{F}_\\text{drag} + \\mathbf{F}_\\text{magnetic} = -\\eta(T)\\mathbf{v} + q \\mathbf{v} \\times \\mathbf{B}(t)\n\\```\nwhere we will take ``q = 1`` for simplicity.\n\nFrom physics, we know:\n- The magnetic force comes from a cross product with the field: ``\\mathbf{F}_\\text{magnetic} = \\mathbf{v} \\times \\mathbf{B}``\n- The drag force opposes motion, and we'll define a simple model for it: ``\\mathbf{F}_\\text{drag} = -\\eta(T)\\mathbf{v}``\n\nNow, the parts of this model we don't know:\n- The magnetic field ``\\mathbf{B}(t)`` varies in time throughout the experiment, but this pattern repeats for each experiment. We want to learn the components of this field, symbolically!\n- The drag coefficient ``\\eta(T)`` depends only on temperature. We also want to figure out what this is!\n\nWe'll generate synthetic data from a known model and then try to rediscover these relationships,\n**only knowing the total force** on a particle for a given experiment, as well as the input variables:\ntime, velocity, and temperature.\nWe will do this with template expressions to encode the physical structure of the problem.\n\nLet's say we run this experiment 1000 times:\n=#\nn = 1000\nrng = Random.MersenneTwister(0);\n\n#=\nEach time we run the experiment, the temperature is a bit different:\n=#\nT = 298.15 .+ 0.5 .* rand(rng, n)\nT[1:3]\n\n#=\nWe run the experiment, and record the velocity at a random time\nbetween 0 and 10 seconds.\n=#\nt = 10 .* rand(rng, n)\nt[1:3]\n\n#=\nWe introduce a particle at a random velocity between -1 and 1\n=#\nv = [ntuple(_ -> 2 * rand(rng) - 1, 3) for _ in 1:n]\nv[1:3]\n\n#=\n**Now, let's create the true unknown model.**\n\nLet's assume the magnetic field is sinusoidal with frequency 1 Hz along axes x and y,\nand decays exponentially along the z-axis:\n\n\\```math\n\\mathbf{B}(t) = \\begin{pmatrix}\n\\sin(\\omega t) \\\\\n\\cos(\\omega t) \\\\\ne^{-t/10}\n\\end{pmatrix}\n\\quad \\text{where} \\quad \\omega = 2\\pi\n\\```\n\nThis gives us a rotating magnetic field in the x-y plane that weakens along z:\n=#\nomega = 2π\nB = [(sin(omega * ti), cos(omega * ti), exp(-ti / 10)) for ti in t]\nB[1:3]\n\n#=\nWe assume the drag force is linear in the velocity and\ndepends on the temperature with a power law:\n\n\\```math\n\\mathbf{F}_\\text{drag} = -\\alpha T^{1/2} \\mathbf{v}\n\\quad \\text{where} \\quad \\alpha = 10^{-5}\n\\```\n\nThis creates a temperature-dependent damping effect:\n=#\nF_d = [-1e-5 * Ti^(1//2) .* vi for (Ti, vi) in zip(T, v)]\nF_d[1:3]\n\n#=\nNow, let's compute the true magnetic force, in 3D:\n=#\ncross((a1, a2, a3), (b1, b2, b3)) = (a2 * b3 - a3 * b2, a3 * b1 - a1 * b3, a1 * b2 - a2 * b1)\nF_mag = [cross(vi, Bi) for (vi, Bi) in zip(v, B)]\nF_mag[1:3]\n\n#=\nWe then sum these to get the total force:\n=#\nF = [fd .+ fm for (fd, fm) in zip(F_d, F_mag)]\nF[1:3]\n\n#=\nThis forms our dataset!\n=#\ndata = (; t, v, T, F, B, F_d, F_mag)\nkeys(data)\n\n#=\nNow, let's format the input variables for input to the regressor:\n=#\nX = (;\n    t=data.t,\n    v_x=[vi[1] for vi in data.v],\n    v_y=[vi[2] for vi in data.v],\n    v_z=[vi[3] for vi in data.v],\n    T=data.T,\n)\nkeys(X)\n\n#=\nTemplate expressions allow us to regress directly on a struct,\nso here we can define a `Force` type:\n=#\nstruct Force{T}\n    x::T\n    y::T\n    z::T\nend\ny = [Force(F...) for F in data.F]\ny[1:3]\n\n#=\nOur input variable names are as follows:\n=#\nvariable_names = [\"t\", \"v_x\", \"v_y\", \"v_z\", \"T\"]\n\n#=\nTemplate expressions require you to define a _structure_ function,\nwhich describes how to combine the sub-expressions into a single\nexpression, numerically evaluate them, and print them.\nThese are evaluated using `ComposableExpression` for the individual\nsubexpressions (which allow them to be composed into new expressions),\nand `ValidVector` for carrying through evaluation results.\n\nLet's define our structure function. Note that this takes two arguments,\none being a named tuple of our expressions (`::ComposableExpression`), and the other being a tuple\nof the input variables (`::ValidVector`).\n=#\nfunction compute_force((; B_x, B_y, B_z, F_d_scale), (t, v_x, v_y, v_z, T))\n    ## First, we evaluate each subexpression on the variables we wish\n    ## to have each depend on:\n    _B_x = B_x(t)\n    _B_y = B_y(t)\n    _B_z = B_z(t)\n    _F_d_scale = F_d_scale(T)\n    ## Note that we can also evaluate an expression multiple times,\n    ## including in a hierarchy!\n\n    ## Now, let's do the same computation we did above to\n    ## get the total force vectors. Note that the evaluation\n    ## output is wrapped in `ValidVector`, so we need\n    ## to extract the `.x` to get raw vectors:\n    B = [(bx, by, bz) for (bx, by, bz) in zip(_B_x.x, _B_y.x, _B_z.x)]\n    v = [(vx, vy, vz) for (vx, vy, vz) in zip(v_x.x, v_y.x, v_z.x)]\n\n\n    ## Now, let's compute the drag force using our model:\n    F_d = [_F_d_scale .* vi for (vi, _F_d_scale) in zip(v, _F_d_scale.x)]\n\n    ## Now, the magnetic force:\n    F_mag = [cross(vi, Bi) for (vi, Bi) in zip(v, B)]\n\n    ## Finally, we combine the drag and magnetic forces into the total force:\n    F = [Force((fd .+ fm)...) for (fd, fm) in zip(F_d, F_mag)]\n\n    ## The output of this function needs to be another `ValidVector`,\n    ## which carries through the validity of the evaluation. We compute\n    ## this below.\n    ValidVector(F, _B_x.valid && _B_y.valid && _B_z.valid && _F_d_scale.valid)\n    ## (Note that if you were doing operations that could not handle NaNs,\n    ## you may need to return early - just be sure to also return the `ValidVector`!)\nend\n\n#=\nNote above that we have constrained what variables each subexpression depends on.\n\nWe have constrained the magnetic field to only depend on time,\nand the drag force scale to only depend on temperature.\nThe other variables we simply pass through and use in the evaluation.\n\nNow, we can create our template expression, with the\nsubexpression symbols we wish to learn:\n=#\nstructure = TemplateStructure{(:B_x, :B_y, :B_z, :F_d_scale)}(compute_force)\n\n#=\nFirst, let's look at an example of how this would be used\nin a TemplateExpression, for some guess at the form of\nthe solution:\n=#\noptions = Options(; binary_operators=(+, *, /, -), unary_operators=(sin, cos, sqrt, exp))\n## The inner operators are an `DynamicExpressions.OperatorEnum` which is used by `Expression`:\noperators = options.operators\nt = ComposableExpression(Node{Float64}(; feature=1); operators, variable_names)\nT = ComposableExpression(Node{Float64}(; feature=5); operators, variable_names)\nB_x = B_y = B_z = 2.1 * cos(t)\nF_d_scale = 1.0 * sqrt(T)\n\nex = TemplateExpression(\n    (; B_x, B_y, B_z, F_d_scale);\n    structure, operators, variable_names\n)\n\n#=\nSo we can see that it prints the expression as we've defined it.\n\nNow, we can create a regressor that builds template expressions\nwhich follow this structure!\n=#\nmodel = SRRegressor(;\n    binary_operators=(+, -, *, /),\n    unary_operators=(sin, cos, sqrt, exp),\n    niterations=500,\n    maxsize=35,\n    expression_type=TemplateExpression,\n    expression_options=(; structure=structure),\n    ## Note that the elementwise loss needs to operate directly on each row of `y`:\n    elementwise_loss=(F1, F2) -> (F1.x - F2.x)^2 + (F1.y - F2.y)^2 + (F1.z - F2.z)^2,\n    batching=true,\n    batch_size=30,\n);\n\n#=\nNote how we also have to define the custom `elementwise_loss`\nfunction. This is because our `combine_vectors` function\nreturns a `Force` struct, so we need to combine it against the truth!\n\nNext, we can set up our machine and fit:\n=#\n\nmach = machine(model, X, y)\n\n#=\nAt this point, you would run:\n\\```julia\nfit!(mach)\n\\```\n\nwhich should print using your `combine_strings` function\nduring the search. The final result is accessible with:\n\\```julia\nreport(mach)\n\\```\nwhich would return a named tuple of the fitted results,\nincluding the `.equations` field, which is a vector\nof `TemplateExpression` objects that dominated the Pareto front.\n=#","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"which uses Literate.jl to generate this page.","category":"page"},{"location":"examples/template_expression/","page":"Template Expressions","title":"Template Expressions","text":"</details>","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#MLJ-interface","page":"API","title":"MLJ interface","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"SRRegressor\nMultitargetSRRegressor","category":"page"},{"location":"api/#SymbolicRegression.MLJInterfaceModule.SRRegressor","page":"API","title":"SymbolicRegression.MLJInterfaceModule.SRRegressor","text":"SRRegressor\n\nA model type for constructing a Symbolic Regression via Evolutionary Search, based on SymbolicRegression.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSRRegressor = @load SRRegressor pkg=SymbolicRegression\n\nDo model = SRRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SRRegressor(defaults=...).\n\nSingle-target Symbolic Regression regressor (SRRegressor) searches for symbolic expressions that predict a single target variable from a set of input variables. All data is assumed to be Continuous. The search is performed using an evolutionary algorithm. This algorithm is described in the paper https://arxiv.org/abs/2305.01582.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nOR\n\nmach = machine(model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X). Variable names in discovered expressions will be taken from the column names of X, if available. Units in columns of X (use DynamicQuantities for units) will trigger dimensional analysis to be used.\ny is the target, which can be any AbstractVector whose element scitype is   Continuous; check the scitype with scitype(y). Units in y (use DynamicQuantities   for units) will trigger dimensional analysis to be used.\nw is the observation weights which can either be nothing (default) or an AbstractVector whose element scitype is Count or Continuous.\n\nTrain the machine using fit!(mach), inspect the discovered expressions with report(mach), and predict on new data with predict(mach, Xnew). Note that unlike other regressors, symbolic regression stores a list of trained models. The model chosen from this list is defined by the function selection_method keyword argument, which by default balances accuracy and complexity. You can override this at prediction time by passing a named tuple with keys data and idx.\n\nHyper-parameters\n\ndefaults: What set of defaults to use for Options. The default,   nothing, will simply take the default options from the current version of SymbolicRegression.   However, you may also select the defaults from an earlier version, such as v\"0.24.5\".\nbinary_operators: Vector of binary operators (functions) to use.   Each operator should be defined for two input scalars,   and one output scalar. All operators   need to be defined over the entire real line (excluding infinity - these   are stopped before they are input), or return NaN where not defined.   For speed, define it so it takes two reals   of the same type as input, and outputs the same type. For the SymbolicUtils   simplification backend, you will need to define a generic method of the   operator so it takes arbitrary types.\nunary_operators: Same, but for   unary operators (one input scalar, gives an output scalar).\nconstraints: Array of pairs specifying size constraints   for each operator. The constraints for a binary operator should be a 2-tuple   (e.g., (-1, -1)) and the constraints for a unary operator should be an Int.   A size constraint is a limit to the size of the subtree   in each argument of an operator. e.g., [(^)=>(-1, 3)] means that the   ^ operator can have arbitrary size (-1) in its left argument,   but a maximum size of 3 in its right argument. Default is   no constraints.\nbatching: Whether to evolve based on small mini-batches of data,   rather than the entire dataset.\nbatch_size: What batch size to use if using batching.\nelementwise_loss: What elementwise loss function to use. Can be one of   the following losses, or any other loss of type   SupervisedLoss. You can also pass a function that takes   a scalar target (left argument), and scalar predicted (right   argument), and returns a scalar. This will be averaged   over the predicted data. If weights are supplied, your   function should take a third argument for the weight scalar.   Included losses:       Regression:           - LPDistLoss{P}(),           - L1DistLoss(),           - L2DistLoss() (mean square),           - LogitDistLoss(),           - HuberLoss(d),           - L1EpsilonInsLoss(ϵ),           - L2EpsilonInsLoss(ϵ),           - PeriodicLoss(c),           - QuantileLoss(τ),       Classification:           - ZeroOneLoss(),           - PerceptronLoss(),           - L1HingeLoss(),           - SmoothedL1HingeLoss(γ),           - ModifiedHuberLoss(),           - L2MarginLoss(),           - ExpLoss(),           - SigmoidLoss(),           - DWDMarginLoss(q).\nloss_function: Alternatively, you may redefine the loss used   as any function of tree::AbstractExpressionNode{T}, dataset::Dataset{T},   and options::AbstractOptions, so long as you output a non-negative   scalar of type T. This is useful if you want to use a loss   that takes into account derivatives, or correlations across   the dataset. This also means you could use a custom evaluation   for a particular expression. If you are using   batching=true, then your function should   accept a fourth argument idx, which is either nothing   (indicating that the full dataset should be used), or a vector   of indices to use for the batch.   For example,\n  function my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\n      prediction, flag = eval_tree_array(tree, dataset.X, options)\n      if !flag\n          return L(Inf)\n      end\n      return sum((prediction .- dataset.y) .^ 2) / dataset.n\n  end\nexpression_type::Type{E}=Expression: The type of expression to use.   For example, Expression.\nnode_type::Type{N}=default_node_type(Expression): The type of node to use for the search.   For example, Node or GraphNode. The default is computed by default_node_type(expression_type).\npopulations: How many populations of equations to use.\npopulation_size: How many equations in each population.\nncycles_per_iteration: How many generations to consider per iteration.\ntournament_selection_n: Number of expressions considered in each tournament.\ntournament_selection_p: The fittest expression in a tournament is to be   selected with probability p, the next fittest with probability p*(1-p),   and so forth.\ntopn: Number of equations to return to the host process, and to   consider for the hall of fame.\ncomplexity_of_operators: What complexity should be assigned to each operator,   and the occurrence of a constant or variable. By default, this is 1   for all operators. Can be a real number as well, in which case   the complexity of an expression will be rounded to the nearest integer.   Input this in the form of, e.g., [(^) => 3, sin => 2].\ncomplexity_of_constants: What complexity should be assigned to use of a constant.   By default, this is 1.\ncomplexity_of_variables: What complexity should be assigned to use of a variable,   which can also be a vector indicating different per-variable complexity.   By default, this is 1.\ncomplexity_mapping: Alternatively, you can pass a function that takes   the expression as input and returns the complexity. Make sure that   this operates on AbstractExpression (and unpacks to AbstractExpressionNode),   and returns an integer.\nalpha: The probability of accepting an equation mutation   during regularized evolution is given by exp(-delta_loss/(alpha * T)),   where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.\nmaxsize: Maximum size of equations during the search.\nmaxdepth: Maximum depth of equations during the search, by default   this is set equal to the maxsize.\nparsimony: A multiplicative factor for how much complexity is   punished.\ndimensional_constraint_penalty: An additive factor if the dimensional   constraint is violated.\ndimensionless_constants_only: Whether to only allow dimensionless   constants.\nuse_frequency: Whether to use a parsimony that adapts to the   relative proportion of equations at each complexity; this will   ensure that there are a balanced number of equations considered   for every complexity.\nuse_frequency_in_tournament: Whether to use the adaptive parsimony described   above inside the score, rather than just at the mutation accept/reject stage.\nadaptive_parsimony_scaling: How much to scale the adaptive parsimony term   in the loss. Increase this if the search is spending too much time   optimizing the most complex equations.\nturbo: Whether to use LoopVectorization.@turbo to evaluate expressions.   This can be significantly faster, but is only compatible with certain   operators. Experimental!\nbumper: Whether to use Bumper.jl for faster evaluation. Experimental!\nmigration: Whether to migrate equations between processes.\nhof_migration: Whether to migrate equations from the hall of fame   to processes.\nfraction_replaced: What fraction of each population to replace with   migrated equations at the end of each cycle.\nfraction_replaced_hof: What fraction to replace with hall of fame   equations at the end of each cycle.\nshould_simplify: Whether to simplify equations. If you   pass a custom objective, this will be set to false.\nshould_optimize_constants: Whether to use an optimization algorithm   to periodically optimize constants in equations.\noptimizer_algorithm: Select algorithm to use for optimizing constants. Default   is Optim.BFGS(linesearch=LineSearches.BackTracking()).\noptimizer_nrestarts: How many different random starting positions to consider   for optimization of constants.\noptimizer_probability: Probability of performing optimization of constants at   the end of a given iteration.\noptimizer_iterations: How many optimization iterations to perform. This gets   passed to Optim.Options as iterations. The default is 8.\noptimizer_f_calls_limit: How many function calls to allow during optimization.   This gets passed to Optim.Options as f_calls_limit. The default is   10_000.\noptimizer_options: General options for the constant optimization. For details   we refer to the documentation on Optim.Options from the Optim.jl package.   Options can be provided here as NamedTuple, e.g. (iterations=16,), as a   Dict, e.g. Dict(:x_tol => 1.0e-32,), or as an Optim.Options instance.\nautodiff_backend: The backend to use for differentiation, which should be   an instance of AbstractADType (see ADTypes.jl).   Default is nothing, which means Optim.jl will estimate gradients (likely   with finite differences). You can also pass a symbolic version of the backend   type, such as :Zygote for Zygote, :Enzyme, etc. Most backends will not   work, and many will never work due to incompatibilities, though support for some   is gradually being added.\nperturbation_factor: When mutating a constant, either   multiply or divide by (1+perturbation_factor)^(rand()+1).\nprobability_negate_constant: Probability of negating a constant in the equation   when mutating it.\nmutation_weights: Relative probabilities of the mutations. The struct   MutationWeights (or any AbstractMutationWeights) should be passed to these options.   See its documentation on MutationWeights for the different weights.\ncrossover_probability: Probability of performing crossover.\nannealing: Whether to use simulated annealing.\nwarmup_maxsize_by: Whether to slowly increase the max size from 5 up to   maxsize. If nonzero, specifies the fraction through the search   at which the maxsize should be reached.\nverbosity: Whether to print debugging statements or   not.\nprint_precision: How many digits to print when printing   equations. By default, this is 5.\noutput_directory: The base directory to save output files to. Files   will be saved in a subdirectory according to the run ID. By default,   this is ./outputs.\nsave_to_file: Whether to save equations to a file during the search.\nbin_constraints: See constraints. This is the same, but specified for binary   operators only (for example, if you have an operator that is both a binary   and unary operator).\nuna_constraints: Likewise, for unary operators.\nseed: What random seed to use. nothing uses no seed.\nprogress: Whether to use a progress bar output (verbosity will   have no effect).\nearly_stop_condition: Float - whether to stop early if the mean loss gets below this value.   Function - a function taking (loss, complexity) as arguments and returning true or false.\ntimeout_in_seconds: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).\nmax_evals: Int (or Nothing) - the maximum number of evaluations of expressions to perform.\nskip_mutation_failures: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated   expression with the original expression and proceed normally.\nnested_constraints: Specifies how many times a combination of operators can be nested. For example,   [sin => [cos => 0], cos => [cos => 2]] specifies that cos may never appear within a sin,   but sin can be nested with itself an unlimited number of times. The second term specifies that cos   can be nested up to 2 times within a cos, so that cos(cos(cos(x))) is allowed (as well as any combination   of + or - within it), but cos(cos(cos(cos(x)))) is not allowed. When an operator is not specified,   it is assumed that it can be nested an unlimited number of times. This requires that there is no operator   which is used both in the unary operators and the binary operators (e.g., - could be both subtract, and negation).   For binary operators, both arguments are treated the same way, and the max of each argument is constrained.\ndeterministic: Use a global counter for the birth time, rather than calls to time(). This gives   perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used   in serial mode.\ndefine_helper_functions: Whether to define helper functions   for constructing and evaluating trees.\nniterations::Int=10: The number of iterations to perform the search.   More iterations will improve the results.\nparallelism=:multithreading: What parallelism mode to use.   The options are :multithreading, :multiprocessing, and :serial.   By default, multithreading will be used. Multithreading uses less memory,   but multiprocessing can handle multi-node compute. If using :multithreading   mode, the number of threads available to julia are used. If using   :multiprocessing, numprocs processes will be created dynamically if   procs is unset. If you have already allocated processes, pass them   to the procs argument and they will be used.   You may also pass a string instead of a symbol, like \"multithreading\".\nnumprocs::Union{Int, Nothing}=nothing:  The number of processes to use,   if you want equation_search to set this up automatically. By default   this will be 4, but can be any number (you should pick a number <=   the number of cores available).\nprocs::Union{Vector{Int}, Nothing}=nothing: If you have set up   a distributed run manually with procs = addprocs() and @everywhere,   pass the procs to this keyword argument.\naddprocs_function::Union{Function, Nothing}=nothing: If using multiprocessing   (parallelism=:multithreading), and are not passing procs manually,   then they will be allocated dynamically using addprocs. However,   you may also pass a custom function to use instead of addprocs.   This function should take a single positional argument,   which is the number of processes to use, as well as the lazy keyword argument.   For example, if set up on a slurm cluster, you could pass   addprocs_function = addprocs_slurm, which will set up slurm processes.\nheap_size_hint_in_bytes::Union{Int,Nothing}=nothing: On Julia 1.9+, you may set the --heap-size-hint   flag on Julia processes, recommending garbage collection once a process   is close to the recommended size. This is important for long-running distributed   jobs where each process has an independent memory, and can help avoid   out-of-memory errors. By default, this is set to Sys.free_memory() / numprocs.\nruntests::Bool=true: Whether to run (quick) tests before starting the   search, to see if there will be any problems during the equation search   related to the host environment.\nrun_id::Union{String,Nothing}=nothing: A unique identifier for the run.   This will be used to store outputs from the run in the outputs directory.   If not specified, a unique ID will be generated.\nloss_type::Type=Nothing: If you would like to use a different type   for the loss than for the data you passed, specify the type here.   Note that if you pass complex data ::Complex{L}, then the loss   type will automatically be set to L.\nselection_method::Function: Function to selection expression from   the Pareto frontier for use in predict.   See SymbolicRegression.MLJInterfaceModule.choose_best for an example.   This function should return a single integer specifying   the index of the expression to use. By default, this maximizes   the score (a pound-for-pound rating) of expressions reaching the threshold   of 1.5x the minimum loss. To override this at prediction time, you can pass   a named tuple with keys data and idx to predict. See the Operations   section for details.\ndimensions_type::AbstractDimensions: The type of dimensions to use when storing   the units of the data. By default this is DynamicQuantities.SymbolicDimensions.\n\nOperations\n\npredict(mach, Xnew): Return predictions of the target given features Xnew, which   should have same scitype as X above. The expression used for prediction is defined   by the selection_method function, which can be seen by viewing report(mach).best_idx.\npredict(mach, (data=Xnew, idx=i)): Return predictions of the target given features   Xnew, which should have same scitype as X above. By passing a named tuple with keys   data and idx, you are able to specify the equation you wish to evaluate in idx.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nbest_idx::Int: The index of the best expression in the Pareto frontier,  as determined by the selection_method function. Override in predict by passing   a named tuple with keys data and idx.\nequations::Vector{Node{T}}: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). T is equal to the element type of the passed data.\nequation_strings::Vector{String}: The expressions discovered by the search, represented as strings for easy inspection.\n\nReport\n\nThe fields of report(mach) are:\n\nbest_idx::Int: The index of the best expression in the Pareto frontier,  as determined by the selection_method function. Override in predict by passing  a named tuple with keys data and idx.\nequations::Vector{Node{T}}: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity).\nequation_strings::Vector{String}: The expressions discovered by the search, represented as strings for easy inspection.\ncomplexities::Vector{Int}: The complexity of each expression in the Pareto frontier.\nlosses::Vector{L}: The loss of each expression in the Pareto frontier, according to the loss function specified in the model. The type L is the loss type, which is usually the same as the element type of data passed (i.e., T), but can differ if complex data types are passed.\nscores::Vector{L}: A metric which considers both the complexity and loss of an expression, equal to the change in the log-loss divided by the change in complexity, relative to the previous expression along the Pareto frontier. A larger score aims to indicate an expression is more likely to be the true expression generating the data, but this is very problem-dependent and generally several other factors should be considered.\n\nExamples\n\nusing MLJ\nSRRegressor = @load SRRegressor pkg=SymbolicRegression\nX, y = @load_boston\nmodel = SRRegressor(binary_operators=[+, -, *], unary_operators=[exp], niterations=100)\nmach = machine(model, X, y)\nfit!(mach)\ny_hat = predict(mach, X)\n# View the equation used:\nr = report(mach)\nprintln(\"Equation used:\", r.equation_strings[r.best_idx])\n\nWith units and variable names:\n\nusing MLJ\nusing DynamicQuantities\nSRegressor = @load SRRegressor pkg=SymbolicRegression\n\nX = (; x1=rand(32) .* us\"km/h\", x2=rand(32) .* us\"km\")\ny = @. X.x2 / X.x1 + 0.5us\"h\"\nmodel = SRRegressor(binary_operators=[+, -, *, /])\nmach = machine(model, X, y)\nfit!(mach)\ny_hat = predict(mach, X)\n# View the equation used:\nr = report(mach)\nprintln(\"Equation used:\", r.equation_strings[r.best_idx])\n\nSee also MultitargetSRRegressor.\n\n\n\n\n\n","category":"type"},{"location":"api/#SymbolicRegression.MLJInterfaceModule.MultitargetSRRegressor","page":"API","title":"SymbolicRegression.MLJInterfaceModule.MultitargetSRRegressor","text":"MultitargetSRRegressor\n\nA model type for constructing a Multi-Target Symbolic Regression via Evolutionary Search, based on SymbolicRegression.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultitargetSRRegressor = @load MultitargetSRRegressor pkg=SymbolicRegression\n\nDo model = MultitargetSRRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultitargetSRRegressor(defaults=...).\n\nMulti-target Symbolic Regression regressor (MultitargetSRRegressor) conducts several searches for expressions that predict each target variable from a set of input variables. All data is assumed to be Continuous. The search is performed using an evolutionary algorithm. This algorithm is described in the paper https://arxiv.org/abs/2305.01582.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nOR\n\nmach = machine(model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype\n\nContinuous; check column scitypes with schema(X). Variable names in discovered expressions will be taken from the column names of X, if available. Units in columns of X (use DynamicQuantities for units) will trigger dimensional analysis to be used.\n\ny is the target, which can be any table of target variables whose element scitype is Continuous; check the scitype with schema(y). Units in columns of y (use DynamicQuantities for units) will trigger dimensional analysis to be used.\nw is the observation weights which can either be nothing (default) or an AbstractVector whose element scitype is Count or Continuous. The same weights are used for all targets.\n\nTrain the machine using fit!(mach), inspect the discovered expressions with report(mach), and predict on new data with predict(mach, Xnew). Note that unlike other regressors, symbolic regression stores a list of lists of trained models. The models chosen from each of these lists is defined by the function selection_method keyword argument, which by default balances accuracy and complexity. You can override this at prediction time by passing a named tuple with keys data and idx.\n\nHyper-parameters\n\ndefaults: What set of defaults to use for Options. The default,   nothing, will simply take the default options from the current version of SymbolicRegression.   However, you may also select the defaults from an earlier version, such as v\"0.24.5\".\nbinary_operators: Vector of binary operators (functions) to use.   Each operator should be defined for two input scalars,   and one output scalar. All operators   need to be defined over the entire real line (excluding infinity - these   are stopped before they are input), or return NaN where not defined.   For speed, define it so it takes two reals   of the same type as input, and outputs the same type. For the SymbolicUtils   simplification backend, you will need to define a generic method of the   operator so it takes arbitrary types.\nunary_operators: Same, but for   unary operators (one input scalar, gives an output scalar).\nconstraints: Array of pairs specifying size constraints   for each operator. The constraints for a binary operator should be a 2-tuple   (e.g., (-1, -1)) and the constraints for a unary operator should be an Int.   A size constraint is a limit to the size of the subtree   in each argument of an operator. e.g., [(^)=>(-1, 3)] means that the   ^ operator can have arbitrary size (-1) in its left argument,   but a maximum size of 3 in its right argument. Default is   no constraints.\nbatching: Whether to evolve based on small mini-batches of data,   rather than the entire dataset.\nbatch_size: What batch size to use if using batching.\nelementwise_loss: What elementwise loss function to use. Can be one of   the following losses, or any other loss of type   SupervisedLoss. You can also pass a function that takes   a scalar target (left argument), and scalar predicted (right   argument), and returns a scalar. This will be averaged   over the predicted data. If weights are supplied, your   function should take a third argument for the weight scalar.   Included losses:       Regression:           - LPDistLoss{P}(),           - L1DistLoss(),           - L2DistLoss() (mean square),           - LogitDistLoss(),           - HuberLoss(d),           - L1EpsilonInsLoss(ϵ),           - L2EpsilonInsLoss(ϵ),           - PeriodicLoss(c),           - QuantileLoss(τ),       Classification:           - ZeroOneLoss(),           - PerceptronLoss(),           - L1HingeLoss(),           - SmoothedL1HingeLoss(γ),           - ModifiedHuberLoss(),           - L2MarginLoss(),           - ExpLoss(),           - SigmoidLoss(),           - DWDMarginLoss(q).\nloss_function: Alternatively, you may redefine the loss used   as any function of tree::AbstractExpressionNode{T}, dataset::Dataset{T},   and options::AbstractOptions, so long as you output a non-negative   scalar of type T. This is useful if you want to use a loss   that takes into account derivatives, or correlations across   the dataset. This also means you could use a custom evaluation   for a particular expression. If you are using   batching=true, then your function should   accept a fourth argument idx, which is either nothing   (indicating that the full dataset should be used), or a vector   of indices to use for the batch.   For example,\n  function my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\n      prediction, flag = eval_tree_array(tree, dataset.X, options)\n      if !flag\n          return L(Inf)\n      end\n      return sum((prediction .- dataset.y) .^ 2) / dataset.n\n  end\nexpression_type::Type{E}=Expression: The type of expression to use.   For example, Expression.\nnode_type::Type{N}=default_node_type(Expression): The type of node to use for the search.   For example, Node or GraphNode. The default is computed by default_node_type(expression_type).\npopulations: How many populations of equations to use.\npopulation_size: How many equations in each population.\nncycles_per_iteration: How many generations to consider per iteration.\ntournament_selection_n: Number of expressions considered in each tournament.\ntournament_selection_p: The fittest expression in a tournament is to be   selected with probability p, the next fittest with probability p*(1-p),   and so forth.\ntopn: Number of equations to return to the host process, and to   consider for the hall of fame.\ncomplexity_of_operators: What complexity should be assigned to each operator,   and the occurrence of a constant or variable. By default, this is 1   for all operators. Can be a real number as well, in which case   the complexity of an expression will be rounded to the nearest integer.   Input this in the form of, e.g., [(^) => 3, sin => 2].\ncomplexity_of_constants: What complexity should be assigned to use of a constant.   By default, this is 1.\ncomplexity_of_variables: What complexity should be assigned to use of a variable,   which can also be a vector indicating different per-variable complexity.   By default, this is 1.\ncomplexity_mapping: Alternatively, you can pass a function that takes   the expression as input and returns the complexity. Make sure that   this operates on AbstractExpression (and unpacks to AbstractExpressionNode),   and returns an integer.\nalpha: The probability of accepting an equation mutation   during regularized evolution is given by exp(-delta_loss/(alpha * T)),   where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.\nmaxsize: Maximum size of equations during the search.\nmaxdepth: Maximum depth of equations during the search, by default   this is set equal to the maxsize.\nparsimony: A multiplicative factor for how much complexity is   punished.\ndimensional_constraint_penalty: An additive factor if the dimensional   constraint is violated.\ndimensionless_constants_only: Whether to only allow dimensionless   constants.\nuse_frequency: Whether to use a parsimony that adapts to the   relative proportion of equations at each complexity; this will   ensure that there are a balanced number of equations considered   for every complexity.\nuse_frequency_in_tournament: Whether to use the adaptive parsimony described   above inside the score, rather than just at the mutation accept/reject stage.\nadaptive_parsimony_scaling: How much to scale the adaptive parsimony term   in the loss. Increase this if the search is spending too much time   optimizing the most complex equations.\nturbo: Whether to use LoopVectorization.@turbo to evaluate expressions.   This can be significantly faster, but is only compatible with certain   operators. Experimental!\nbumper: Whether to use Bumper.jl for faster evaluation. Experimental!\nmigration: Whether to migrate equations between processes.\nhof_migration: Whether to migrate equations from the hall of fame   to processes.\nfraction_replaced: What fraction of each population to replace with   migrated equations at the end of each cycle.\nfraction_replaced_hof: What fraction to replace with hall of fame   equations at the end of each cycle.\nshould_simplify: Whether to simplify equations. If you   pass a custom objective, this will be set to false.\nshould_optimize_constants: Whether to use an optimization algorithm   to periodically optimize constants in equations.\noptimizer_algorithm: Select algorithm to use for optimizing constants. Default   is Optim.BFGS(linesearch=LineSearches.BackTracking()).\noptimizer_nrestarts: How many different random starting positions to consider   for optimization of constants.\noptimizer_probability: Probability of performing optimization of constants at   the end of a given iteration.\noptimizer_iterations: How many optimization iterations to perform. This gets   passed to Optim.Options as iterations. The default is 8.\noptimizer_f_calls_limit: How many function calls to allow during optimization.   This gets passed to Optim.Options as f_calls_limit. The default is   10_000.\noptimizer_options: General options for the constant optimization. For details   we refer to the documentation on Optim.Options from the Optim.jl package.   Options can be provided here as NamedTuple, e.g. (iterations=16,), as a   Dict, e.g. Dict(:x_tol => 1.0e-32,), or as an Optim.Options instance.\nautodiff_backend: The backend to use for differentiation, which should be   an instance of AbstractADType (see ADTypes.jl).   Default is nothing, which means Optim.jl will estimate gradients (likely   with finite differences). You can also pass a symbolic version of the backend   type, such as :Zygote for Zygote, :Enzyme, etc. Most backends will not   work, and many will never work due to incompatibilities, though support for some   is gradually being added.\nperturbation_factor: When mutating a constant, either   multiply or divide by (1+perturbation_factor)^(rand()+1).\nprobability_negate_constant: Probability of negating a constant in the equation   when mutating it.\nmutation_weights: Relative probabilities of the mutations. The struct   MutationWeights (or any AbstractMutationWeights) should be passed to these options.   See its documentation on MutationWeights for the different weights.\ncrossover_probability: Probability of performing crossover.\nannealing: Whether to use simulated annealing.\nwarmup_maxsize_by: Whether to slowly increase the max size from 5 up to   maxsize. If nonzero, specifies the fraction through the search   at which the maxsize should be reached.\nverbosity: Whether to print debugging statements or   not.\nprint_precision: How many digits to print when printing   equations. By default, this is 5.\noutput_directory: The base directory to save output files to. Files   will be saved in a subdirectory according to the run ID. By default,   this is ./outputs.\nsave_to_file: Whether to save equations to a file during the search.\nbin_constraints: See constraints. This is the same, but specified for binary   operators only (for example, if you have an operator that is both a binary   and unary operator).\nuna_constraints: Likewise, for unary operators.\nseed: What random seed to use. nothing uses no seed.\nprogress: Whether to use a progress bar output (verbosity will   have no effect).\nearly_stop_condition: Float - whether to stop early if the mean loss gets below this value.   Function - a function taking (loss, complexity) as arguments and returning true or false.\ntimeout_in_seconds: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).\nmax_evals: Int (or Nothing) - the maximum number of evaluations of expressions to perform.\nskip_mutation_failures: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated   expression with the original expression and proceed normally.\nnested_constraints: Specifies how many times a combination of operators can be nested. For example,   [sin => [cos => 0], cos => [cos => 2]] specifies that cos may never appear within a sin,   but sin can be nested with itself an unlimited number of times. The second term specifies that cos   can be nested up to 2 times within a cos, so that cos(cos(cos(x))) is allowed (as well as any combination   of + or - within it), but cos(cos(cos(cos(x)))) is not allowed. When an operator is not specified,   it is assumed that it can be nested an unlimited number of times. This requires that there is no operator   which is used both in the unary operators and the binary operators (e.g., - could be both subtract, and negation).   For binary operators, both arguments are treated the same way, and the max of each argument is constrained.\ndeterministic: Use a global counter for the birth time, rather than calls to time(). This gives   perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used   in serial mode.\ndefine_helper_functions: Whether to define helper functions   for constructing and evaluating trees.\nniterations::Int=10: The number of iterations to perform the search.   More iterations will improve the results.\nparallelism=:multithreading: What parallelism mode to use.   The options are :multithreading, :multiprocessing, and :serial.   By default, multithreading will be used. Multithreading uses less memory,   but multiprocessing can handle multi-node compute. If using :multithreading   mode, the number of threads available to julia are used. If using   :multiprocessing, numprocs processes will be created dynamically if   procs is unset. If you have already allocated processes, pass them   to the procs argument and they will be used.   You may also pass a string instead of a symbol, like \"multithreading\".\nnumprocs::Union{Int, Nothing}=nothing:  The number of processes to use,   if you want equation_search to set this up automatically. By default   this will be 4, but can be any number (you should pick a number <=   the number of cores available).\nprocs::Union{Vector{Int}, Nothing}=nothing: If you have set up   a distributed run manually with procs = addprocs() and @everywhere,   pass the procs to this keyword argument.\naddprocs_function::Union{Function, Nothing}=nothing: If using multiprocessing   (parallelism=:multithreading), and are not passing procs manually,   then they will be allocated dynamically using addprocs. However,   you may also pass a custom function to use instead of addprocs.   This function should take a single positional argument,   which is the number of processes to use, as well as the lazy keyword argument.   For example, if set up on a slurm cluster, you could pass   addprocs_function = addprocs_slurm, which will set up slurm processes.\nheap_size_hint_in_bytes::Union{Int,Nothing}=nothing: On Julia 1.9+, you may set the --heap-size-hint   flag on Julia processes, recommending garbage collection once a process   is close to the recommended size. This is important for long-running distributed   jobs where each process has an independent memory, and can help avoid   out-of-memory errors. By default, this is set to Sys.free_memory() / numprocs.\nruntests::Bool=true: Whether to run (quick) tests before starting the   search, to see if there will be any problems during the equation search   related to the host environment.\nrun_id::Union{String,Nothing}=nothing: A unique identifier for the run.   This will be used to store outputs from the run in the outputs directory.   If not specified, a unique ID will be generated.\nloss_type::Type=Nothing: If you would like to use a different type   for the loss than for the data you passed, specify the type here.   Note that if you pass complex data ::Complex{L}, then the loss   type will automatically be set to L.\nselection_method::Function: Function to selection expression from   the Pareto frontier for use in predict.   See SymbolicRegression.MLJInterfaceModule.choose_best for an example.   This function should return a single integer specifying   the index of the expression to use. By default, this maximizes   the score (a pound-for-pound rating) of expressions reaching the threshold   of 1.5x the minimum loss. To override this at prediction time, you can pass   a named tuple with keys data and idx to predict. See the Operations   section for details.\ndimensions_type::AbstractDimensions: The type of dimensions to use when storing   the units of the data. By default this is DynamicQuantities.SymbolicDimensions.\n\nOperations\n\npredict(mach, Xnew): Return predictions of the target given features Xnew, which   should have same scitype as X above. The expression used for prediction is defined   by the selection_method function, which can be seen by viewing report(mach).best_idx.\npredict(mach, (data=Xnew, idx=i)): Return predictions of the target given features   Xnew, which should have same scitype as X above. By passing a named tuple with keys   data and idx, you are able to specify the equation you wish to evaluate in idx.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nbest_idx::Vector{Int}: The index of the best expression in each Pareto frontier, as determined by the selection_method function. Override in predict by passing a named tuple with keys data and idx.\nequations::Vector{Vector{Node{T}}}: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). The outer vector is indexed by target variable, and the inner vector is ordered by increasing complexity. T is equal to the element type of the passed data.\nequation_strings::Vector{Vector{String}}: The expressions discovered by the search, represented as strings for easy inspection.\n\nReport\n\nThe fields of report(mach) are:\n\nbest_idx::Vector{Int}: The index of the best expression in each Pareto frontier,  as determined by the selection_method function. Override in predict by passing  a named tuple with keys data and idx.\nequations::Vector{Vector{Node{T}}}: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). The outer vector is indexed by target variable, and the inner vector is ordered by increasing complexity.\nequation_strings::Vector{Vector{String}}: The expressions discovered by the search, represented as strings for easy inspection.\ncomplexities::Vector{Vector{Int}}: The complexity of each expression in each Pareto frontier.\nlosses::Vector{Vector{L}}: The loss of each expression in each Pareto frontier, according to the loss function specified in the model. The type L is the loss type, which is usually the same as the element type of data passed (i.e., T), but can differ if complex data types are passed.\nscores::Vector{Vector{L}}: A metric which considers both the complexity and loss of an expression, equal to the change in the log-loss divided by the change in complexity, relative to the previous expression along the Pareto frontier. A larger score aims to indicate an expression is more likely to be the true expression generating the data, but this is very problem-dependent and generally several other factors should be considered.\n\nExamples\n\nusing MLJ\nMultitargetSRRegressor = @load MultitargetSRRegressor pkg=SymbolicRegression\nX = (a=rand(100), b=rand(100), c=rand(100))\nY = (y1=(@. cos(X.c) * 2.1 - 0.9), y2=(@. X.a * X.b + X.c))\nmodel = MultitargetSRRegressor(binary_operators=[+, -, *], unary_operators=[exp], niterations=100)\nmach = machine(model, X, Y)\nfit!(mach)\ny_hat = predict(mach, X)\n# View the equations used:\nr = report(mach)\nfor (output_index, (eq, i)) in enumerate(zip(r.equation_strings, r.best_idx))\n    println(\"Equation used for \", output_index, \": \", eq[i])\nend\n\nSee also SRRegressor.\n\n\n\n\n\n","category":"type"},{"location":"api/#Low-Level-API","page":"API","title":"Low-Level API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"equation_search","category":"page"},{"location":"api/#SymbolicRegression.equation_search","page":"API","title":"SymbolicRegression.equation_search","text":"equation_search(X, y[; kws...])\n\nPerform a distributed equation search for functions f_i which describe the mapping f_i(X[:, j]) ≈ y[i, j]. Options are configured using SymbolicRegression.Options(...), which should be passed as a keyword argument to options. One can turn off parallelism with numprocs=0, which is useful for debugging and profiling.\n\nArguments\n\nX::AbstractMatrix{T}:  The input dataset to predict y from.   The first dimension is features, the second dimension is rows.\ny::Union{AbstractMatrix{T}, AbstractVector{T}}: The values to predict. The first dimension   is the output feature to predict with each equation, and the   second dimension is rows.\nniterations::Int=100: The number of iterations to perform the search.   More iterations will improve the results.\nweights::Union{AbstractMatrix{T}, AbstractVector{T}, Nothing}=nothing: Optionally   weight the loss for each y by this value (same shape as y).\noptions::AbstractOptions=Options(): The options for the search, such as   which operators to use, evolution hyperparameters, etc.\nvariable_names::Union{Vector{String}, Nothing}=nothing: The names   of each feature in X, which will be used during printing of equations.\ndisplay_variable_names::Union{Vector{String}, Nothing}=variable_names: Names   to use when printing expressions during the search, but not when saving   to an equation file.\ny_variable_names::Union{String,AbstractVector{String},Nothing}=nothing: The   names of each output feature in y, which will be used during printing   of equations.\nparallelism=:multithreading: What parallelism mode to use.   The options are :multithreading, :multiprocessing, and :serial.   By default, multithreading will be used. Multithreading uses less memory,   but multiprocessing can handle multi-node compute. If using :multithreading   mode, the number of threads available to julia are used. If using   :multiprocessing, numprocs processes will be created dynamically if   procs is unset. If you have already allocated processes, pass them   to the procs argument and they will be used.   You may also pass a string instead of a symbol, like \"multithreading\".\nnumprocs::Union{Int, Nothing}=nothing:  The number of processes to use,   if you want equation_search to set this up automatically. By default   this will be 4, but can be any number (you should pick a number <=   the number of cores available).\nprocs::Union{Vector{Int}, Nothing}=nothing: If you have set up   a distributed run manually with procs = addprocs() and @everywhere,   pass the procs to this keyword argument.\naddprocs_function::Union{Function, Nothing}=nothing: If using multiprocessing   (parallelism=:multiprocessing), and are not passing procs manually,   then they will be allocated dynamically using addprocs. However,   you may also pass a custom function to use instead of addprocs.   This function should take a single positional argument,   which is the number of processes to use, as well as the lazy keyword argument.   For example, if set up on a slurm cluster, you could pass   addprocs_function = addprocs_slurm, which will set up slurm processes.\nheap_size_hint_in_bytes::Union{Int,Nothing}=nothing: On Julia 1.9+, you may set the --heap-size-hint   flag on Julia processes, recommending garbage collection once a process   is close to the recommended size. This is important for long-running distributed   jobs where each process has an independent memory, and can help avoid   out-of-memory errors. By default, this is set to Sys.free_memory() / numprocs.\nruntests::Bool=true: Whether to run (quick) tests before starting the   search, to see if there will be any problems during the equation search   related to the host environment.\nsaved_state=nothing: If you have already   run equation_search and want to resume it, pass the state here.   To get this to work, you need to have set returnstate=true,   which will cause `equationsearch` to return the state. The second   element of the state is the regular return value with the hall of fame.   Note that you cannot change the operators or dataset, but most other options   should be changeable.\nreturn_state::Union{Bool, Nothing}=nothing: Whether to return the   state of the search for warm starts. By default this is false.\nloss_type::Type=Nothing: If you would like to use a different type   for the loss than for the data you passed, specify the type here.   Note that if you pass complex data ::Complex{L}, then the loss   type will automatically be set to L.\nverbosity: Whether to print debugging statements or not.\nlogger::Union{AbstractSRLogger,Nothing}=nothing: An optional logger to record   the progress of the search. You can use an SRLogger to wrap a custom logger,   or pass nothing to disable logging.\nprogress: Whether to use a progress bar output. Only available for   single target output.\nX_units::Union{AbstractVector,Nothing}=nothing: The units of the dataset,   to be used for dimensional constraints. For example, if X_units=[\"kg\", \"m\"],   then the first feature will have units of kilograms, and the second will   have units of meters.\ny_units=nothing: The units of the output, to be used for dimensional constraints.   If y is a matrix, then this can be a vector of units, in which case   each element corresponds to each output feature.\n\nReturns\n\nhallOfFame::HallOfFame: The best equations seen during the search.   hallOfFame.members gives an array of PopMember objects, which   have their tree (equation) stored in .tree. Their score (loss)   is given in .score. The array of PopMember objects   is enumerated by size from 1 to options.maxsize.\n\n\n\n\n\n","category":"function"},{"location":"api/#Options","page":"API","title":"Options","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Options\nMutationWeights","category":"page"},{"location":"api/#SymbolicRegression.CoreModule.OptionsStructModule.Options","page":"API","title":"SymbolicRegression.CoreModule.OptionsStructModule.Options","text":"Options(;kws...) <: AbstractOptions\n\nConstruct options for equation_search and other functions. The current arguments have been tuned using the median values from https://github.com/MilesCranmer/PySR/discussions/115.\n\nArguments\n\ndefaults: What set of defaults to use for Options. The default,   nothing, will simply take the default options from the current version of SymbolicRegression.   However, you may also select the defaults from an earlier version, such as v\"0.24.5\".\nbinary_operators: Vector of binary operators (functions) to use.   Each operator should be defined for two input scalars,   and one output scalar. All operators   need to be defined over the entire real line (excluding infinity - these   are stopped before they are input), or return NaN where not defined.   For speed, define it so it takes two reals   of the same type as input, and outputs the same type. For the SymbolicUtils   simplification backend, you will need to define a generic method of the   operator so it takes arbitrary types.\nunary_operators: Same, but for   unary operators (one input scalar, gives an output scalar).\nconstraints: Array of pairs specifying size constraints   for each operator. The constraints for a binary operator should be a 2-tuple   (e.g., (-1, -1)) and the constraints for a unary operator should be an Int.   A size constraint is a limit to the size of the subtree   in each argument of an operator. e.g., [(^)=>(-1, 3)] means that the   ^ operator can have arbitrary size (-1) in its left argument,   but a maximum size of 3 in its right argument. Default is   no constraints.\nbatching: Whether to evolve based on small mini-batches of data,   rather than the entire dataset.\nbatch_size: What batch size to use if using batching.\nelementwise_loss: What elementwise loss function to use. Can be one of   the following losses, or any other loss of type   SupervisedLoss. You can also pass a function that takes   a scalar target (left argument), and scalar predicted (right   argument), and returns a scalar. This will be averaged   over the predicted data. If weights are supplied, your   function should take a third argument for the weight scalar.   Included losses:       Regression:           - LPDistLoss{P}(),           - L1DistLoss(),           - L2DistLoss() (mean square),           - LogitDistLoss(),           - HuberLoss(d),           - L1EpsilonInsLoss(ϵ),           - L2EpsilonInsLoss(ϵ),           - PeriodicLoss(c),           - QuantileLoss(τ),       Classification:           - ZeroOneLoss(),           - PerceptronLoss(),           - L1HingeLoss(),           - SmoothedL1HingeLoss(γ),           - ModifiedHuberLoss(),           - L2MarginLoss(),           - ExpLoss(),           - SigmoidLoss(),           - DWDMarginLoss(q).\nloss_function: Alternatively, you may redefine the loss used   as any function of tree::AbstractExpressionNode{T}, dataset::Dataset{T},   and options::AbstractOptions, so long as you output a non-negative   scalar of type T. This is useful if you want to use a loss   that takes into account derivatives, or correlations across   the dataset. This also means you could use a custom evaluation   for a particular expression. If you are using   batching=true, then your function should   accept a fourth argument idx, which is either nothing   (indicating that the full dataset should be used), or a vector   of indices to use for the batch.   For example,\n  function my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\n      prediction, flag = eval_tree_array(tree, dataset.X, options)\n      if !flag\n          return L(Inf)\n      end\n      return sum((prediction .- dataset.y) .^ 2) / dataset.n\n  end\nexpression_type::Type{E}=Expression: The type of expression to use.   For example, Expression.\nnode_type::Type{N}=default_node_type(Expression): The type of node to use for the search.   For example, Node or GraphNode. The default is computed by default_node_type(expression_type).\npopulations: How many populations of equations to use.\npopulation_size: How many equations in each population.\nncycles_per_iteration: How many generations to consider per iteration.\ntournament_selection_n: Number of expressions considered in each tournament.\ntournament_selection_p: The fittest expression in a tournament is to be   selected with probability p, the next fittest with probability p*(1-p),   and so forth.\ntopn: Number of equations to return to the host process, and to   consider for the hall of fame.\ncomplexity_of_operators: What complexity should be assigned to each operator,   and the occurrence of a constant or variable. By default, this is 1   for all operators. Can be a real number as well, in which case   the complexity of an expression will be rounded to the nearest integer.   Input this in the form of, e.g., [(^) => 3, sin => 2].\ncomplexity_of_constants: What complexity should be assigned to use of a constant.   By default, this is 1.\ncomplexity_of_variables: What complexity should be assigned to use of a variable,   which can also be a vector indicating different per-variable complexity.   By default, this is 1.\ncomplexity_mapping: Alternatively, you can pass a function that takes   the expression as input and returns the complexity. Make sure that   this operates on AbstractExpression (and unpacks to AbstractExpressionNode),   and returns an integer.\nalpha: The probability of accepting an equation mutation   during regularized evolution is given by exp(-delta_loss/(alpha * T)),   where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.\nmaxsize: Maximum size of equations during the search.\nmaxdepth: Maximum depth of equations during the search, by default   this is set equal to the maxsize.\nparsimony: A multiplicative factor for how much complexity is   punished.\ndimensional_constraint_penalty: An additive factor if the dimensional   constraint is violated.\ndimensionless_constants_only: Whether to only allow dimensionless   constants.\nuse_frequency: Whether to use a parsimony that adapts to the   relative proportion of equations at each complexity; this will   ensure that there are a balanced number of equations considered   for every complexity.\nuse_frequency_in_tournament: Whether to use the adaptive parsimony described   above inside the score, rather than just at the mutation accept/reject stage.\nadaptive_parsimony_scaling: How much to scale the adaptive parsimony term   in the loss. Increase this if the search is spending too much time   optimizing the most complex equations.\nturbo: Whether to use LoopVectorization.@turbo to evaluate expressions.   This can be significantly faster, but is only compatible with certain   operators. Experimental!\nbumper: Whether to use Bumper.jl for faster evaluation. Experimental!\nmigration: Whether to migrate equations between processes.\nhof_migration: Whether to migrate equations from the hall of fame   to processes.\nfraction_replaced: What fraction of each population to replace with   migrated equations at the end of each cycle.\nfraction_replaced_hof: What fraction to replace with hall of fame   equations at the end of each cycle.\nshould_simplify: Whether to simplify equations. If you   pass a custom objective, this will be set to false.\nshould_optimize_constants: Whether to use an optimization algorithm   to periodically optimize constants in equations.\noptimizer_algorithm: Select algorithm to use for optimizing constants. Default   is Optim.BFGS(linesearch=LineSearches.BackTracking()).\noptimizer_nrestarts: How many different random starting positions to consider   for optimization of constants.\noptimizer_probability: Probability of performing optimization of constants at   the end of a given iteration.\noptimizer_iterations: How many optimization iterations to perform. This gets   passed to Optim.Options as iterations. The default is 8.\noptimizer_f_calls_limit: How many function calls to allow during optimization.   This gets passed to Optim.Options as f_calls_limit. The default is   10_000.\noptimizer_options: General options for the constant optimization. For details   we refer to the documentation on Optim.Options from the Optim.jl package.   Options can be provided here as NamedTuple, e.g. (iterations=16,), as a   Dict, e.g. Dict(:x_tol => 1.0e-32,), or as an Optim.Options instance.\nautodiff_backend: The backend to use for differentiation, which should be   an instance of AbstractADType (see ADTypes.jl).   Default is nothing, which means Optim.jl will estimate gradients (likely   with finite differences). You can also pass a symbolic version of the backend   type, such as :Zygote for Zygote, :Enzyme, etc. Most backends will not   work, and many will never work due to incompatibilities, though support for some   is gradually being added.\nperturbation_factor: When mutating a constant, either   multiply or divide by (1+perturbation_factor)^(rand()+1).\nprobability_negate_constant: Probability of negating a constant in the equation   when mutating it.\nmutation_weights: Relative probabilities of the mutations. The struct   MutationWeights (or any AbstractMutationWeights) should be passed to these options.   See its documentation on MutationWeights for the different weights.\ncrossover_probability: Probability of performing crossover.\nannealing: Whether to use simulated annealing.\nwarmup_maxsize_by: Whether to slowly increase the max size from 5 up to   maxsize. If nonzero, specifies the fraction through the search   at which the maxsize should be reached.\nverbosity: Whether to print debugging statements or   not.\nprint_precision: How many digits to print when printing   equations. By default, this is 5.\noutput_directory: The base directory to save output files to. Files   will be saved in a subdirectory according to the run ID. By default,   this is ./outputs.\nsave_to_file: Whether to save equations to a file during the search.\nbin_constraints: See constraints. This is the same, but specified for binary   operators only (for example, if you have an operator that is both a binary   and unary operator).\nuna_constraints: Likewise, for unary operators.\nseed: What random seed to use. nothing uses no seed.\nprogress: Whether to use a progress bar output (verbosity will   have no effect).\nearly_stop_condition: Float - whether to stop early if the mean loss gets below this value.   Function - a function taking (loss, complexity) as arguments and returning true or false.\ntimeout_in_seconds: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).\nmax_evals: Int (or Nothing) - the maximum number of evaluations of expressions to perform.\nskip_mutation_failures: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated   expression with the original expression and proceed normally.\nnested_constraints: Specifies how many times a combination of operators can be nested. For example,   [sin => [cos => 0], cos => [cos => 2]] specifies that cos may never appear within a sin,   but sin can be nested with itself an unlimited number of times. The second term specifies that cos   can be nested up to 2 times within a cos, so that cos(cos(cos(x))) is allowed (as well as any combination   of + or - within it), but cos(cos(cos(cos(x)))) is not allowed. When an operator is not specified,   it is assumed that it can be nested an unlimited number of times. This requires that there is no operator   which is used both in the unary operators and the binary operators (e.g., - could be both subtract, and negation).   For binary operators, both arguments are treated the same way, and the max of each argument is constrained.\ndeterministic: Use a global counter for the birth time, rather than calls to time(). This gives   perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used   in serial mode.\ndefine_helper_functions: Whether to define helper functions   for constructing and evaluating trees.\n\n\n\n\n\n","category":"type"},{"location":"api/#SymbolicRegression.CoreModule.MutationWeightsModule.MutationWeights","page":"API","title":"SymbolicRegression.CoreModule.MutationWeightsModule.MutationWeights","text":"MutationWeights(;kws...) <: AbstractMutationWeights\n\nThis defines how often different mutations occur. These weightings will be normalized to sum to 1.0 after initialization.\n\nArguments\n\nmutate_constant::Float64: How often to mutate a constant.\nmutate_operator::Float64: How often to mutate an operator.\nswap_operands::Float64: How often to swap the operands of a binary operator.\nrotate_tree::Float64: How often to perform a tree rotation at a random node.\nadd_node::Float64: How often to append a node to the tree.\ninsert_node::Float64: How often to insert a node into the tree.\ndelete_node::Float64: How often to delete a node from the tree.\nsimplify::Float64: How often to simplify the tree.\nrandomize::Float64: How often to create a random tree.\ndo_nothing::Float64: How often to do nothing.\noptimize::Float64: How often to optimize the constants in the tree, as a mutation.   Note that this is different from optimizer_probability, which is   performed at the end of an iteration for all individuals.\nform_connection::Float64: Only used for GraphNode, not regular Node.   Otherwise, this will automatically be set to 0.0. How often to form a   connection between two nodes.\nbreak_connection::Float64: Only used for GraphNode, not regular Node.   Otherwise, this will automatically be set to 0.0. How often to break a   connection between two nodes.\n\nSee Also\n\nAbstractMutationWeights: Use to define custom mutation weight types.\n\n\n\n\n\n","category":"type"},{"location":"api/#Printing","page":"API","title":"Printing","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"string_tree","category":"page"},{"location":"api/#DynamicExpressions.StringsModule.string_tree","page":"API","title":"DynamicExpressions.StringsModule.string_tree","text":"string_tree(\n    tree::AbstractExpressionNode{T},\n    operators::Union{AbstractOperatorEnum,Nothing}=nothing;\n    f_variable::F1=string_variable,\n    f_constant::F2=string_constant,\n    variable_names::Union{Array{String,1},Nothing}=nothing,\n    # Deprecated\n    varMap=nothing,\n)::String where {T,F1<:Function,F2<:Function}\n\nConvert an equation to a string.\n\nArguments\n\ntree: the tree to convert to a string\noperators: the operators used to define the tree\n\nKeyword Arguments\n\nf_variable: (optional) function to convert a variable to a string, with arguments (feature::UInt8, variable_names).\nf_constant: (optional) function to convert a constant to a string, with arguments (val,)\nvariable_names::Union{Array{String, 1}, Nothing}=nothing: (optional) what variables to print for each feature.\n\n\n\n\n\nstring_tree(\n    ex::AbstractExpression,\n    operators::Union{AbstractOperatorEnum,Nothing}=nothing;\n    variable_names=nothing,\n    kws...\n)\n\nConvert an expression to a string representation.\n\nThis method unpacks the operators and variable names from the expression and calls string_tree for AbstractExpressionNode.\n\nArguments\n\nex: The expression to convert to a string.\noperators: (Optional) Operators to use. If nothing, operators are obtained from the expression.\nvariable_names: (Optional) Variable names to use in the string representation. If nothing, variable names are obtained from the expression.\nkws...: Additional keyword arguments.\n\nReturns\n\nA string representation of the expression.\n\n\n\n\n\nstring_tree(tree::AbstractExpressionNode, options::AbstractOptions; kws...)\n\nConvert an equation to a string.\n\nArguments\n\ntree::AbstractExpressionNode: The equation to convert to a string.\noptions::AbstractOptions: The options holding the definition of operators.\nvariable_names::Union{Array{String, 1}, Nothing}=nothing: what variables   to print for each feature.\n\n\n\n\n\n","category":"function"},{"location":"api/#Evaluation","page":"API","title":"Evaluation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"eval_tree_array\nEvalOptions","category":"page"},{"location":"api/#DynamicExpressions.EvaluateModule.eval_tree_array","page":"API","title":"DynamicExpressions.EvaluateModule.eval_tree_array","text":"eval_tree_array(\n    tree::AbstractExpressionNode{T},\n    cX::AbstractMatrix{T},\n    operators::OperatorEnum;\n    eval_options::Union{EvalOptions,Nothing}=nothing,\n) where {T}\n\nEvaluate a binary tree (equation) over a given input data matrix. The operators contain all of the operators used. This function fuses doublets and triplets of operations for lower memory usage.\n\nArguments\n\ntree::AbstractExpressionNode: The root node of the tree to evaluate.\ncX::AbstractMatrix{T}: The input data to evaluate the tree on, with shape [num_features, num_rows].\noperators::OperatorEnum: The operators used in the tree.\neval_options::Union{EvalOptions,Nothing}: See EvalOptions for documentation   on the different evaluation modes.\n\nReturns\n\n(output, complete)::Tuple{AbstractVector{T}, Bool}: the result,   which is a 1D array, as well as if the evaluation completed   successfully (true/false). A false complete means an infinity   or nan was encountered, and a large loss should be assigned   to the equation.\n\nNotes\n\nThis function can be represented by the following pseudocode:\n\ndef eval(current_node)\n    if current_node is leaf\n        return current_node.value\n    elif current_node is degree 1\n        return current_node.operator(eval(current_node.left_child))\n    else\n        return current_node.operator(eval(current_node.left_child), eval(current_node.right_child))\n\nThe bulk of the code is for optimizations and pre-emptive NaN/Inf checks, which speed up evaluation significantly.\n\n\n\n\n\neval_tree_array(tree::AbstractExpressionNode, cX::AbstractMatrix, operators::GenericOperatorEnum; throw_errors::Bool=true)\n\nEvaluate a generic binary tree (equation) over a given input data, whatever that input data may be. The operators enum contains all of the operators used. Unlike eval_tree_array with the normal OperatorEnum, the array cX is sliced only along the first dimension. i.e., if cX is a vector, then the output of a feature node will be a scalar. If cX is a 3D tensor, then the output of a feature node will be a 2D tensor. Note also that tree.feature will index along the first axis of cX.\n\nHowever, there is no requirement about input and output types in general. You may set up your tree such that some operator nodes work on tensors, while other operator nodes work on scalars. eval_tree_array will simply return nothing if a given operator is not defined for the given input type.\n\nThis function can be represented by the following pseudocode:\n\nfunction eval(current_node)\n    if current_node is leaf\n        return current_node.value\n    elif current_node is degree 1\n        return current_node.operator(eval(current_node.left_child))\n    else\n        return current_node.operator(eval(current_node.left_child), eval(current_node.right_child))\n\nArguments\n\ntree::AbstractExpressionNode: The root node of the tree to evaluate.\ncX::AbstractArray: The input data to evaluate the tree on.\noperators::GenericOperatorEnum: The operators used in the tree.\nthrow_errors::Bool=true: Whether to throw errors   if they occur during evaluation. Otherwise,   MethodErrors will be caught before they happen and    evaluation will return nothing,   rather than throwing an error. This is useful in cases   where you are unsure if a particular tree is valid or not,   and would prefer to work with nothing as an output.\n\nReturns\n\n(output, complete)::Tuple{Any, Bool}: the result,   as well as if the evaluation completed successfully (true/false).   If evaluation failed, nothing will be returned for the first argument.   A false complete means an operator was called on input types   that it was not defined for.\n\n\n\n\n\neval_tree_array(\n    ex::AbstractExpression,\n    cX::AbstractMatrix,\n    operators::Union{AbstractOperatorEnum,Nothing}=nothing;\n    kws...\n)\n\nEvaluate an expression over a given input data matrix.\n\nThis method unpacks the operators from the expression and calls eval_tree_array for AbstractExpressionNode.\n\nArguments\n\nex: The expression to evaluate.\ncX: The input data matrix.\noperators: (Optional) Operators to use. If nothing, operators are obtained from the expression.\nkws...: Additional keyword arguments.\n\nReturns\n\nA tuple (output, complete) indicating the result and success of the evaluation.\n\n\n\n\n\neval_tree_array(tree::Union{AbstractExpression,AbstractExpressionNode}, X::AbstractArray, options::AbstractOptions; kws...)\n\nEvaluate a binary tree (equation) over a given input data matrix. The operators contain all of the operators used. This function fuses doublets and triplets of operations for lower memory usage.\n\nThis function can be represented by the following pseudocode:\n\nfunction eval(current_node)\n    if current_node is leaf\n        return current_node.value\n    elif current_node is degree 1\n        return current_node.operator(eval(current_node.left_child))\n    else\n        return current_node.operator(eval(current_node.left_child), eval(current_node.right_child))\n\nThe bulk of the code is for optimizations and pre-emptive NaN/Inf checks, which speed up evaluation significantly.\n\nArguments\n\ntree::Union{AbstractExpression,AbstractExpressionNode}: The root node of the tree to evaluate.\nX::AbstractArray: The input data to evaluate the tree on.\noptions::AbstractOptions: Options used to define the operators used in the tree.\n\nReturns\n\n(output, complete)::Tuple{AbstractVector, Bool}: the result,   which is a 1D array, as well as if the evaluation completed   successfully (true/false). A false complete means an infinity   or nan was encountered, and a large loss should be assigned   to the equation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicExpressions.EvaluateModule.EvalOptions","page":"API","title":"DynamicExpressions.EvaluateModule.EvalOptions","text":"EvalOptions{T,B,E}\n\nThis holds options for expression evaluation, such as evaluation backend.\n\nFields\n\nturbo::Val{T}=Val(false): If Val{true}, use LoopVectorization.jl for faster   evaluation.\nbumper::Val{B}=Val(false): If Val{true}, use Bumper.jl for faster evaluation.\nearly_exit::Val{E}=Val(true): If Val{true}, any element of any step becoming   NaN or Inf will terminate the computation. For eval_tree_array, this will   result in the second return value, the completion flag, being false. For    calling an expression using tree(X), this will result in NaNs filling   the entire buffer. This early exit is performed to avoid wasting compute cycles.   Setting Val{false} will continue the computation as usual and thus result in   NaNs only in the elements that actually have NaNs.\n\n\n\n\n\n","category":"type"},{"location":"api/#Derivatives","page":"API","title":"Derivatives","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"SymbolicRegression.jl can automatically and efficiently compute derivatives of expressions with respect to variables or constants. This is done using either eval_diff_tree_array, to compute derivative with respect to a single variable, or with eval_grad_tree_array, to compute the gradient with respect all variables (or, all constants). Both use forward-mode automatic, but use Zygote.jl to compute derivatives of each operator, so this is very efficient.","category":"page"},{"location":"api/","page":"API","title":"API","text":"eval_diff_tree_array\neval_grad_tree_array","category":"page"},{"location":"api/#DynamicExpressions.EvaluateDerivativeModule.eval_diff_tree_array","page":"API","title":"DynamicExpressions.EvaluateDerivativeModule.eval_diff_tree_array","text":"eval_diff_tree_array(\n    tree::AbstractExpressionNode{T},\n    cX::AbstractMatrix{T},\n    operators::OperatorEnum,\n    direction::Integer;\n    turbo::Union{Bool,Val}=Val(false)\n) where {T<:Number}\n\nCompute the forward derivative of an expression, using a similar structure and optimization to evaltreearray. direction is the index of a particular variable in the expression. e.g., direction=1 would indicate derivative with respect to x1.\n\nArguments\n\ntree::AbstractExpressionNode: The expression tree to evaluate.\ncX::AbstractMatrix{T}: The data matrix, with shape [num_features, num_rows].\noperators::OperatorEnum: The operators used to create the tree.\ndirection::Integer: The index of the variable to take the derivative with respect to.\nturbo::Union{Bool,Val}: Use LoopVectorization.jl for faster evaluation. Currently this does not have   any effect.\n\nReturns\n\n(evaluation, derivative, complete)::Tuple{AbstractVector{T}, AbstractVector{T}, Bool}: the normal evaluation,   the derivative, and whether the evaluation completed as normal (or encountered a nan or inf).\n\n\n\n\n\neval_diff_tree_array(tree::Union{AbstractExpression,AbstractExpressionNode}, X::AbstractArray, options::AbstractOptions, direction::Int)\n\nCompute the forward derivative of an expression, using a similar structure and optimization to evaltreearray. direction is the index of a particular variable in the expression. e.g., direction=1 would indicate derivative with respect to x1.\n\nArguments\n\ntree::Union{AbstractExpression,AbstractExpressionNode}: The expression tree to evaluate.\nX::AbstractArray: The data matrix, with each column being a data point.\noptions::AbstractOptions: The options containing the operators used to create the tree.\ndirection::Int: The index of the variable to take the derivative with respect to.\n\nReturns\n\n(evaluation, derivative, complete)::Tuple{AbstractVector, AbstractVector, Bool}: the normal evaluation,   the derivative, and whether the evaluation completed as normal (or encountered a nan or inf).\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicExpressions.EvaluateDerivativeModule.eval_grad_tree_array","page":"API","title":"DynamicExpressions.EvaluateDerivativeModule.eval_grad_tree_array","text":"eval_grad_tree_array(tree::AbstractExpressionNode{T}, cX::AbstractMatrix{T}, operators::OperatorEnum; variable::Union{Bool,Val}=Val(false), turbo::Union{Bool,Val}=Val(false))\n\nCompute the forward-mode derivative of an expression, using a similar structure and optimization to evaltreearray. variable specifies whether we should take derivatives with respect to features (i.e., cX), or with respect to every constant in the expression.\n\nArguments\n\ntree::AbstractExpressionNode{T}: The expression tree to evaluate.\ncX::AbstractMatrix{T}: The data matrix, with each column being a data point.\noperators::OperatorEnum: The operators used to create the tree.\nvariable::Union{Bool,Val}: Whether to take derivatives with respect to features (i.e., cX - with variable=true),   or with respect to every constant in the expression (variable=false).\nturbo::Union{Bool,Val}: Use LoopVectorization.jl for faster evaluation. Currently this does not have   any effect.\n\nReturns\n\n(evaluation, gradient, complete)::Tuple{AbstractVector{T}, AbstractMatrix{T}, Bool}: the normal evaluation,   the gradient, and whether the evaluation completed as normal (or encountered a nan or inf).\n\n\n\n\n\neval_grad_tree_array(\n    ex::AbstractExpression,\n    cX::AbstractMatrix,\n    operators::Union{AbstractOperatorEnum,Nothing}=nothing;\n    kws...\n)\n\nCompute the forward-mode derivative of an expression.\n\nThis method unpacks the operators from the expression and calls eval_grad_tree_array for AbstractExpressionNode.\n\nArguments\n\nex: The expression to evaluate.\ncX: The data matrix.\noperators: (Optional) Operators to use. If nothing, operators are obtained from the expression.\nkws...: Additional keyword arguments.\n\nReturns\n\nA tuple (output, gradient, complete) indicating the result, gradient, and success of the evaluation.\n\n\n\n\n\neval_grad_tree_array(tree::Union{AbstractExpression,AbstractExpressionNode}, X::AbstractArray, options::AbstractOptions; variable::Bool=false)\n\nCompute the forward-mode derivative of an expression, using a similar structure and optimization to evaltreearray. variable specifies whether we should take derivatives with respect to features (i.e., X), or with respect to every constant in the expression.\n\nArguments\n\ntree::Union{AbstractExpression,AbstractExpressionNode}: The expression tree to evaluate.\nX::AbstractArray: The data matrix, with each column being a data point.\noptions::AbstractOptions: The options containing the operators used to create the tree.\nvariable::Bool: Whether to take derivatives with respect to features (i.e., X - with variable=true),   or with respect to every constant in the expression (variable=false).\n\nReturns\n\n(evaluation, gradient, complete)::Tuple{AbstractVector, AbstractArray, Bool}: the normal evaluation,   the gradient, and whether the evaluation completed as normal (or encountered a nan or inf).\n\n\n\n\n\n","category":"function"},{"location":"api/#SymbolicUtils.jl-interface","page":"API","title":"SymbolicUtils.jl interface","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"node_to_symbolic","category":"page"},{"location":"api/#DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic","page":"API","title":"DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic","text":"node_to_symbolic(tree::AbstractExpressionNode, operators::AbstractOperatorEnum;\n            variable_names::Union{Array{String, 1}, Nothing}=nothing,\n            index_functions::Bool=false)\n\nThe interface to SymbolicUtils.jl. Passing a tree to this function will generate a symbolic equation in SymbolicUtils.jl format.\n\nArguments\n\ntree::AbstractExpressionNode: The equation to convert.\noperators::AbstractOperatorEnum: OperatorEnum, which contains the operators used in the equation.\nvariable_names::Union{Array{String, 1}, Nothing}=nothing: What variable names to use for   each feature. Default is [x1, x2, x3, ...].\nindex_functions::Bool=false: Whether to generate special names for the   operators, which then allows one to convert back to a AbstractExpressionNode format   using symbolic_to_node.   (CURRENTLY UNAVAILABLE - See https://github.com/MilesCranmer/SymbolicRegression.jl/pull/84).\n\n\n\n\n\nnode_to_symbolic(tree::AbstractExpressionNode, options::Options; kws...)\n\nConvert an expression to SymbolicUtils.jl form.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"Note that use of this function requires SymbolicUtils.jl to be installed and loaded.","category":"page"},{"location":"api/#Pareto-frontier","page":"API","title":"Pareto frontier","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"calculate_pareto_frontier","category":"page"},{"location":"api/#SymbolicRegression.HallOfFameModule.calculate_pareto_frontier","page":"API","title":"SymbolicRegression.HallOfFameModule.calculate_pareto_frontier","text":"calculate_pareto_frontier(hallOfFame::HallOfFame{T,L,P}) where {T<:DATA_TYPE,L<:LOSS_TYPE}\n\n\n\n\n\n","category":"function"},{"location":"api/#Logging","page":"API","title":"Logging","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"SRLogger","category":"page"},{"location":"api/#SymbolicRegression.LoggingModule.SRLogger","page":"API","title":"SymbolicRegression.LoggingModule.SRLogger","text":"SRLogger(logger::AbstractLogger; log_interval::Int=100)\n\nA logger for symbolic regression that wraps another logger.\n\nArguments\n\nlogger: The base logger to wrap\nlog_interval: Number of steps between logging events. Default is 100 (log every 100 steps).\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"The SRLogger allows you to track the progress of symbolic regression searches. It can wrap any AbstractLogger that implements the Julia logging interface, such as from TensorBoardLogger.jl or Wandb.jl.","category":"page"},{"location":"api/","page":"API","title":"API","text":"using TensorBoardLogger\n\nlogger = SRLogger(TBLogger(\"logs/run\"), log_interval=2)\n\nmodel = SRRegressor(;\n    logger=logger,\n    kws...\n)","category":"page"},{"location":"index_base/#Contents","page":"Contents","title":"Contents","text":"","category":"section"},{"location":"index_base/","page":"Contents","title":"Contents","text":"Pages = [\"examples.md\", \"examples/template_expression.md\", \"api.md\", \"types.md\", \"losses.md\"]","category":"page"},{"location":"examples/#Toy-Examples-with-Code","page":"Short Examples","title":"Toy Examples with Code","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"using SymbolicRegression\nusing MLJ","category":"page"},{"location":"examples/#.-Simple-search","page":"Short Examples","title":"1. Simple search","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Here's a simple example where we find the expression 2 cos(x4) + x1^2 - 2.","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"X = 2randn(1000, 5)\ny = @. 2*cos(X[:, 4]) + X[:, 1]^2 - 2\n\nmodel = SRRegressor(\n    binary_operators=[+, -, *, /],\n    unary_operators=[cos],\n    niterations=30\n)\nmach = machine(model, X, y)\nfit!(mach)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Let's look at the returned table:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"r = report(mach)\nr","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"We can get the selected best tradeoff expression with:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"r.equations[r.best_idx]","category":"page"},{"location":"examples/#.-Custom-operator","page":"Short Examples","title":"2. Custom operator","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Here, we define a custom operator and use it to find an expression:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"X = 2randn(1000, 5)\ny = @. 1/X[:, 1]\n\nmy_inv(x) = 1/x\n\nmodel = SRRegressor(\n    binary_operators=[+, *],\n    unary_operators=[my_inv],\n)\nmach = machine(model, X, y)\nfit!(mach)\nr = report(mach)\nprintln(r.equations[r.best_idx])","category":"page"},{"location":"examples/#.-Multiple-outputs","page":"Short Examples","title":"3. Multiple outputs","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Here, we do the same thing, but with multiple expressions at once, each requiring a different feature. This means that we need to use MultitargetSRRegressor instead of SRRegressor:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"X = 2rand(1000, 5) .+ 0.1\ny = @. 1/X[:, 1:3]\n\nmy_inv(x) = 1/x\n\nmodel = MultitargetSRRegressor(; binary_operators=[+, *], unary_operators=[my_inv])\nmach = machine(model, X, y)\nfit!(mach)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"The report gives us lists of expressions instead:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"r = report(mach)\nfor i in 1:3\n    println(\"y[$(i)] = \", r.equations[i][r.best_idx[i]])\nend","category":"page"},{"location":"examples/#.-Plotting-an-expression","page":"Short Examples","title":"4. Plotting an expression","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"For now, let's consider the expressions for output 1 from the previous example: We can get a SymbolicUtils version with:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"using SymbolicUtils\n\neqn = node_to_symbolic(r.equations[1][r.best_idx[1]])","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"We can get the LaTeX version with Latexify:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"using Latexify\n\nlatexify(string(eqn))","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"We can also plot the prediction against the truth:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"using Plots\n\nypred = predict(mach, X)\nscatter(y[1, :], ypred[1, :], xlabel=\"Truth\", ylabel=\"Prediction\")","category":"page"},{"location":"examples/#.-Other-types","page":"Short Examples","title":"5. Other types","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"SymbolicRegression.jl can handle most numeric types you wish to use. For example, passing a Float32 array will result in the search using 32-bit precision everywhere in the codebase:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"X = 2randn(Float32, 1000, 5)\ny = @. 2*cos(X[:, 4]) + X[:, 1]^2 - 2\n\nmodel = SRRegressor(binary_operators=[+, -, *, /], unary_operators=[cos], niterations=30)\nmach = machine(model, X, y)\nfit!(mach)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"we can see that the output types are Float32:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"r = report(mach)\nbest = r.equations[r.best_idx]\nprintln(typeof(best))\n# Expression{Float32,Node{Float32},...}","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"We can also use Complex numbers (ignore the warning from MLJ):","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"cos_re(x::Complex{T}) where {T} = cos(abs(x)) + 0im\n\nX = 15 .* rand(ComplexF64, 1000, 5) .- 7.5\ny = @. 2*cos_re((2+1im) * X[:, 4]) + 0.1 * X[:, 1]^2 - 2\n\nmodel = SRRegressor(\n    binary_operators=[+, -, *, /],\n    unary_operators=[cos_re],\n    maxsize=30,\n    niterations=100\n)\nmach = machine(model, X, y)\nfit!(mach)","category":"page"},{"location":"examples/#.-Dimensional-constraints","page":"Short Examples","title":"6. Dimensional constraints","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"One other feature we can exploit is dimensional analysis. Say that we know the physical units of each feature and output, and we want to find an expression that is dimensionally consistent.","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"We can do this as follows, using DynamicQuantities to assign units. First, let's make some data on Newton's law of gravitation:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"using DynamicQuantities\nusing SymbolicRegression\n\nM = (rand(100) .+ 0.1) .* Constants.M_sun\nm = 100 .* (rand(100) .+ 0.1) .* u\"kg\"\nr = (rand(100) .+ 0.1) .* Constants.R_earth\n\nG = Constants.G\n\nF = @. (G * M * m / r^2)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"(Note that the u macro from DynamicQuantities will automatically convert to SI units. To avoid this, use the us macro.)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Now, let's ready the data for MLJ:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"X = (; M=M, m=m, r=r)\ny = F","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Since this data has such a large dynamic range, let's also create a custom loss function that looks at the error in log-space:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"function loss_fnc(prediction, target)\n    # Useful loss for large dynamic range\n    scatter_loss = abs(log((abs(prediction)+1e-20) / (abs(target)+1e-20)))\n    sign_loss = 10 * (sign(prediction) - sign(target))^2\n    return scatter_loss + sign_loss\nend","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Now let's define and fit our model:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"model = SRRegressor(\n    binary_operators=[+, -, *, /],\n    unary_operators=[square],\n    elementwise_loss=loss_fnc,\n    complexity_of_constants=2,\n    maxsize=25,\n    niterations=100,\n    populations=50,\n    dimensional_constraint_penalty=10^5,\n)\nmach = machine(model, X, y)\nfit!(mach)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"You can observe that all expressions with a loss under our penalty are dimensionally consistent! (The \"[?]\" indicates free units in a constant, which can cancel out other units in the expression.) For example,","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"\"y[m s⁻² kg] = (M[kg] * 2.6353e-22[?])\"","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"would indicate that the expression is dimensionally consistent, with a constant \"2.6353e-22[m s⁻²]\".","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Note that you can also search for dimensionless units by settings dimensionless_constants_only to true.","category":"page"},{"location":"examples/#.-Working-with-Expressions","page":"Short Examples","title":"7. Working with Expressions","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Expressions in SymbolicRegression.jl are represented using the Expression{T,Node{T},...} type, which provides a more robust way to combine structure, operators, and constraints. Here's an example:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"using SymbolicRegression\n\n# Define options with operators and structure\noptions = Options(\n    binary_operators=[+, -, *],\n    unary_operators=[cos],\n    expression_options=(\n        structure=TemplateStructure(),\n        variable_constraints=Dict(1 => [1, 2], 2 => [2])\n    )\n)\n\n# Create expression nodes with constraints\noperators = options.operators\nvariable_names = [\"x1\", \"x2\"]\nx1 = Expression(\n    Node{Float64}(feature=1),\n    operators=operators,\n    variable_names=variable_names,\n    structure=options.expression_options.structure\n)\nx2 = Expression(\n    Node{Float64}(feature=2),\n    operators=operators,\n    variable_names=variable_names,\n    structure=options.expression_options.structure\n)\n\n# Construct and evaluate expression\nexpr = x1 * cos(x2 - 3.2)\nX = rand(Float64, 2, 100)\noutput = expr(X)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"This Expression type, contains both the structure and the operators used in the expression. These are what are returned by the search. The raw Node type (which is what used to be output directly) is accessible with","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"get_contents(expr)","category":"page"},{"location":"examples/#.-Template-Expressions","page":"Short Examples","title":"8. Template Expressions","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Template expressions allow you to define structured expressions where different parts can be constrained to use specific variables. In this example, we'll create expressions that constrain the functional form in highly specific ways. (For a more complex example, see [\"Searching with template expressions\"](examples/templateexpression.md)_)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"First, let's set up our basic configuration:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"using SymbolicRegression\nusing Random: rand, MersenneTwister\nusing MLJBase: machine, fit!, report","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"The key part is defining our template structure. This determines how different parts of the expression combine:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"structure = TemplateStructure{(:f, :g)}(\n    ((; f, g), (x1, x2, x3)) -> f(x1, x2) + g(x2) - g(x3)\n)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"With this structure, we are telling the algorithm that it can learn any symbolic expressions f and g, with f a function of two inputs, and g a function of one input. The result of","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"f(x_1 x_2) + g(x_2) - g(x_3)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"will be compared with the target y.","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Let's generate some example data:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"n = 100\nrng = MersenneTwister(0)\nx1 = 10rand(rng, n)\nx2 = 10rand(rng, n)\nx3 = 10rand(rng, n)\nX = (; x1, x2, x3)\ny = [\n    2 * cos(x1[i] + 3.2) + x2[i]^2 - 0.8 * x3[i]^2\n    for i in eachindex(x1)\n]","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Now, remember our structure: for the model to learn this, it would need to correctly disentangle the contribution of f and g!","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Now we can set up and train our model. Note that we pass the structure in to expression_options:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"model = SRRegressor(;\n    binary_operators=(+, -, *, /),\n    unary_operators=(cos,),\n    niterations=500,\n    maxsize=25,\n    expression_type=TemplateExpression,\n    expression_options=(; structure),\n)\n\nmach = machine(model, X, y)\nfit!(mach)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"If all goes well, you should see a printout with the following expression:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"y = ╭ f = ((#2 * 0.2) * #2) + (cos(#1 + 0.058407) * -2)\n    ╰ g = #1 * (#1 * 0.8)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"This is what we were looking for! We can see that under f(x_1 x_2) + g(x_2) - g(x_3), this correctly expands to 2 cos(x_1 + 32) + x_2^2 - 08 x_3^2.","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"We can also access the individual parts of the template expression directly from the report:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"r = report(mach)\nbest_expr = r.equations[r.best_idx]\n\n# Access individual parts of the template expression\nprintln(\"f: \", get_contents(best_expr).f)\nprintln(\"g: \", get_contents(best_expr).g)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"The TemplateExpression combines these under the structure so we can directly and efficiently evaluate this:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"best_expr(randn(3, 20))","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"The above code demonstrates how template expressions can be used to:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Define structured expressions with multiple components\nConstrains which variables can be used in each component\nCreate expressions that can output multiple values","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"You can even output custom structs - see the more detailed Template Expression example!","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"Be sure to also check out the Parametric Expression example.","category":"page"},{"location":"examples/#.-Logging-with-TensorBoard","page":"Short Examples","title":"9. Logging with TensorBoard","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"You can track the progress of symbolic regression searches using TensorBoard or other logging backends. Here's an example using TensorBoardLogger and wrapping it with SRLogger:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"using SymbolicRegression\nusing TensorBoardLogger\nusing MLJ\n\nlogger = SRLogger(TBLogger(\"logs/sr_run\"))\n\n# Create and fit model with logger\nmodel = SRRegressor(\n    binary_operators=[+, -, *],\n    maxsize=40,\n    niterations=100,\n    logger=logger\n)\n\nX = (a=rand(500), b=rand(500))\ny = @. 2 * cos(X.a * 23.5) - X.b^2\n\nmach = machine(model, X, y)\nfit!(mach)","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"You can then view the logs with:","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"tensorboard --logdir logs","category":"page"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"The TensorBoard interface will show the loss curves over time (at each complexity), as well as the Pareto frontier volume which can be used as an overall metric of the search performance.","category":"page"},{"location":"examples/#.-Additional-features","page":"Short Examples","title":"10. Additional features","text":"","category":"section"},{"location":"examples/","page":"Short Examples","title":"Short Examples","text":"For the many other features available in SymbolicRegression.jl, check out the API page for Options. You might also find it useful to browse the documentation for the Python frontend PySR, which has additional documentation. In particular, the tuning page is useful for improving search performance.","category":"page"},{"location":"losses/#Losses","page":"Losses","title":"Losses","text":"","category":"section"},{"location":"losses/","page":"Losses","title":"Losses","text":"These losses, and their documentation, are included from the LossFunctions.jl package.","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"Pass the function as, e.g., elementwise_loss=L1DistLoss().","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"You can also declare your own loss as a function that takes two (unweighted) or three (weighted) scalar arguments. For example,","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"f(x, y, w) = abs(x-y)*w\noptions = Options(elementwise_loss=f)","category":"page"},{"location":"losses/#Regression","page":"Losses","title":"Regression","text":"","category":"section"},{"location":"losses/","page":"Losses","title":"Losses","text":"Regression losses work on the distance between targets and predictions: r = x - y.","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"LPDistLoss{P}\nL1DistLoss\nL2DistLoss\nPeriodicLoss\nHuberLoss\nL1EpsilonInsLoss\nL2EpsilonInsLoss\nLogitDistLoss\nQuantileLoss","category":"page"},{"location":"losses/#LossFunctions.LPDistLoss","page":"Losses","title":"LossFunctions.LPDistLoss","text":"LPDistLoss{P} <: DistanceLoss\n\nThe P-th power absolute distance loss. It is Lipschitz continuous iff P == 1, convex if and only if P >= 1, and strictly convex iff P > 1.\n\nL(r) = r^P\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L1DistLoss","page":"Losses","title":"LossFunctions.L1DistLoss","text":"L1DistLoss <: DistanceLoss\n\nThe absolute distance loss. Special case of the LPDistLoss with P=1. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(r) = r\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    3 │\\.                     ./│    1 │            ┌------------│\n      │ '\\.                 ./' │      │            |            │\n      │   \\.               ./   │      │            |            │\n      │    '\\.           ./'    │      │_           |           _│\n    L │      \\.         ./      │   L' │            |            │\n      │       '\\.     ./'       │      │            |            │\n      │         \\.   ./         │      │            |            │\n    0 │          '\\./'          │   -1 │------------┘            │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -3                        3\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L2DistLoss","page":"Losses","title":"LossFunctions.L2DistLoss","text":"L2DistLoss <: DistanceLoss\n\nThe least squares loss. Special case of the LPDistLoss with P=2. It is strictly convex.\n\nL(r) = r^2\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    9 │\\                       /│    3 │                   .r/   │\n      │\".                     .\"│      │                 .r'     │\n      │ \".                   .\" │      │              _./'       │\n      │  \".                 .\"  │      │_           .r/         _│\n    L │   \".               .\"   │   L' │         _:/'            │\n      │    '\\.           ./'    │      │       .r'               │\n      │      \\.         ./      │      │     .r'                 │\n    0 │        \"-.___.-\"        │   -3 │  _/r'                   │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -2                        2\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.PeriodicLoss","page":"Losses","title":"LossFunctions.PeriodicLoss","text":"PeriodicLoss <: DistanceLoss\n\nMeasures distance on a circle of specified circumference c.\n\nL(r) = 1 - cos left( frac2 r pic right)\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.HuberLoss","page":"Losses","title":"LossFunctions.HuberLoss","text":"HuberLoss <: DistanceLoss\n\nLoss function commonly used for robustness to outliers. For large values of d it becomes close to the L1DistLoss, while for small values of d it resembles the L2DistLoss. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(r) = begincases fracr^22  quad textif   r  le alpha  alpha  r  - fracalpha^32  quad textotherwise endcases\n\n\n\n              Lossfunction (d=1)               Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │                         │    1 │                .+-------│\n      │                         │      │              ./'        │\n      │\\.                     ./│      │             ./          │\n      │ '.                   .' │      │_           ./          _│\n    L │   \\.               ./   │   L' │           /'            │\n      │     \\.           ./     │      │          /'             │\n      │      '.         .'      │      │        ./'              │\n    0 │        '-.___.-'        │   -1 │-------+'                │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L1EpsilonInsLoss","page":"Losses","title":"LossFunctions.L1EpsilonInsLoss","text":"L1EpsilonInsLoss <: DistanceLoss\n\nThe ϵ-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than ϵ, but penalizes larger deviances linarily. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(r) = max  0  r  - epsilon \n\n\n\n              Lossfunction (ϵ=1)               Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │\\                       /│    1 │                  ┌------│\n      │ \\                     / │      │                  |      │\n      │  \\                   /  │      │                  |      │\n      │   \\                 /   │      │_      ___________!     _│\n    L │    \\               /    │   L' │      |                  │\n      │     \\             /     │      │      |                  │\n      │      \\           /      │      │      |                  │\n    0 │       \\_________/       │   -1 │------┘                  │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -2                        2\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L2EpsilonInsLoss","page":"Losses","title":"LossFunctions.L2EpsilonInsLoss","text":"L2EpsilonInsLoss <: DistanceLoss\n\nThe quadratic ϵ-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than ϵ, but penalizes larger deviances quadratically. It is convex, but not strictly convex.\n\nL(r) = max  0  r  - epsilon ^2\n\n\n\n              Lossfunction (ϵ=0.5)             Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    8 │                         │    1 │                  /      │\n      │:                       :│      │                 /       │\n      │'.                     .'│      │                /        │\n      │ \\.                   ./ │      │_         _____/        _│\n    L │  \\.                 ./  │   L' │         /               │\n      │   \\.               ./   │      │        /                │\n      │    '\\.           ./'    │      │       /                 │\n    0 │      '-._______.-'      │   -1 │      /                  │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -2                        2\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.LogitDistLoss","page":"Losses","title":"LossFunctions.LogitDistLoss","text":"LogitDistLoss <: DistanceLoss\n\nThe distance-based logistic loss for regression. It is strictly convex and Lipschitz continuous.\n\nL(r) = - ln frac4 e^r(1 + e^r)^2\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │                         │    1 │                   _--'''│\n      │\\                       /│      │                ./'      │\n      │ \\.                   ./ │      │              ./         │\n      │  '.                 .'  │      │_           ./          _│\n    L │   '.               .'   │   L' │           ./            │\n      │     \\.           ./     │      │         ./              │\n      │      '.         .'      │      │       ./                │\n    0 │        '-.___.-'        │   -1 │___.-''                  │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -4                        4\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.QuantileLoss","page":"Losses","title":"LossFunctions.QuantileLoss","text":"QuantileLoss <: DistanceLoss\n\nThe distance-based quantile loss, also known as pinball loss, can be used to estimate conditional τ-quantiles. It is Lipschitz continuous and convex, but not strictly convex. Furthermore it is symmetric if and only if τ = 1/2.\n\nL(r) = begincases -left( 1 - tau  right) r  quad textif  r  0  tau r  quad textif  r ge 0  endcases\n\n\n\n              Lossfunction (τ=0.7)             Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │'\\                       │  0.3 │            ┌------------│\n      │  \\.                     │      │            |            │\n      │   '\\                    │      │_           |           _│\n      │     \\.                  │      │            |            │\n    L │      '\\              ._-│   L' │            |            │\n      │        \\.         ..-'  │      │            |            │\n      │         '.     _r/'     │      │            |            │\n    0 │           '_./'         │ -0.7 │------------┘            │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -3                        3\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#Classification","page":"Losses","title":"Classification","text":"","category":"section"},{"location":"losses/","page":"Losses","title":"Losses","text":"Classifications losses (assuming binary) work on the margin between targets and predictions: r = x y, assuming the target y is either -1 or +1.","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"ZeroOneLoss\nPerceptronLoss\nLogitMarginLoss\nL1HingeLoss\nL2HingeLoss\nSmoothedL1HingeLoss\nModifiedHuberLoss\nL2MarginLoss\nExpLoss\nSigmoidLoss\nDWDMarginLoss","category":"page"},{"location":"losses/#LossFunctions.ZeroOneLoss","page":"Losses","title":"LossFunctions.ZeroOneLoss","text":"ZeroOneLoss <: MarginLoss\n\nThe classical classification loss. It penalizes every misclassified observation with a loss of 1 while every correctly classified observation has a loss of 0. It is not convex nor continuous and thus seldom used directly. Instead one usually works with some classification-calibrated surrogate loss, such as L1HingeLoss.\n\nL(a) = begincases 1  quad textif  a  0  0  quad textif  a = 0 endcases\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    1 │------------┐            │    1 │                         │\n      │            |            │      │                         │\n      │            |            │      │                         │\n      │            |            │      │_________________________│\n      │            |            │      │                         │\n      │            |            │      │                         │\n      │            |            │      │                         │\n    0 │            └------------│   -1 │                         │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                y * h(x)                         y * h(x)\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.PerceptronLoss","page":"Losses","title":"LossFunctions.PerceptronLoss","text":"PerceptronLoss <: MarginLoss\n\nThe perceptron loss linearly penalizes every prediction where the resulting agreement <= 0. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = max  0 -a \n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │\\.                       │    0 │            ┌------------│\n      │ '..                     │      │            |            │\n      │   \\.                    │      │            |            │\n      │     '.                  │      │            |            │\n    L │      '.                 │   L' │            |            │\n      │        \\.               │      │            |            │\n      │         '.              │      │            |            │\n    0 │           \\.____________│   -1 │------------┘            │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.LogitMarginLoss","page":"Losses","title":"LossFunctions.LogitMarginLoss","text":"LogitMarginLoss <: MarginLoss\n\nThe margin version of the logistic loss. It is infinitely many times differentiable, strictly convex, and Lipschitz continuous.\n\nL(a) = ln (1 + e^-a)\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │ \\.                      │    0 │                  ._--/\"\"│\n      │   \\.                    │      │               ../'      │\n      │     \\.                  │      │              ./         │\n      │       \\..               │      │            ./'          │\n    L │         '-_             │   L' │          .,'            │\n      │            '-_          │      │         ./              │\n      │               '\\-._     │      │      .,/'               │\n    0 │                    '\"\"*-│   -1 │__.--''                  │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -4                        4\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L1HingeLoss","page":"Losses","title":"LossFunctions.L1HingeLoss","text":"L1HingeLoss <: MarginLoss\n\nThe hinge loss linearly penalizes every predicition where the resulting agreement < 1 . It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = max  0 1 - a \n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    3 │'\\.                      │    0 │                  ┌------│\n      │  ''_                    │      │                  |      │\n      │     \\.                  │      │                  |      │\n      │       '.                │      │                  |      │\n    L │         ''_             │   L' │                  |      │\n      │            \\.           │      │                  |      │\n      │              '.         │      │                  |      │\n    0 │                ''_______│   -1 │------------------┘      │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L2HingeLoss","page":"Losses","title":"LossFunctions.L2HingeLoss","text":"L2HingeLoss <: MarginLoss\n\nThe truncated least squares loss quadratically penalizes every predicition where the resulting agreement < 1. It is locally Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = max  0 1 - a ^2\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    5 │     .                   │    0 │                 ,r------│\n      │     '.                  │      │               ,/        │\n      │      '\\                 │      │             ,/          │\n      │        \\                │      │           ,/            │\n    L │         '.              │   L' │         ./              │\n      │          '.             │      │       ./                │\n      │            \\.           │      │     ./                  │\n    0 │              '-.________│   -5 │   ./                    │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.SmoothedL1HingeLoss","page":"Losses","title":"LossFunctions.SmoothedL1HingeLoss","text":"SmoothedL1HingeLoss <: MarginLoss\n\nAs the name suggests a smoothed version of the L1 hinge loss. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = begincases frac05gamma cdot max  0 1 - a  ^2  quad textif  a ge 1 - gamma  1 - fracgamma2 - a  quad textotherwise endcases\n\n\n\n              Lossfunction (γ=2)               Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │\\.                       │    0 │                 ,r------│\n      │ '.                      │      │               ./'       │\n      │   \\.                    │      │              ,/         │\n      │     '.                  │      │            ./'          │\n    L │      '.                 │   L' │           ,'            │\n      │        \\.               │      │         ,/              │\n      │          ',             │      │       ./'               │\n    0 │            '*-._________│   -1 │______./                 │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.ModifiedHuberLoss","page":"Losses","title":"LossFunctions.ModifiedHuberLoss","text":"ModifiedHuberLoss <: MarginLoss\n\nA special (4 times scaled) case of the SmoothedL1HingeLoss with γ=2. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = begincases max  0 1 - a  ^2  quad textif  a ge -1  - 4 a  quad textotherwise endcases\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    5 │    '.                   │    0 │                .+-------│\n      │     '.                  │      │              ./'        │\n      │      '\\                 │      │             ,/          │\n      │        \\                │      │           ,/            │\n    L │         '.              │   L' │         ./              │\n      │          '.             │      │       ./'               │\n      │            \\.           │      │______/'                 │\n    0 │              '-.________│   -5 │                         │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L2MarginLoss","page":"Losses","title":"LossFunctions.L2MarginLoss","text":"L2MarginLoss <: MarginLoss\n\nThe margin-based least-squares loss for classification, which penalizes every prediction where agreement != 1 quadratically. It is locally Lipschitz continuous and strongly convex.\n\nL(a) = left( 1 - a right)^2\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    5 │     .                   │    2 │                       ,r│\n      │     '.                  │      │                     ,/  │\n      │      '\\                 │      │                   ,/    │\n      │        \\                │      ├                 ,/      ┤\n    L │         '.              │   L' │               ./        │\n      │          '.             │      │             ./          │\n      │            \\.          .│      │           ./            │\n    0 │              '-.____.-' │   -3 │         ./              │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.ExpLoss","page":"Losses","title":"LossFunctions.ExpLoss","text":"ExpLoss <: MarginLoss\n\nThe margin-based exponential loss for classification, which penalizes every prediction exponentially. It is infinitely many times differentiable, locally Lipschitz continuous and strictly convex, but not clipable.\n\nL(a) = e^-a\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    5 │  \\.                     │    0 │               _,,---:'\"\"│\n      │   l                     │      │           _r/\"'         │\n      │    l.                   │      │        .r/'             │\n      │     \":                  │      │      .r'                │\n    L │       \\.                │   L' │     ./                  │\n      │        \"\\..             │      │    .'                   │\n      │           '\":,_         │      │   ,'                    │\n    0 │                \"\"---:.__│   -5 │  ./                     │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.SigmoidLoss","page":"Losses","title":"LossFunctions.SigmoidLoss","text":"SigmoidLoss <: MarginLoss\n\nContinuous loss which penalizes every prediction with a loss within in the range (0,2). It is infinitely many times differentiable, Lipschitz continuous but nonconvex.\n\nL(a) = 1 - tanh(a)\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │\"\"'--,.                  │    0 │..                     ..│\n      │      '\\.                │      │ \"\\.                 ./\" │\n      │         '.              │      │    ',             ,'    │\n      │           \\.            │      │      \\           /      │\n    L │            \"\\.          │   L' │       \\         /       │\n      │              \\.         │      │        \\.     ./        │\n      │                \\,       │      │         \\.   ./         │\n    0 │                  '\"-:.__│   -1 │          ',_,'          │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.DWDMarginLoss","page":"Losses","title":"LossFunctions.DWDMarginLoss","text":"DWDMarginLoss <: MarginLoss\n\nThe distance weighted discrimination margin loss. It is a differentiable generalization of the L1HingeLoss that is different than the SmoothedL1HingeLoss. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = begincases 1 - a  quad textif  a ge fracqq+1  frac1a^q fracq^q(q+1)^q+1  quad textotherwise endcases\n\n\n\n              Lossfunction (q=1)               Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │      \".                 │    0 │                     ._r-│\n      │        \\.               │      │                   ./    │\n      │         ',              │      │                 ./      │\n      │           \\.            │      │                 /       │\n    L │            \"\\.          │   L' │                .        │\n      │              \\.         │      │                /        │\n      │               \":__      │      │               ;         │\n    0 │                   '\"\"---│   -1 │---------------┘         │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"<script type=\"module\">\n  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@9/dist/mermaid.esm.min.mjs';\n  mermaid.initialize({ startOnLoad: true });\n</script>","category":"page"},{"location":"","page":"Home","title":"Home","text":"SymbolicRegression.jl searches for symbolic expressions which optimize a particular objective.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div align=\"center\">\n<video width=\"800\" height=\"600\" controls>\n<source src=\"https://github.com/MilesCranmer/SymbolicRegression.jl/assets/7593028/f5b68f1f-9830-497f-a197-6ae332c94ee0\" type=\"video/mp4\">\n</video>\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"Latest release Documentation Forums Paper\n(Image: version) (Image: Dev) (Image: Discussions) (Image: Paper)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Build status Coverage\n(Image: CI) (Image: Coverage Status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Check out PySR for a Python frontend. Cite this software","category":"page"},{"location":"","page":"Home","title":"Home","text":"Contents:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Quickstart\nMLJ Interface\nLow-Level Interface\nConstructing expressions\nExporting to SymbolicUtils.jl\nContributors ✨\nCode structure\nSearch options","category":"page"},{"location":"#Quickstart","page":"Home","title":"Quickstart","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Install in Julia with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"SymbolicRegression\")","category":"page"},{"location":"#MLJ-Interface","page":"Home","title":"MLJ Interface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The easiest way to use SymbolicRegression.jl is with MLJ. Let's see an example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"import SymbolicRegression: SRRegressor\nimport MLJ: machine, fit!, predict, report\n\n# Dataset with two named features:\nX = (a = rand(500), b = rand(500))\n\n# and one target:\ny = @. 2 * cos(X.a * 23.5) - X.b ^ 2\n\n# with some noise:\ny = y .+ randn(500) .* 1e-3\n\nmodel = SRRegressor(\n    niterations=50,\n    binary_operators=[+, -, *],\n    unary_operators=[cos],\n)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Now, let's create and train this model on our data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mach = machine(model, X, y)\n\nfit!(mach)","category":"page"},{"location":"","page":"Home","title":"Home","text":"You will notice that expressions are printed using the column names of our table. If, instead of a table-like object, a simple array is passed (e.g., X=randn(100, 2)), x1, ..., xn will be used for variable names.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let's look at the expressions discovered:","category":"page"},{"location":"","page":"Home","title":"Home","text":"report(mach)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, we can make predictions with the expressions on new data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"predict(mach, X)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This will make predictions using the expression selected by model.selection_method, which by default is a mix of accuracy and complexity.","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can override this selection and select an equation from the Pareto front manually with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"predict(mach, (data=X, idx=2))","category":"page"},{"location":"","page":"Home","title":"Home","text":"where here we choose to evaluate the second equation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For fitting multiple outputs, one can use MultitargetSRRegressor (and pass an array of indices to idx in predict for selecting specific equations). For a full list of options available to each regressor, see the API page.","category":"page"},{"location":"#Low-Level-Interface","page":"Home","title":"Low-Level Interface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The heart of SymbolicRegression.jl is the equation_search function. This takes a 2D array and attempts to model a 1D array using analytic functional forms. Note: unlike the MLJ interface, this assumes column-major input of shape [features, rows].","category":"page"},{"location":"","page":"Home","title":"Home","text":"import SymbolicRegression: Options, equation_search\n\nX = randn(2, 100)\ny = 2 * cos.(X[2, :]) + X[1, :] .^ 2 .- 2\n\noptions = Options(\n    binary_operators=[+, *, /, -],\n    unary_operators=[cos, exp],\n    populations=20\n)\n\nhall_of_fame = equation_search(\n    X, y, niterations=40, options=options,\n    parallelism=:multithreading\n)","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can view the resultant equations in the dominating Pareto front (best expression seen at each complexity) with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"import SymbolicRegression: calculate_pareto_frontier\n\ndominating = calculate_pareto_frontier(hall_of_fame)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is a vector of PopMember type - which contains the expression along with the score. We can get the expressions with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"trees = [member.tree for member in dominating]","category":"page"},{"location":"","page":"Home","title":"Home","text":"Each of these equations is an Expression{T} type for some constant type T (like Float32).","category":"page"},{"location":"","page":"Home","title":"Home","text":"These expression objects are callable – you can simply pass in data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"tree = trees[end]\noutput = tree(X)","category":"page"},{"location":"#Constructing-expressions","page":"Home","title":"Constructing expressions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Expressions are represented under-the-hood as the Node type which is developed in the DynamicExpressions.jl package. The Expression type wraps this and includes metadata about operators and variable names.","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can manipulate and construct expressions directly. For example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SymbolicRegression: Options, Expression, Node\n\noptions = Options(;\n    binary_operators=[+, -, *, /], unary_operators=[cos, exp, sin]\n)\noperators = options.operators\nvariable_names = [\"x1\", \"x2\", \"x3\"]\nx1, x2, x3 = [Expression(Node(Float64; feature=i); operators, variable_names) for i=1:3]\n\ntree = cos(x1 - 3.2 * x2) - x1 * x1","category":"page"},{"location":"","page":"Home","title":"Home","text":"This tree has Float64 constants, so the type of the entire tree will be promoted to Node{Float64}.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can convert all constants (recursively) to Float32:","category":"page"},{"location":"","page":"Home","title":"Home","text":"float32_tree = convert(Expression{Float32}, tree)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can then evaluate this tree on a dataset:","category":"page"},{"location":"","page":"Home","title":"Home","text":"X = rand(Float32, 3, 100)\n\ntree(X)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This callable format is the easy-to-use version which will automatically set all values to NaN if there were any Inf or NaN during evaluation. You can call the raw evaluation method with eval_tree_array:","category":"page"},{"location":"","page":"Home","title":"Home","text":"output, did_succeed = eval_tree_array(tree, X)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where did_succeed explicitly declares whether the evaluation was successful.","category":"page"},{"location":"#Exporting-to-SymbolicUtils.jl","page":"Home","title":"Exporting to SymbolicUtils.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We can view the equations in the dominating Pareto frontier with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"dominating = calculate_pareto_frontier(hall_of_fame)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can convert the best equation to SymbolicUtils.jl with the following function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"import SymbolicRegression: node_to_symbolic\n\neqn = node_to_symbolic(dominating[end].tree)\nprintln(simplify(eqn*5 + 3))","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can also print out the full pareto frontier like so:","category":"page"},{"location":"","page":"Home","title":"Home","text":"import SymbolicRegression: compute_complexity, string_tree\n\nprintln(\"Complexity\\tMSE\\tEquation\")\n\nfor member in dominating\n    complexity = compute_complexity(member, options)\n    loss = member.loss\n    string = string_tree(member.tree, options)\n\n    println(\"$(complexity)\\t$(loss)\\t$(string)\")\nend","category":"page"},{"location":"#Contributors","page":"Home","title":"Contributors ✨","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We are eager to welcome new contributors! If you have an idea for a new feature, don't hesitate to share it on the issues page or forums.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://www.linkedin.com/in/markkittisopikul/\"><img src=\"https://avatars.githubusercontent.com/u/8062771?v=4?s=50\" width=\"50px;\" alt=\"Mark Kittisopikul\"/><br /><sub><b>Mark Kittisopikul</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=mkitti\" title=\"Code\">💻</a> <a href=\"#ideas-mkitti\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#infra-mkitti\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#platform-mkitti\" title=\"Packaging/porting to new platform\">📦</a> <a href=\"#promotion-mkitti\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Amkitti\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#tool-mkitti\" title=\"Tools\">🔧</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=mkitti\" title=\"Tests\">⚠️</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/tttc3\"><img src=\"https://avatars.githubusercontent.com/u/97948946?v=4?s=50\" width=\"50px;\" alt=\"T Coxon\"/><br /><sub><b>T Coxon</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3Atttc3\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=tttc3\" title=\"Code\">💻</a> <a href=\"#plugin-tttc3\" title=\"Plugin/utility libraries\">🔌</a> <a href=\"#ideas-tttc3\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#infra-tttc3\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#maintenance-tttc3\" title=\"Maintenance\">🚧</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Atttc3\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#tool-tttc3\" title=\"Tools\">🔧</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=tttc3\" title=\"Tests\">⚠️</a> <a href=\"#userTesting-tttc3\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/DhananjayAshok\"><img src=\"https://avatars.githubusercontent.com/u/46792537?v=4?s=50\" width=\"50px;\" alt=\"Dhananjay Ashok\"/><br /><sub><b>Dhananjay Ashok</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=DhananjayAshok\" title=\"Code\">💻</a> <a href=\"#example-DhananjayAshok\" title=\"Examples.\">🌍</a> <a href=\"#ideas-DhananjayAshok\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#maintenance-DhananjayAshok\" title=\"Maintenance\">🚧</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=DhananjayAshok\" title=\"Tests\">⚠️</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://gitlab.com/johanbluecreek\"><img src=\"https://avatars.githubusercontent.com/u/852554?v=4?s=50\" width=\"50px;\" alt=\"Johan Blåbäck\"/><br /><sub><b>Johan Blåbäck</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3Ajohanbluecreek\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=johanbluecreek\" title=\"Code\">💻</a> <a href=\"#ideas-johanbluecreek\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#maintenance-johanbluecreek\" title=\"Maintenance\">🚧</a> <a href=\"#promotion-johanbluecreek\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Ajohanbluecreek\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=johanbluecreek\" title=\"Tests\">⚠️</a> <a href=\"#userTesting-johanbluecreek\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://mathopt.de/people/martensen/index.php\"><img src=\"https://avatars.githubusercontent.com/u/20998300?v=4?s=50\" width=\"50px;\" alt=\"JuliusMartensen\"/><br /><sub><b>JuliusMartensen</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3AAlCap23\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=AlCap23\" title=\"Code\">💻</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=AlCap23\" title=\"Documentation\">📖</a> <a href=\"#plugin-AlCap23\" title=\"Plugin/utility libraries\">🔌</a> <a href=\"#ideas-AlCap23\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#infra-AlCap23\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#maintenance-AlCap23\" title=\"Maintenance\">🚧</a> <a href=\"#platform-AlCap23\" title=\"Packaging/porting to new platform\">📦</a> <a href=\"#promotion-AlCap23\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3AAlCap23\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#tool-AlCap23\" title=\"Tools\">🔧</a> <a href=\"#userTesting-AlCap23\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/ngam\"><img src=\"https://avatars.githubusercontent.com/u/67342040?v=4?s=50\" width=\"50px;\" alt=\"ngam\"/><br /><sub><b>ngam</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=ngam\" title=\"Code\">💻</a> <a href=\"#infra-ngam\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#platform-ngam\" title=\"Packaging/porting to new platform\">📦</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Angam\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#tool-ngam\" title=\"Tools\">🔧</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=ngam\" title=\"Tests\">⚠️</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/kazewong\"><img src=\"https://avatars.githubusercontent.com/u/8803931?v=4?s=50\" width=\"50px;\" alt=\"Kaze Wong\"/><br /><sub><b>Kaze Wong</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3Akazewong\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=kazewong\" title=\"Code\">💻</a> <a href=\"#ideas-kazewong\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#infra-kazewong\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#maintenance-kazewong\" title=\"Maintenance\">🚧</a> <a href=\"#promotion-kazewong\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Akazewong\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#research-kazewong\" title=\"Research\">🔬</a> <a href=\"#userTesting-kazewong\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/ChrisRackauckas\"><img src=\"https://avatars.githubusercontent.com/u/1814174?v=4?s=50\" width=\"50px;\" alt=\"Christopher Rackauckas\"/><br /><sub><b>Christopher Rackauckas</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3AChrisRackauckas\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=ChrisRackauckas\" title=\"Code\">💻</a> <a href=\"#plugin-ChrisRackauckas\" title=\"Plugin/utility libraries\">🔌</a> <a href=\"#ideas-ChrisRackauckas\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#infra-ChrisRackauckas\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#promotion-ChrisRackauckas\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3AChrisRackauckas\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#research-ChrisRackauckas\" title=\"Research\">🔬</a> <a href=\"#tool-ChrisRackauckas\" title=\"Tools\">🔧</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=ChrisRackauckas\" title=\"Tests\">⚠️</a> <a href=\"#userTesting-ChrisRackauckas\" title=\"User Testing\">📓</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://kidger.site/\"><img src=\"https://avatars.githubusercontent.com/u/33688385?v=4?s=50\" width=\"50px;\" alt=\"Patrick Kidger\"/><br /><sub><b>Patrick Kidger</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3Apatrick-kidger\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=patrick-kidger\" title=\"Code\">💻</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=patrick-kidger\" title=\"Documentation\">📖</a> <a href=\"#plugin-patrick-kidger\" title=\"Plugin/utility libraries\">🔌</a> <a href=\"#ideas-patrick-kidger\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#maintenance-patrick-kidger\" title=\"Maintenance\">🚧</a> <a href=\"#promotion-patrick-kidger\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Apatrick-kidger\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#research-patrick-kidger\" title=\"Research\">🔬</a> <a href=\"#tool-patrick-kidger\" title=\"Tools\">🔧</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=patrick-kidger\" title=\"Tests\">⚠️</a> <a href=\"#userTesting-patrick-kidger\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/OkonSamuel\"><img src=\"https://avatars.githubusercontent.com/u/39421418?v=4?s=50\" width=\"50px;\" alt=\"Okon Samuel\"/><br /><sub><b>Okon Samuel</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3AOkonSamuel\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=OkonSamuel\" title=\"Code\">💻</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=OkonSamuel\" title=\"Documentation\">📖</a> <a href=\"#maintenance-OkonSamuel\" title=\"Maintenance\">🚧</a> <a href=\"#ideas-OkonSamuel\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#infra-OkonSamuel\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3AOkonSamuel\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=OkonSamuel\" title=\"Tests\">⚠️</a> <a href=\"#userTesting-OkonSamuel\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/w2ll2am\"><img src=\"https://avatars.githubusercontent.com/u/16038228?v=4?s=50\" width=\"50px;\" alt=\"William Booth-Clibborn\"/><br /><sub><b>William Booth-Clibborn</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=w2ll2am\" title=\"Code\">💻</a> <a href=\"#example-w2ll2am\" title=\"Examples.\">🌍</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=w2ll2am\" title=\"Documentation\">📖</a> <a href=\"#userTesting-w2ll2am\" title=\"User Testing\">📓</a> <a href=\"#maintenance-w2ll2am\" title=\"Maintenance\">🚧</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Aw2ll2am\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#tool-w2ll2am\" title=\"Tools\">🔧</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=w2ll2am\" title=\"Tests\">⚠️</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://pablo-lemos.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/38078898?v=4?s=50\" width=\"50px;\" alt=\"Pablo Lemos\"/><br /><sub><b>Pablo Lemos</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3APablo-Lemos\" title=\"Bug reports\">🐛</a> <a href=\"#ideas-Pablo-Lemos\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#promotion-Pablo-Lemos\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3APablo-Lemos\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#research-Pablo-Lemos\" title=\"Research\">🔬</a> <a href=\"#userTesting-Pablo-Lemos\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/Moelf\"><img src=\"https://avatars.githubusercontent.com/u/5306213?v=4?s=50\" width=\"50px;\" alt=\"Jerry Ling\"/><br /><sub><b>Jerry Ling</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3AMoelf\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=Moelf\" title=\"Code\">💻</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=Moelf\" title=\"Documentation\">📖</a> <a href=\"#example-Moelf\" title=\"Examples.\">🌍</a> <a href=\"#ideas-Moelf\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#promotion-Moelf\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3AMoelf\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#userTesting-Moelf\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/CharFox1\"><img src=\"https://avatars.githubusercontent.com/u/35052672?v=4?s=50\" width=\"50px;\" alt=\"Charles Fox\"/><br /><sub><b>Charles Fox</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3ACharFox1\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=CharFox1\" title=\"Code\">💻</a> <a href=\"#ideas-CharFox1\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#maintenance-CharFox1\" title=\"Maintenance\">🚧</a> <a href=\"#promotion-CharFox1\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3ACharFox1\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#research-CharFox1\" title=\"Research\">🔬</a> <a href=\"#userTesting-CharFox1\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/johannbrehmer\"><img src=\"https://avatars.githubusercontent.com/u/17068560?v=4?s=50\" width=\"50px;\" alt=\"Johann Brehmer\"/><br /><sub><b>Johann Brehmer</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=johannbrehmer\" title=\"Code\">💻</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=johannbrehmer\" title=\"Documentation\">📖</a> <a href=\"#ideas-johannbrehmer\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#promotion-johannbrehmer\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Ajohannbrehmer\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#research-johannbrehmer\" title=\"Research\">🔬</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=johannbrehmer\" title=\"Tests\">⚠️</a> <a href=\"#userTesting-johannbrehmer\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"http://www.cosmicmar.com/\"><img src=\"https://avatars.githubusercontent.com/u/1510968?v=4?s=50\" width=\"50px;\" alt=\"Marius Millea\"/><br /><sub><b>Marius Millea</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=marius311\" title=\"Code\">💻</a> <a href=\"#ideas-marius311\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#promotion-marius311\" title=\"Promotion\">📣</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Amarius311\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#userTesting-marius311\" title=\"User Testing\">📓</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://gitlab.com/cobac\"><img src=\"https://avatars.githubusercontent.com/u/27872944?v=4?s=50\" width=\"50px;\" alt=\"Coba\"/><br /><sub><b>Coba</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3Acobac\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=cobac\" title=\"Code\">💻</a> <a href=\"#ideas-cobac\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Acobac\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#userTesting-cobac\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/pitmonticone\"><img src=\"https://avatars.githubusercontent.com/u/38562595?v=4?s=50\" width=\"50px;\" alt=\"Pietro Monticone\"/><br /><sub><b>Pietro Monticone</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3Apitmonticone\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=pitmonticone\" title=\"Documentation\">📖</a> <a href=\"#ideas-pitmonticone\" title=\"Ideas, planning, and feedback.\">💡</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/sheevy\"><img src=\"https://avatars.githubusercontent.com/u/1525683?v=4?s=50\" width=\"50px;\" alt=\"Mateusz Kubica\"/><br /><sub><b>Mateusz Kubica</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=sheevy\" title=\"Documentation\">📖</a> <a href=\"#ideas-sheevy\" title=\"Ideas, planning, and feedback.\">💡</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://jaywadekar.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/5493388?v=4?s=50\" width=\"50px;\" alt=\"Jay Wadekar\"/><br /><sub><b>Jay Wadekar</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3AJayWadekar\" title=\"Bug reports\">🐛</a> <a href=\"#ideas-JayWadekar\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#promotion-JayWadekar\" title=\"Promotion\">📣</a> <a href=\"#research-JayWadekar\" title=\"Research\">🔬</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/ablaom\"><img src=\"https://avatars.githubusercontent.com/u/30517088?v=4?s=50\" width=\"50px;\" alt=\"Anthony Blaom, PhD\"/><br /><sub><b>Anthony Blaom, PhD</b></sub></a><br /><a href=\"#infra-ablaom\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#ideas-ablaom\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Aablaom\" title=\"Reviewed Pull Requests\">👀</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/Jgmedina95\"><img src=\"https://avatars.githubusercontent.com/u/97254349?v=4?s=50\" width=\"50px;\" alt=\"Jgmedina95\"/><br /><sub><b>Jgmedina95</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3AJgmedina95\" title=\"Bug reports\">🐛</a> <a href=\"#ideas-Jgmedina95\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3AJgmedina95\" title=\"Reviewed Pull Requests\">👀</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/mcabbott\"><img src=\"https://avatars.githubusercontent.com/u/32575566?v=4?s=50\" width=\"50px;\" alt=\"Michael Abbott\"/><br /><sub><b>Michael Abbott</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=mcabbott\" title=\"Code\">💻</a> <a href=\"#ideas-mcabbott\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Amcabbott\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#tool-mcabbott\" title=\"Tools\">🔧</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/oscardssmith\"><img src=\"https://avatars.githubusercontent.com/u/11729272?v=4?s=50\" width=\"50px;\" alt=\"Oscar Smith\"/><br /><sub><b>Oscar Smith</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=oscardssmith\" title=\"Code\">💻</a> <a href=\"#ideas-oscardssmith\" title=\"Ideas, planning, and feedback.\">💡</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://ericphanson.com/\"><img src=\"https://avatars.githubusercontent.com/u/5846501?v=4?s=50\" width=\"50px;\" alt=\"Eric Hanson\"/><br /><sub><b>Eric Hanson</b></sub></a><br /><a href=\"#ideas-ericphanson\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#promotion-ericphanson\" title=\"Promotion\">📣</a> <a href=\"#userTesting-ericphanson\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/henriquebecker91\"><img src=\"https://avatars.githubusercontent.com/u/14113435?v=4?s=50\" width=\"50px;\" alt=\"Henrique Becker\"/><br /><sub><b>Henrique Becker</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=henriquebecker91\" title=\"Code\">💻</a> <a href=\"#ideas-henriquebecker91\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/pulls?q=is%3Apr+reviewed-by%3Ahenriquebecker91\" title=\"Reviewed Pull Requests\">👀</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/qwertyjl\"><img src=\"https://avatars.githubusercontent.com/u/110912592?v=4?s=50\" width=\"50px;\" alt=\"qwertyjl\"/><br /><sub><b>qwertyjl</b></sub></a><br /><a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/issues?q=author%3Aqwertyjl\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/MilesCranmer/SymbolicRegression.jl/commits?author=qwertyjl\" title=\"Documentation\">📖</a> <a href=\"#ideas-qwertyjl\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#userTesting-qwertyjl\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://huijzer.xyz/\"><img src=\"https://avatars.githubusercontent.com/u/20724914?v=4?s=50\" width=\"50px;\" alt=\"Rik Huijzer\"/><br /><sub><b>Rik Huijzer</b></sub></a><br /><a href=\"#ideas-rikhuijzer\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#infra-rikhuijzer\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://github.com/GCaptainNemo\"><img src=\"https://avatars.githubusercontent.com/u/43086239?v=4?s=50\" width=\"50px;\" alt=\"Hongyu Wang\"/><br /><sub><b>Hongyu Wang</b></sub></a><br /><a href=\"#ideas-GCaptainNemo\" title=\"Ideas, planning, and feedback.\">💡</a> <a href=\"#promotion-GCaptainNemo\" title=\"Promotion\">📣</a> <a href=\"#research-GCaptainNemo\" title=\"Research\">🔬</a></td>\n      <td align=\"center\" valign=\"top\" width=\"12.5%\"><a href=\"https://sauravmaheshkar.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/61241031?v=4?s=50\" width=\"50px;\" alt=\"Saurav Maheshkar\"/><br /><sub><b>Saurav Maheshkar</b></sub></a><br /><a href=\"#tool-SauravMaheshkar\" title=\"Tools\">🔧</a></td>\n    </tr>\n  </tbody>\n</table>","category":"page"},{"location":"#Code-structure","page":"Home","title":"Code structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SymbolicRegression.jl is organized roughly as follows. Rounded rectangles indicate objects, and rectangles indicate functions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(if you can't see this diagram being rendered, try pasting it into mermaid-js.github.io/mermaid-live-editor)","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"mermaid\">\n\nflowchart TB\n    op([Options])\n    d([Dataset])\n    op --> ES\n    d --> ES\n    subgraph ES[equation_search]\n        direction TB\n        IP[sr_spawner]\n        IP --> p1\n        IP --> p2\n        subgraph p1[Thread 1]\n            direction LR\n            pop1([Population])\n            pop1 --> src[s_r_cycle]\n            src --> opt[optimize_and_simplify_population]\n            opt --> pop1\n        end\n        subgraph p2[Thread 2]\n            direction LR\n            pop2([Population])\n            pop2 --> src2[s_r_cycle]\n            src2 --> opt2[optimize_and_simplify_population]\n            opt2 --> pop2\n        end\n        pop1 --> hof\n        pop2 --> hof\n        hof([HallOfFame])\n        hof --> migration\n        pop1 <-.-> migration\n        pop2 <-.-> migration\n        migration[migrate!]\n    end\n    ES --> output([HallOfFame])\n\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"The HallOfFame objects store the expressions with the lowest loss seen at each complexity.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The dependency structure of the code itself is as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"mermaid\">\n\nstateDiagram-v2\n    AdaptiveParsimony --> Mutate\n    AdaptiveParsimony --> Population\n    AdaptiveParsimony --> RegularizedEvolution\n    AdaptiveParsimony --> SearchUtils\n    AdaptiveParsimony --> SingleIteration\n    AdaptiveParsimony --> SymbolicRegression\n    CheckConstraints --> Mutate\n    CheckConstraints --> SymbolicRegression\n    Complexity --> CheckConstraints\n    Complexity --> HallOfFame\n    Complexity --> LossFunctions\n    Complexity --> MLJInterface\n    Complexity --> Mutate\n    Complexity --> PopMember\n    Complexity --> Population\n    Complexity --> SearchUtils\n    Complexity --> SingleIteration\n    Complexity --> SymbolicRegression\n    ConstantOptimization --> ExpressionBuilder\n    ConstantOptimization --> Mutate\n    ConstantOptimization --> SingleIteration\n    Core --> AdaptiveParsimony\n    Core --> CheckConstraints\n    Core --> Complexity\n    Core --> ConstantOptimization\n    Core --> DimensionalAnalysis\n    Core --> ExpressionBuilder\n    Core --> ExpressionBuilder\n    Core --> HallOfFame\n    Core --> InterfaceDynamicExpressions\n    Core --> LossFunctions\n    Core --> MLJInterface\n    Core --> Migration\n    Core --> Mutate\n    Core --> MutationFunctions\n    Core --> PopMember\n    Core --> Population\n    Core --> Recorder\n    Core --> RegularizedEvolution\n    Core --> SearchUtils\n    Core --> SingleIteration\n    Core --> SymbolicRegression\n    Dataset --> Core\n    DimensionalAnalysis --> LossFunctions\n    ExpressionBuilder --> SymbolicRegression\n    HallOfFame --> ExpressionBuilder\n    HallOfFame --> MLJInterface\n    HallOfFame --> SearchUtils\n    HallOfFame --> SingleIteration\n    HallOfFame --> SymbolicRegression\n    HallOfFame --> deprecates\n    InterfaceDynamicExpressions --> ExpressionBuilder\n    InterfaceDynamicExpressions --> HallOfFame\n    InterfaceDynamicExpressions --> LossFunctions\n    InterfaceDynamicExpressions --> SymbolicRegression\n    InterfaceDynamicQuantities --> Dataset\n    InterfaceDynamicQuantities --> MLJInterface\n    LossFunctions --> ConstantOptimization\n    LossFunctions --> ExpressionBuilder\n    LossFunctions --> ExpressionBuilder\n    LossFunctions --> Mutate\n    LossFunctions --> PopMember\n    LossFunctions --> Population\n    LossFunctions --> SingleIteration\n    LossFunctions --> SymbolicRegression\n    MLJInterface --> SymbolicRegression\n    Migration --> SymbolicRegression\n    Mutate --> RegularizedEvolution\n    MutationFunctions --> ExpressionBuilder\n    MutationFunctions --> Mutate\n    MutationFunctions --> Population\n    MutationFunctions --> SymbolicRegression\n    MutationFunctions --> deprecates\n    MutationWeights --> Core\n    MutationWeights --> Options\n    MutationWeights --> OptionsStruct\n    Operators --> Core\n    Operators --> Options\n    Options --> Core\n    OptionsStruct --> Core\n    OptionsStruct --> Options\n    OptionsStruct --> Options\n    PopMember --> ConstantOptimization\n    PopMember --> ExpressionBuilder\n    PopMember --> HallOfFame\n    PopMember --> Migration\n    PopMember --> Mutate\n    PopMember --> Population\n    PopMember --> SearchUtils\n    PopMember --> SingleIteration\n    PopMember --> SymbolicRegression\n    Population --> ExpressionBuilder\n    Population --> Migration\n    Population --> RegularizedEvolution\n    Population --> SearchUtils\n    Population --> SingleIteration\n    Population --> SymbolicRegression\n    ProgramConstants --> Core\n    ProgramConstants --> Dataset\n    ProgramConstants --> Operators\n    ProgressBars --> SearchUtils\n    ProgressBars --> SymbolicRegression\n    Recorder --> Mutate\n    Recorder --> RegularizedEvolution\n    Recorder --> SingleIteration\n    Recorder --> SymbolicRegression\n    RegularizedEvolution --> SingleIteration\n    SearchUtils --> SymbolicRegression\n    SingleIteration --> SymbolicRegression\n    Utils --> ConstantOptimization\n    Utils --> Dataset\n    Utils --> DimensionalAnalysis\n    Utils --> HallOfFame\n    Utils --> InterfaceDynamicExpressions\n    Utils --> MLJInterface\n    Utils --> Migration\n    Utils --> Operators\n    Utils --> Options\n    Utils --> PopMember\n    Utils --> Population\n    Utils --> RegularizedEvolution\n    Utils --> SearchUtils\n    Utils --> SingleIteration\n    Utils --> SymbolicRegression\n\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"Bash command to generate dependency structure from src directory (requires vim-stream):","category":"page"},{"location":"","page":"Home","title":"Home","text":"echo 'stateDiagram-v2'\nIFS=$'\\n'\nfor f in *.jl; do\n    for line in $(cat $f | grep -e 'import \\.\\.' -e 'import \\.' -e 'using \\.' -e 'using \\.\\.'); do\n        echo $(echo $line | vims -s 'dwf:d$' -t '%s/^\\.*//g' '%s/Module//g') $(basename \"$f\" .jl);\n    done;\ndone | vims -l 'f a--> ' | sort","category":"page"},{"location":"#Search-options","page":"Home","title":"Search options","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See https://ai.damtp.cam.ac.uk/symbolicregression/stable/api/#Options","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"examples.md\", \"examples/template_expression.md\", \"api.md\", \"types.md\", \"losses.md\"]","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"EditURL = \"../../../examples/parameterized_function.jl\"","category":"page"},{"location":"examples/parameterized_function/#Learning-Parameterized-Expressions","page":"Parameterized Expressions","title":"Learning Parameterized Expressions","text":"","category":"section"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"Note: Parametric expressions are currently considered experimental and may change in the future.","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"Parameterized expressions in SymbolicRegression.jl allow you to discover symbolic expressions that contain optimizable parameters. This is particularly useful when you have data that follows different patterns based on some categorical variable, or when you want to learn an expression with constants that should be optimized during the search.","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"In this tutorial, we'll generate synthetic data with class-dependent parameters and use symbolic regression to discover the parameterized expressions.","category":"page"},{"location":"examples/parameterized_function/#The-Problem","page":"Parameterized Expressions","title":"The Problem","text":"","category":"section"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"Let's create a synthetic dataset where the underlying function changes based on a class label:","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"y = 2cos(x_2 + 01) + x_1^2 - 32     textclass 1 \ntextOR \ny = 2cos(x_2 + 15) + x_1^2 - 05     textclass 2","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"We will need to simultaneously learn the symbolic expression and per-class parameters!","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"using SymbolicRegression\nusing Random: MersenneTwister\nusing Zygote\nusing MLJBase: machine, fit!, predict, report\nusing Test","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"Now, we generate synthetic data, with these 2 different classes.","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"Note that the class feature is given special treatment for the SRRegressor as a categorical variable:","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"X = let rng = MersenneTwister(0), n = 30\n    (; x1=randn(rng, n), x2=randn(rng, n), class=rand(rng, 1:2, n))\nend","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"Now, we generate target values using the true model that has class-dependent parameters:","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"y = let P1 = [0.1, 1.5], P2 = [3.2, 0.5]\n    [2 * cos(x2 + P1[class]) + x1^2 - P2[class] for (x1, x2, class) in zip(X.x1, X.x2, X.class)]\nend","category":"page"},{"location":"examples/parameterized_function/#Setting-up-the-Search","page":"Parameterized Expressions","title":"Setting up the Search","text":"","category":"section"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"We'll configure the symbolic regression search to:","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"Use parameterized expressions with up to 2 parameters\nUse Zygote.jl for automatic differentiation during parameter optimization (important when using parametric expressions, as it is higher dimensional)","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"model = SRRegressor(;\n    niterations=100,\n    binary_operators=[+, *, /, -],\n    unary_operators=[cos, exp],\n    populations=30,\n    expression_type=ParametricExpression,\n    expression_options=(; max_parameters=2),\n    autodiff_backend=:Zygote,\n);\nnothing #hide","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"Now, let's set up the machine and fit it:","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"mach = machine(model, X, y)","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"At this point, you would run:","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"fit!(mach)","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"You can extract the best expression and parameters with:","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"report(mach).equations[end]","category":"page"},{"location":"examples/parameterized_function/#Key-Takeaways","page":"Parameterized Expressions","title":"Key Takeaways","text":"","category":"section"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"ParametricExpressions allows us to discover symbolic expressions with optimizable parameters\nThe parameters can capture class-dependent variations in the underlying model","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"This approach is particularly useful when you suspect your data follows a common functional form, but with varying parameters across different conditions or class!","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"<details>\n<summary> Show raw source code </summary>","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"#=\n# Learning Parameterized Expressions\n\n_Note: Parametric expressions are currently considered experimental and may change in the future._\n\nParameterized expressions in SymbolicRegression.jl allow you to discover symbolic expressions that contain\noptimizable parameters. This is particularly useful when you have data that follows different patterns\nbased on some categorical variable, or when you want to learn an expression with constants that should\nbe optimized during the search.\n\nIn this tutorial, we'll generate synthetic data with class-dependent parameters and use symbolic regression to discover the parameterized expressions.\n\n## The Problem\n\nLet's create a synthetic dataset where the underlying function changes based on a class label:\n\n\\```math\ny = 2\\cos(x_2 + 0.1) + x_1^2 - 3.2 \\ \\ \\ \\ \\text{[class 1]} \\\\\n\\text{OR} \\\\\ny = 2\\cos(x_2 + 1.5) + x_1^2 - 0.5 \\ \\ \\ \\ \\text{[class 2]}\n\\```\n\nWe will need to simultaneously learn the symbolic expression and per-class parameters!\n=#\nusing SymbolicRegression\nusing Random: MersenneTwister\nusing Zygote\nusing MLJBase: machine, fit!, predict, report\nusing Test\n\n#=\nNow, we generate synthetic data, with these 2 different classes.\n\nNote that the `class` feature is given special treatment for the [`SRRegressor`](@ref)\nas a categorical variable:\n=#\n\nX = let rng = MersenneTwister(0), n = 30\n    (; x1=randn(rng, n), x2=randn(rng, n), class=rand(rng, 1:2, n))\nend\n\n#=\nNow, we generate target values using the true model that\nhas class-dependent parameters:\n=#\ny = let P1 = [0.1, 1.5], P2 = [3.2, 0.5]\n    [2 * cos(x2 + P1[class]) + x1^2 - P2[class] for (x1, x2, class) in zip(X.x1, X.x2, X.class)]\nend\n\n#=\n## Setting up the Search\n\nWe'll configure the symbolic regression search to:\n- Use parameterized expressions with up to 2 parameters\n- Use Zygote.jl for automatic differentiation during parameter optimization (important when using parametric expressions, as it is higher dimensional)\n=#\n\nstop_at = Ref(1e-4)  #src\n\nmodel = SRRegressor(;\n    niterations=100,\n    binary_operators=[+, *, /, -],\n    unary_operators=[cos, exp],\n    populations=30,\n    expression_type=ParametricExpression,\n    expression_options=(; max_parameters=2),\n    autodiff_backend=:Zygote,\n    early_stop_condition=(loss, _) -> loss < stop_at[],  #src\n);\n\n#=\nNow, let's set up the machine and fit it:\n=#\n\nmach = machine(model, X, y)\n\n#=\nAt this point, you would run:\n\n\\```julia\nfit!(mach)\n\\```\n\nYou can extract the best expression and parameters with:\n\n\\```julia\nreport(mach).equations[end]\n\\```\n\n## Key Takeaways\n\n1. [`ParametricExpression`](@ref)s allows us to discover symbolic expressions with optimizable parameters\n2. The parameters can capture class-dependent variations in the underlying model\n\nThis approach is particularly useful when you suspect your data follows a common\nfunctional form, but with varying parameters across different conditions or class!\n=#","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"which uses Literate.jl to generate this page.","category":"page"},{"location":"examples/parameterized_function/","page":"Parameterized Expressions","title":"Parameterized Expressions","text":"</details>","category":"page"},{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/#Equations","page":"Types","title":"Equations","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Equations are specified as binary trees with the Node type, defined as follows.","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"Node","category":"page"},{"location":"types/#DynamicExpressions.NodeModule.Node","page":"Types","title":"DynamicExpressions.NodeModule.Node","text":"Node{T} <: AbstractExpressionNode{T}\n\nNode defines a symbolic expression stored in a binary tree. A single Node instance is one \"node\" of this tree, and has references to its children. By tracing through the children nodes, you can evaluate or print a given expression.\n\nFields\n\ndegree::UInt8: Degree of the node. 0 for constants, 1 for   unary operators, 2 for binary operators.\nconstant::Bool: Whether the node is a constant.\nval::T: Value of the node. If degree==0, and constant==true,   this is the value of the constant. It has a type specified by the   overall type of the Node (e.g., Float64).\nfeature::UInt16: Index of the feature to use in the   case of a feature node. Only used if degree==0 and constant==false.    Only defined if degree == 0 && constant == false.\nop::UInt8: If degree==1, this is the index of the operator   in operators.unaops. If degree==2, this is the index of the   operator in operators.binops. In other words, this is an enum   of the operators, and is dependent on the specific OperatorEnum   object. Only defined if degree >= 1\nl::Node{T}: Left child of the node. Only defined if degree >= 1.   Same type as the parent node.\nr::Node{T}: Right child of the node. Only defined if degree == 2.   Same type as the parent node. This is to be passed as the right   argument to the binary operator.\n\nConstructors\n\nNode([T]; val=nothing, feature=nothing, op=nothing, l=nothing, r=nothing, children=nothing, allocator=default_allocator)\nNode{T}(; val=nothing, feature=nothing, op=nothing, l=nothing, r=nothing, children=nothing, allocator=default_allocator)\n\nCreate a new node in an expression tree. If T is not specified in either the type or the first argument, it will be inferred from the value of val passed or l and/or r. If it cannot be inferred from these, it will default to Float32.\n\nThe children keyword can be used instead of l and r and should be a tuple of children. This is to permit the use of splatting in constructors.\n\nYou may also construct nodes via the convenience operators generated by creating an OperatorEnum.\n\nYou may also choose to specify a default memory allocator for the node other than simply Node{T}() in the allocator keyword argument.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Types","title":"Types","text":"When you create an Options object, the operators passed are also re-defined for Node types. This allows you use, e.g., t=Node(; feature=1) * 3f0 to create a tree, so long as * was specified as a binary operator. This works automatically for operators defined in Base, although you can also get this to work for user-defined operators by using @extend_operators:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"@extend_operators options","category":"page"},{"location":"types/#SymbolicRegression.InterfaceDynamicExpressionsModule.@extend_operators-Tuple{Any}","page":"Types","title":"SymbolicRegression.InterfaceDynamicExpressionsModule.@extend_operators","text":"@extend_operators options\n\nExtends all operators defined in this options object to work on the AbstractExpressionNode type. While by default this is already done for operators defined in Base when you create an options and pass define_helper_functions=true, this does not apply to the user-defined operators. Thus, to do so, you must apply this macro to the operator enum in the same module you have the operators defined.\n\n\n\n\n\n","category":"macro"},{"location":"types/","page":"Types","title":"Types","text":"When using these node constructors, types will automatically be promoted. You can convert the type of a node using convert:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"convert(::Type{Node{T1}}, tree::Node{T2}) where {T1, T2}","category":"page"},{"location":"types/#Base.convert-Union{Tuple{T2}, Tuple{T1}, Tuple{Type{Node{T1}}, Node{T2}}} where {T1, T2}","page":"Types","title":"Base.convert","text":"convert(::Type{<:AbstractExpressionNode{T1}}, n::AbstractExpressionNode{T2}) where {T1,T2}\n\nConvert a AbstractExpressionNode{T2} to a AbstractExpressionNode{T1}. This will recursively convert all children nodes to AbstractExpressionNode{T1}, using convert(T1, tree.val) at constant nodes.\n\nArguments\n\n::Type{AbstractExpressionNode{T1}}: Type to convert to.\ntree::AbstractExpressionNode{T2}: AbstractExpressionNode to convert.\n\n\n\n\n\n","category":"method"},{"location":"types/","page":"Types","title":"Types","text":"You can set a tree (in-place) with set_node!:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"set_node!","category":"page"},{"location":"types/#DynamicExpressions.NodeModule.set_node!","page":"Types","title":"DynamicExpressions.NodeModule.set_node!","text":"set_node!(tree::AbstractExpressionNode{T}, new_tree::AbstractExpressionNode{T}) where {T}\n\nSet every field of tree equal to the corresponding field of new_tree.\n\n\n\n\n\n","category":"function"},{"location":"types/","page":"Types","title":"Types","text":"You can create a copy of a node with copy_node:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"copy_node(tree::Node)","category":"page"},{"location":"types/#DynamicExpressions.NodeModule.copy_node-Tuple{Node}","page":"Types","title":"DynamicExpressions.NodeModule.copy_node","text":"copy_node(tree::AbstractExpressionNode; break_sharing::Val{BS}=Val(false)) where {BS}\n\nCopy a node, recursively copying all children nodes. This is more efficient than the built-in copy.\n\nIf break_sharing is set to Val(true), sharing in a tree will be ignored.\n\n\n\n\n\n","category":"method"},{"location":"types/#Expressions","page":"Types","title":"Expressions","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Expressions are represented using the Expression type, which combines the raw Node type with an OperatorEnum.","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"Expression","category":"page"},{"location":"types/#DynamicExpressions.ExpressionModule.Expression","page":"Types","title":"DynamicExpressions.ExpressionModule.Expression","text":"Expression{T, N, D} <: AbstractExpression{T, N}\n\n(Experimental) Defines a high-level, user-facing, expression type that encapsulates an expression tree (like Node) along with associated metadata for evaluation and rendering.\n\nFields\n\ntree::N: The root node of the raw expression tree.\nmetadata::Metadata{D}: A named tuple of settings for the expression,   such as the operators and variable names.\n\nConstructors\n\nExpression(tree::AbstractExpressionNode, metadata::NamedTuple): Construct from the fields\n@parse_expression(expr, operators=operators, variable_names=variable_names, node_type=Node): Parse a Julia expression with a given context and create an Expression object.\n\nUsage\n\nThis type is intended for end-users to interact with and manipulate expressions at a high level, abstracting away the complexities of the underlying expression tree operations.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Types","title":"Types","text":"These types allow you to define and manipulate expressions with a clear separation between the structure and the operators used.","category":"page"},{"location":"types/#Parametric-Expressions","page":"Types","title":"Parametric Expressions","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Parametric expressions are a type of expression that includes parameters which can be optimized during the search.","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"ParametricExpression\nParametricNode","category":"page"},{"location":"types/#DynamicExpressions.ParametricExpressionModule.ParametricExpression","page":"Types","title":"DynamicExpressions.ParametricExpressionModule.ParametricExpression","text":"ParametricExpression{T,N<:ParametricNode{T},D<:NamedTuple} <: AbstractExpression{T,N}\n\n(Experimental) An expression to store parameters for a tree\n\n\n\n\n\n","category":"type"},{"location":"types/#DynamicExpressions.ParametricExpressionModule.ParametricNode","page":"Types","title":"DynamicExpressions.ParametricExpressionModule.ParametricNode","text":"A type of expression node that also stores a parameter index\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Types","title":"Types","text":"These types allow you to define expressions with parameters that can be tuned to fit the data better. You can specify the maximum number of parameters using the expression_options argument in SRRegressor.","category":"page"},{"location":"types/#Template-Expressions","page":"Types","title":"Template Expressions","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Template expressions allow you to specify predefined structures and constraints for your expressions. These use ComposableExpressions as their internal expression type, which makes them flexible for creating a structure out of a single function.","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"These use the TemplateStructure type to define how expressions should be combined and evaluated.","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"TemplateExpression\nTemplateStructure","category":"page"},{"location":"types/#SymbolicRegression.TemplateExpressionModule.TemplateExpression","page":"Types","title":"SymbolicRegression.TemplateExpressionModule.TemplateExpression","text":"TemplateExpression{T,F,N,E,TS,D} <: AbstractStructuredExpression{T,F,N,E,D}\n\nA symbolic expression that allows the combination of multiple sub-expressions in a structured way, with constraints on variable usage.\n\nTemplateExpression is designed for symbolic regression tasks where domain-specific knowledge or constraints must be imposed on the model's structure.\n\nConstructor\n\nTemplateExpression(trees; structure, operators, variable_names)\ntrees: A NamedTuple holding the sub-expressions (e.g., f = Expression(...), g = Expression(...)).\nstructure: A TemplateStructure which holds functions that define how the sub-expressions are combined   in different contexts.\noperators: An OperatorEnum that defines the allowed operators for the sub-expressions.\nvariable_names: An optional Vector of String that defines the names of the variables in the dataset.\n\nExample\n\nLet's create an example TemplateExpression that combines two sub-expressions f(x1, x2) and g(x3):\n\n# Define operators and variable names\noptions = Options(; binary_operators=(+, *, /, -), unary_operators=(sin, cos))\noperators = options.operators\nvariable_names = [\"x1\", \"x2\", \"x3\"]\n\n# Create sub-expressions\nx1 = Expression(Node{Float64}(; feature=1); operators, variable_names)\nx2 = Expression(Node{Float64}(; feature=2); operators, variable_names)\nx3 = Expression(Node{Float64}(; feature=3); operators, variable_names)\n\n# Create TemplateExpression\nexample_expr = (; f=x1, g=x3)\nst_expr = TemplateExpression(\n    example_expr;\n    structure=TemplateStructure{(:f, :g)}(\n        ((; f, g), (x1, x2, x3)) -> sin(f(x1, x2)) + g(x3)^2\n    ),\n    operators,\n    variable_names,\n)\n\nWhen fitting a model in SymbolicRegression.jl, you would provide the TemplateExpression as the expression_type argument, and then pass expression_options=(; structure=TemplateStructure(...)) as additional options. The variable_constraints will constraint f to only have access to x1 and x2, and g to only have access to x3.\n\n\n\n\n\n","category":"type"},{"location":"types/#SymbolicRegression.TemplateExpressionModule.TemplateStructure","page":"Types","title":"SymbolicRegression.TemplateExpressionModule.TemplateStructure","text":"TemplateStructure{K,E,NF} <: Function\n\nA struct that defines a prescribed structure for a TemplateExpression, including functions that define the result in different contexts.\n\nThe K parameter is used to specify the symbols representing the inner expressions. If not declared using the constructor TemplateStructure{K}(...), the keys of the variable_constraints NamedTuple will be used to infer this.\n\nFields\n\ncombine: Required function taking a NamedTuple of ComposableExpressions (sharing the keys K),   and then tuple representing the data of ValidVectors. For example,   ((; f, g), (x1, x2, x3)) -> f(x1, x2) + g(x3) would be a valid combine function. You may also   re-use the callable expressions and use different inputs, such as   ((; f, g), (x1, x2)) -> f(x1 + g(x2)) - g(x1) is another valid choice.\nnum_features: Optional NamedTuple of function keys => integers representing the number of   features used by each expression. If not provided, it will be inferred using the combine   function. For example, if f takes two arguments, and g takes one, then   num_features = (; f=2, g=1).\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Types","title":"Types","text":"Composable expressions allow you to combine multiple expressions together.","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"ComposableExpression","category":"page"},{"location":"types/#SymbolicRegression.ComposableExpressionModule.ComposableExpression","page":"Types","title":"SymbolicRegression.ComposableExpressionModule.ComposableExpression","text":"ComposableExpression{T,N,D} <: AbstractComposableExpression{T,N} <: AbstractExpression{T,N}\n\nA symbolic expression representing a mathematical formula as an expression tree (tree::N) with associated metadata (metadata::Metadata{D}). Used to construct and manipulate expressions in symbolic regression tasks.\n\nExample:\n\nCreate variables x1 and x2, and build an expression f = x1 * sin(x2):\n\noperators = OperatorEnum(; binary_operators=(+, *, /, -), unary_operators=(sin, cos))\nvariable_names = [\"x1\", \"x2\"]\nx1 = ComposableExpression(Node(Float64; feature=1); operators, variable_names)\nx2 = ComposableExpression(Node(Float64; feature=2); operators, variable_names)\nf = x1 * sin(x2)\n# ^This now references the first and second arguments of things passed to it:\n\nf(x1, x1) # == x1 * sin(x1)\nf(randn(5), randn(5)) # == randn(5) .* sin.(randn(5))\n\n# You can even pass it to itself:\nf(f, f) # == (x1 * sin(x2)) * sin((x1 * sin(x2)))\n\n\n\n\n\n","category":"type"},{"location":"types/#Population","page":"Types","title":"Population","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Groups of equations are given as a population, which is an array of trees tagged with score, loss, and birthdate–-these values are given in the PopMember.","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"Population","category":"page"},{"location":"types/#SymbolicRegression.PopulationModule.Population","page":"Types","title":"SymbolicRegression.PopulationModule.Population","text":"Population(pop::Array{PopMember{T,L}, 1})\n\nCreate population from list of PopMembers.\n\n\n\n\n\nPopulation(dataset::Dataset{T,L};\n           population_size, nlength::Int=3, options::AbstractOptions,\n           nfeatures::Int)\n\nCreate random population and score them on the dataset.\n\n\n\n\n\nPopulation(X::AbstractMatrix{T}, y::AbstractVector{T};\n           population_size, nlength::Int=3,\n           options::AbstractOptions, nfeatures::Int,\n           loss_type::Type=Nothing)\n\nCreate random population and score them on the dataset.\n\n\n\n\n\n","category":"type"},{"location":"types/#Population-members","page":"Types","title":"Population members","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"PopMember","category":"page"},{"location":"types/#SymbolicRegression.PopMemberModule.PopMember","page":"Types","title":"SymbolicRegression.PopMemberModule.PopMember","text":"PopMember(t::AbstractExpression{T}, score::L, loss::L)\n\nCreate a population member with a birth date at the current time. The type of the Node may be different from the type of the score and loss.\n\nArguments\n\nt::AbstractExpression{T}: The tree for the population member.\nscore::L: The score (normalized to a baseline, and offset by a complexity penalty)\nloss::L: The raw loss to assign.\n\n\n\n\n\nPopMember(\n    dataset::Dataset{T,L},\n    t::AbstractExpression{T},\n    options::AbstractOptions\n)\n\nCreate a population member with a birth date at the current time. Automatically compute the score for this tree.\n\nArguments\n\ndataset::Dataset{T,L}: The dataset to evaluate the tree on.\nt::AbstractExpression{T}: The tree for the population member.\noptions::AbstractOptions: What options to use.\n\n\n\n\n\n","category":"type"},{"location":"types/#Hall-of-Fame","page":"Types","title":"Hall of Fame","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"HallOfFame","category":"page"},{"location":"types/#SymbolicRegression.HallOfFameModule.HallOfFame","page":"Types","title":"SymbolicRegression.HallOfFameModule.HallOfFame","text":"HallOfFame{T<:DATA_TYPE,L<:LOSS_TYPE}\n\nList of the best members seen all time in .members, with .members[c] being the best member seen at complexity c. Including only the members which actually have been set, you can run .members[exists].\n\nFields\n\nmembers::Array{PopMember{T,L},1}: List of the best members seen all time.   These are ordered by complexity, with .members[1] the member with complexity 1.\nexists::Array{Bool,1}: Whether the member at the given complexity has been set.\n\n\n\n\n\n","category":"type"},{"location":"types/#Dataset","page":"Types","title":"Dataset","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Dataset\nupdate_baseline_loss!","category":"page"},{"location":"types/#SymbolicRegression.CoreModule.DatasetModule.Dataset","page":"Types","title":"SymbolicRegression.CoreModule.DatasetModule.Dataset","text":"Dataset{T<:DATA_TYPE,L<:LOSS_TYPE}\n\nFields\n\nX::AbstractMatrix{T}: The input features, with shape (nfeatures, n).\ny::AbstractVector{T}: The desired output values, with shape (n,).\nindex::Int: The index of the output feature corresponding to this   dataset, if any.\nn::Int: The number of samples.\nnfeatures::Int: The number of features.\nweights::Union{AbstractVector,Nothing}: If the dataset is weighted,   these specify the per-sample weight (with shape (n,)).\nextra::NamedTuple: Extra information to pass to a custom evaluation   function. Since this is an arbitrary named tuple, you could pass   any sort of dataset you wish to here.\navg_y: The average value of y (weighted, if weights are passed).\nuse_baseline: Whether to use a baseline loss. This will be set to false   if the baseline loss is calculated to be Inf.\nbaseline_loss: The loss of a constant function which predicts the average   value of y. This is loss-dependent and should be updated with   update_baseline_loss!.\nvariable_names::Array{String,1}: The names of the features,   with shape (nfeatures,).\ndisplay_variable_names::Array{String,1}: A version of variable_names   but for printing to the terminal (e.g., with unicode versions).\ny_variable_name::String: The name of the output variable.\nX_units: Unit information of X. When used, this is a vector   of DynamicQuantities.Quantity{<:Any,<:Dimensions} with shape (nfeatures,).\ny_units: Unit information of y. When used, this is a single   DynamicQuantities.Quantity{<:Any,<:Dimensions}.\nX_sym_units: Unit information of X. When used, this is a vector   of DynamicQuantities.Quantity{<:Any,<:SymbolicDimensions} with shape (nfeatures,).\ny_sym_units: Unit information of y. When used, this is a single   DynamicQuantities.Quantity{<:Any,<:SymbolicDimensions}.\n\n\n\n\n\n","category":"type"},{"location":"types/#SymbolicRegression.LossFunctionsModule.update_baseline_loss!","page":"Types","title":"SymbolicRegression.LossFunctionsModule.update_baseline_loss!","text":"update_baseline_loss!(dataset::Dataset{T,L}, options::AbstractOptions) where {T<:DATA_TYPE,L<:LOSS_TYPE}\n\nUpdate the baseline loss of the dataset using the loss function specified in options.\n\n\n\n\n\n","category":"function"}]
}
